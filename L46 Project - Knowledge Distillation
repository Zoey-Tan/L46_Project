{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"18zsRW8k5CyDBocX24ykEm9slHQFpjWp4","timestamp":1705855959823}],"gpuType":"V100","mount_file_id":"1m9l5_BPgzZpBAZv2Eh9Y72mn3gmkHosR","authorship_tag":"ABX9TyMQzhjQGQ4/3ARt8S3Wokfw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install torcheval\n","!pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yRpCn445VUzq","executionInfo":{"status":"ok","timestamp":1705944198210,"user_tz":0,"elapsed":12841,"user":{"displayName":"Xiaochen Zoey Tan","userId":"02061312605743465288"}},"outputId":"6ea71a0a-fe36-429f-8353-df5de3a6b4ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torcheval\n","  Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.5.0)\n","Installing collected packages: torcheval\n","Successfully installed torcheval-0.0.7\n","Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]}]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from torch.utils.data import TensorDataset, DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torcheval.metrics.functional import multiclass_precision, multiclass_recall, multiclass_f1_score, multiclass_auprc, multiclass_confusion_matrix, multiclass_precision_recall_curve"],"metadata":{"id":"xdYyQ55NSWHh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0')"],"metadata":{"id":"MSb-8zd-zjFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zCeqo7RTE8f8"},"outputs":[],"source":["with open('/content/drive/MyDrive/l46_dataset/X_train.npy', 'rb') as f:\n","  X_train = np.load(f)\n","with open('/content/drive/MyDrive/l46_dataset/Y_train.npy', 'rb') as f:\n","  y_train = np.load(f)\n","with open('/content/drive/MyDrive/l46_dataset/X_test.npy', 'rb') as f:\n","  X_test = np.load(f)\n","with open('/content/drive/MyDrive/l46_dataset/Y_test.npy', 'rb') as f:\n","  y_test = np.load(f)"]},{"cell_type":"code","source":["X_train_tensor = torch.Tensor(X_train).to(device)\n","X_test_tensor = torch.Tensor(X_test).to(device)\n","y_train_tensor = torch.Tensor(np.argmax(y_train, axis=-1).astype(int)).long().to(device)\n","y_test_tensor = torch.Tensor(np.argmax(y_test, axis=-1).astype(int)).long().to(device)\n","\n","train_set = TensorDataset(X_train_tensor, y_train_tensor)\n","test_set = TensorDataset(X_test_tensor, y_test_tensor)\n","\n","train_dataloader = DataLoader(train_set, batch_size=128)\n","test_dataloader = DataLoader(test_set, batch_size=32)"],"metadata":{"id":"UaHAdmlfYgja"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_tensor.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aElS2bzdzQ9r","executionInfo":{"status":"ok","timestamp":1705944216871,"user_tz":0,"elapsed":4,"user":{"displayName":"Xiaochen Zoey Tan","userId":"02061312605743465288"}},"outputId":"d3286567-ae80-43fc-da99-daade9a1bf40"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1004, 221, 193])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["class BaselineLSTM(nn.Module):\n","    def __init__(self):\n","        super(BaselineLSTM, self).__init__()\n","        self.lstm1 = nn.LSTM(input_size=193, hidden_size=1024, batch_first=True)\n","        self.d1 = nn.Dropout(p=0.2)\n","        self.lstm2 = nn.LSTM(input_size=1024, hidden_size=512, batch_first=True)\n","        self.d2 = nn.Dropout(p=0.2)\n","        self.lstm3 = nn.LSTM(input_size=512, hidden_size=256, batch_first=True)\n","        self.d3 = nn.Dropout(p=0.2)\n","        self.lstm4 = nn.LSTM(input_size=256, hidden_size=128, batch_first=True)\n","        self.d4 = nn.Dropout(p=0.2)\n","        self.lstm5 = nn.LSTM(input_size=128, hidden_size=64, batch_first=True)\n","        self.d5 = nn.Dropout(p=0.2)\n","        self.maxpool = nn.MaxPool1d(2)\n","        self.f = nn.Flatten()\n","        self.fc1 = nn.Linear(7040, 100)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(100, 6)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        x, (final_hidden_state, final_cell_state) = self.lstm1(x)\n","        x = self.d1(x)\n","        x, (final_hidden_state, final_cell_state) = self.lstm2(x)\n","        x = self.d2(x)\n","        x, (final_hidden_state, final_cell_state) = self.lstm3(x)\n","        x = self.d3(x)\n","        x, (final_hidden_state, final_cell_state) = self.lstm4(x)\n","        x = self.d4(x)\n","        x, (final_hidden_state, final_cell_state) = self.lstm5(x)\n","        x = self.d5(x)\n","        x = self.maxpool(x.permute(0,2,1)).permute(0,2,1)\n","        x = self.f(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.softmax(x)\n","        return x\n"],"metadata":{"id":"8_fnInSDSWPw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BaselineConvLSTM(nn.Module):\n","    def __init__(self):\n","        super(BaselineConvLSTM, self).__init__()\n","        self.conv1 = torch.nn.Conv1d(in_channels=193, out_channels=128, kernel_size=2)\n","        self.conv2 = torch.nn.Conv1d(in_channels=128, out_channels=64, kernel_size=2)\n","        self.d1 = nn.Dropout(p=0.5)\n","        self.lstm1 = nn.LSTM(input_size=64, hidden_size=128, batch_first=True)\n","        self.d2 = nn.Dropout(p=0.5)\n","        self.lstm2 = nn.LSTM(input_size=128, hidden_size=64, batch_first=True)\n","        self.d3 = nn.Dropout(p=0.5)\n","        self.maxpool = nn.MaxPool1d(2)\n","        self.f = nn.Flatten()\n","        self.fc1 = nn.Linear(6976, 100)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(100, 6)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        x = x.permute(0, 2, 1)\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = x.permute(0, 2, 1)\n","        x = self.d1(x)\n","        x, (final_hidden_state, final_cell_state) = self.lstm1(x)\n","        x = self.d2(x)\n","        x, (final_hidden_state, final_cell_state) = self.lstm2(x)\n","        x = self.d3(x)\n","        x = self.maxpool(x.permute(0,2,1)).permute(0,2,1)\n","        x = self.f(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.softmax(x)\n","        return x"],"metadata":{"id":"J_msAOONIr0U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BaselineConv(nn.Module):\n","    def __init__(self):\n","        super(BaselineConv, self).__init__()\n","        self.conv1 = torch.nn.Conv1d(in_channels=193, out_channels=64, kernel_size=2,)\n","        self.d1 = nn.Dropout(p=0.8)\n","        self.f = nn.Flatten()\n","        #self.fc1 = nn.Linear(12288, 100)\n","        #self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(14080, 6)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        x = x.permute(0, 2, 1)\n","        x = self.conv1(x)\n","        x = x.permute(0, 2, 1)\n","        x = self.d1(x)\n","        x = self.f(x)\n","        #x = self.fc1(x)\n","        #x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.softmax(x)\n","        return x\n","\n","class BaselineConv(nn.Module):\n","    def __init__(self):\n","        super(BaselineConv, self).__init__()\n","        self.conv1 = torch.nn.Conv1d(in_channels=221, out_channels=64, kernel_size=2,)\n","        self.d1 = nn.Dropout(p=0.8)\n","        self.f = nn.Flatten()\n","        #self.fc1 = nn.Linear(12288, 100)\n","        #self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(12288, 6)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        #x = x.permute(0, 2, 1)\n","        x = self.conv1(x)\n","        #x = x.permute(0, 2, 1)\n","        x = self.d1(x)\n","        x = self.f(x)\n","        #x = self.fc1(x)\n","        #x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.softmax(x)\n","        return x"],"metadata":{"id":"4jbXFJ7-m1rE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchinfo import summary\n","baseline_model = BaselineConv().to(device)\n","summary(baseline_model, (1,221, 193))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qPuMuAcqMsvE","executionInfo":{"status":"ok","timestamp":1705945410808,"user_tz":0,"elapsed":7,"user":{"displayName":"Xiaochen Zoey Tan","userId":"02061312605743465288"}},"outputId":"27252206-2ec4-4d77-f07a-2ef6c8baa2f9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","BaselineConv                             [1, 6]                    --\n","├─Conv1d: 1-1                            [1, 64, 220]              24,768\n","├─Dropout: 1-2                           [1, 220, 64]              --\n","├─Flatten: 1-3                           [1, 14080]                --\n","├─Linear: 1-4                            [1, 6]                    84,486\n","├─Softmax: 1-5                           [1, 6]                    --\n","==========================================================================================\n","Total params: 109,254\n","Trainable params: 109,254\n","Non-trainable params: 0\n","Total mult-adds (M): 5.53\n","==========================================================================================\n","Input size (MB): 0.17\n","Forward/backward pass size (MB): 0.11\n","Params size (MB): 0.44\n","Estimated Total Size (MB): 0.72\n","=========================================================================================="]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["baseline_model = BaselineConvLSTM().to(device)\n","summary(baseline_model, (1,221, 193))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKLOecYLMuKW","executionInfo":{"status":"ok","timestamp":1705945291058,"user_tz":0,"elapsed":3,"user":{"displayName":"Xiaochen Zoey Tan","userId":"02061312605743465288"}},"outputId":"66c56620-3d5f-4446-abf4-130d59fe1c29"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","BaselineConvLSTM                         [1, 6]                    --\n","├─Conv1d: 1-1                            [1, 128, 220]             49,536\n","├─Conv1d: 1-2                            [1, 64, 219]              16,448\n","├─Dropout: 1-3                           [1, 219, 64]              --\n","├─LSTM: 1-4                              [1, 219, 128]             99,328\n","├─Dropout: 1-5                           [1, 219, 128]             --\n","├─LSTM: 1-6                              [1, 219, 64]              49,664\n","├─Dropout: 1-7                           [1, 219, 64]              --\n","├─MaxPool1d: 1-8                         [1, 64, 109]              --\n","├─Flatten: 1-9                           [1, 6976]                 --\n","├─Linear: 1-10                           [1, 100]                  697,700\n","├─ReLU: 1-11                             [1, 100]                  --\n","├─Linear: 1-12                           [1, 6]                    606\n","├─Softmax: 1-13                          [1, 6]                    --\n","==========================================================================================\n","Total params: 913,282\n","Trainable params: 913,282\n","Non-trainable params: 0\n","Total mult-adds (M): 47.83\n","==========================================================================================\n","Input size (MB): 0.17\n","Forward/backward pass size (MB): 0.67\n","Params size (MB): 3.65\n","Estimated Total Size (MB): 4.50\n","=========================================================================================="]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["def train(model, epoch, loss_fn, optimizer, scheduler, batch_size=128):\n","  history = {'train_loss':[], 'train_accuracy':[], 'train_precision':[], 'train_recall':[], 'train_f1':[],\n","             'val_loss':[], 'val_accuracy':[], 'val_precision':[], 'val_recall':[], 'val_f1':[]}\n","  train_dataloader = DataLoader(train_set, batch_size=batch_size)\n","\n","  for epoch in range(epoch):\n","\n","      model.train(True)\n","      running_loss = 0.0\n","      correct = 0\n","      output_list = None\n","      label_list = None\n","      for i, data in enumerate(train_dataloader, 0):\n","\n","          inputs, labels = data\n","          optimizer.zero_grad()\n","          outputs = model(inputs)\n","          labels = labels.long()\n","          loss = loss_fn(torch.log(outputs), labels)\n","          loss.backward()\n","          optimizer.step()\n","          running_loss += loss.item()\n","          correct += (torch.argmax(outputs, dim=-1) == labels).float().sum()\n","          output_list = outputs if output_list is None else torch.cat((output_list,outputs),0)\n","          label_list = labels if label_list is None else torch.cat((label_list,labels),0)\n","\n","      avg_loss = running_loss / (i + 1)\n","      accuracy = correct / len(train_set)\n","      precision = multiclass_precision(output_list, label_list, num_classes=6, average='macro')\n","      recall = multiclass_recall(output_list, label_list, num_classes=6, average='macro')\n","      f1_score = multiclass_f1_score(output_list, label_list, num_classes=6, average='macro')\n","\n","      running_vloss = 0.0\n","      vcorrect = 0\n","      model.eval()\n","\n","      voutput_list = None\n","      vlabel_list = None\n","      with torch.no_grad():\n","          for i, vdata in enumerate(test_dataloader):\n","              vinputs, vlabels = vdata\n","              voutputs = model(vinputs)\n","              vloss = loss_fn(torch.log(voutputs), vlabels)\n","              running_vloss += vloss.item()\n","              vcorrect += (torch.argmax(voutputs, dim=-1) == vlabels).float().sum()\n","              voutput_list = voutputs if voutput_list is None else torch.cat((voutput_list,voutputs),0)\n","              vlabel_list = vlabels if vlabel_list is None else torch.cat((vlabel_list,vlabels),0)\n","\n","      avg_vloss = running_vloss / (i + 1)\n","      vaccuracy = vcorrect / len(test_set)\n","      vprecision = multiclass_precision(voutput_list, vlabel_list, num_classes=6, average='macro')\n","      vrecall = multiclass_recall(voutput_list, vlabel_list, num_classes=6, average='macro')\n","      vf1_score = multiclass_f1_score(voutput_list, vlabel_list, num_classes=6, average='macro')\n","\n","      print('Epoch {}-- Train loss:{:.2f} accuracy:{:.2f} precision:{:.2f} recall:{:.2f} f1:{:.2f}'.format(epoch, avg_loss, accuracy, precision, recall, f1_score))\n","      print('Epoch {}-- Valid loss:{:.2f} accuracy:{:.2f} precision:{:.2f} recall:{:.2f} f1:{:.2f}'.format(epoch, avg_vloss, vaccuracy, vprecision, vrecall, vf1_score))\n","      history['train_loss'].append(avg_loss)\n","      history['train_accuracy'].append(accuracy)\n","      history['train_precision'].append(precision)\n","      history['train_recall'].append(recall)\n","      history['train_f1'].append(f1_score)\n","      history['val_loss'].append(avg_vloss)\n","      history['val_accuracy'].append(vaccuracy)\n","      history['val_precision'].append(vprecision)\n","      history['val_recall'].append(vrecall)\n","      history['val_f1'].append(vf1_score)\n","      scheduler.step(avg_vloss)\n","\n","  return history\n"],"metadata":{"id":"VXHLOQoOXU7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def model_evaluate(model):\n","    model.eval()\n","    voutput_list = None\n","    vlabel_list = None\n","    running_vloss = 0.0\n","    vcorrect = 0\n","    with torch.no_grad():\n","        for i, vdata in enumerate(test_dataloader):\n","            vinputs, vlabels = vdata\n","            voutputs = model(vinputs)\n","            vloss = loss_fn(torch.log(voutputs), vlabels)\n","            running_vloss += vloss.item()\n","            vcorrect += (torch.argmax(voutputs, dim=-1) == vlabels).float().sum()\n","            voutput_list = voutputs if voutput_list is None else torch.cat((voutput_list,voutputs),0)\n","            vlabel_list = vlabels if vlabel_list is None else torch.cat((vlabel_list,vlabels),0)\n","\n","    avg_vloss = running_vloss / (i + 1)\n","    vaccuracy = vcorrect / len(test_set)\n","    vprecision = multiclass_precision(voutput_list, vlabel_list, num_classes=6, average='macro')\n","    vrecall = multiclass_recall(voutput_list, vlabel_list, num_classes=6, average='macro')\n","    vf1_score = multiclass_f1_score(voutput_list, vlabel_list, num_classes=6, average='macro')\n","    vauprc = multiclass_auprc(voutput_list, vlabel_list, num_classes=6, average='macro')\n","    vconfusion_matrix = multiclass_confusion_matrix(voutput_list, vlabel_list, num_classes=6, normalize=\"true\")\n","    vprc = multiclass_precision_recall_curve(voutput_list, vlabel_list, num_classes=6)\n","    analysis = {}\n","    analysis[\"loss\"] = avg_vloss\n","    analysis[\"accuracy\"] = vaccuracy\n","    analysis[\"precision\"] = vprecision\n","    analysis[\"recall\"] = vrecall\n","    analysis[\"f1_score\"] = vf1_score\n","    analysis[\"auprc\"] = vauprc\n","    analysis[\"confusion_matrix\"] = vconfusion_matrix\n","    analysis[\"prc\"] = vprc\n","    return analysis"],"metadata":{"id":"GZ5qPQV-FjDm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(50)\n","student_model_conv = BaselineConv().to(device)\n","student_model_convlstm = BaselineConvLSTM().to(device)\n","weights = torch.Tensor([20,20,1,10,10,10]).to(device)\n","#weights = torch.Tensor([1,1,1,1,1,1]).to(device)\n","loss_fn = nn.NLLLoss(weight=weights)\n","optimizer = torch.optim.Adam(student_model_conv.parameters(), lr=1e-4)\n","scheduler = ReduceLROnPlateau(optimizer)\n","history = train(student_model_conv, 250, loss_fn, optimizer, scheduler)\n","\n","optimizer = torch.optim.Adam(student_model_convlstm.parameters(), lr=1e-4)\n","scheduler = ReduceLROnPlateau(optimizer)\n","history = train(student_model_convlstm, 250, loss_fn, optimizer, scheduler)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dIQkuEG2n9iO","executionInfo":{"status":"ok","timestamp":1705947911663,"user_tz":0,"elapsed":32864,"user":{"displayName":"Xiaochen Zoey Tan","userId":"02061312605743465288"}},"outputId":"ef72508e-c9bf-4e8b-aefd-362f8223fdd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0-- Train loss:1.79 accuracy:0.09 precision:0.15 recall:0.15 f1:0.08\n","Epoch 0-- Valid loss:1.79 accuracy:0.05 precision:0.02 recall:0.16 f1:0.03\n","Epoch 1-- Train loss:1.79 accuracy:0.09 precision:0.21 recall:0.17 f1:0.09\n","Epoch 1-- Valid loss:1.78 accuracy:0.08 precision:0.03 recall:0.22 f1:0.06\n","Epoch 2-- Train loss:1.78 accuracy:0.12 precision:0.23 recall:0.20 f1:0.11\n","Epoch 2-- Valid loss:1.77 accuracy:0.15 precision:0.27 recall:0.28 f1:0.13\n","Epoch 3-- Train loss:1.77 accuracy:0.17 precision:0.24 recall:0.22 f1:0.14\n","Epoch 3-- Valid loss:1.76 accuracy:0.24 precision:0.31 recall:0.31 f1:0.16\n","Epoch 4-- Train loss:1.77 accuracy:0.26 precision:0.24 recall:0.22 f1:0.17\n","Epoch 4-- Valid loss:1.75 accuracy:0.36 precision:0.34 recall:0.38 f1:0.27\n","Epoch 5-- Train loss:1.75 accuracy:0.34 precision:0.26 recall:0.28 f1:0.22\n","Epoch 5-- Valid loss:1.74 accuracy:0.46 precision:0.54 recall:0.44 f1:0.35\n","Epoch 6-- Train loss:1.74 accuracy:0.43 precision:0.29 recall:0.33 f1:0.27\n","Epoch 6-- Valid loss:1.72 accuracy:0.51 precision:0.55 recall:0.48 f1:0.39\n","Epoch 7-- Train loss:1.72 accuracy:0.45 precision:0.30 recall:0.36 f1:0.28\n","Epoch 7-- Valid loss:1.71 accuracy:0.54 precision:0.56 recall:0.51 f1:0.41\n","Epoch 8-- Train loss:1.69 accuracy:0.49 precision:0.31 recall:0.38 f1:0.31\n","Epoch 8-- Valid loss:1.68 accuracy:0.57 precision:0.57 recall:0.52 f1:0.42\n","Epoch 9-- Train loss:1.67 accuracy:0.51 precision:0.34 recall:0.40 f1:0.33\n","Epoch 9-- Valid loss:1.66 accuracy:0.59 precision:0.59 recall:0.55 f1:0.46\n","Epoch 10-- Train loss:1.65 accuracy:0.52 precision:0.33 recall:0.40 f1:0.33\n","Epoch 10-- Valid loss:1.64 accuracy:0.62 precision:0.62 recall:0.59 f1:0.50\n","Epoch 11-- Train loss:1.63 accuracy:0.52 precision:0.32 recall:0.38 f1:0.32\n","Epoch 11-- Valid loss:1.61 accuracy:0.61 precision:0.60 recall:0.57 f1:0.48\n","Epoch 12-- Train loss:1.61 accuracy:0.55 precision:0.35 recall:0.42 f1:0.35\n","Epoch 12-- Valid loss:1.59 accuracy:0.62 precision:0.54 recall:0.57 f1:0.48\n","Epoch 13-- Train loss:1.58 accuracy:0.54 precision:0.34 recall:0.41 f1:0.34\n","Epoch 13-- Valid loss:1.57 accuracy:0.62 precision:0.53 recall:0.57 f1:0.48\n","Epoch 14-- Train loss:1.56 accuracy:0.56 precision:0.36 recall:0.43 f1:0.37\n","Epoch 14-- Valid loss:1.54 accuracy:0.61 precision:0.57 recall:0.57 f1:0.45\n","Epoch 15-- Train loss:1.55 accuracy:0.55 precision:0.35 recall:0.45 f1:0.37\n","Epoch 15-- Valid loss:1.52 accuracy:0.62 precision:0.57 recall:0.58 f1:0.47\n","Epoch 16-- Train loss:1.53 accuracy:0.55 precision:0.36 recall:0.44 f1:0.36\n","Epoch 16-- Valid loss:1.51 accuracy:0.63 precision:0.57 recall:0.58 f1:0.47\n","Epoch 17-- Train loss:1.51 accuracy:0.57 precision:0.37 recall:0.48 f1:0.39\n","Epoch 17-- Valid loss:1.49 accuracy:0.64 precision:0.58 recall:0.61 f1:0.50\n","Epoch 18-- Train loss:1.51 accuracy:0.57 precision:0.35 recall:0.45 f1:0.36\n","Epoch 18-- Valid loss:1.47 accuracy:0.64 precision:0.57 recall:0.61 f1:0.51\n","Epoch 19-- Train loss:1.47 accuracy:0.59 precision:0.39 recall:0.51 f1:0.41\n","Epoch 19-- Valid loss:1.46 accuracy:0.64 precision:0.57 recall:0.60 f1:0.50\n","Epoch 20-- Train loss:1.47 accuracy:0.57 precision:0.37 recall:0.48 f1:0.38\n","Epoch 20-- Valid loss:1.44 accuracy:0.64 precision:0.57 recall:0.60 f1:0.50\n","Epoch 21-- Train loss:1.44 accuracy:0.59 precision:0.39 recall:0.51 f1:0.40\n","Epoch 21-- Valid loss:1.43 accuracy:0.65 precision:0.57 recall:0.61 f1:0.51\n","Epoch 22-- Train loss:1.43 accuracy:0.60 precision:0.39 recall:0.52 f1:0.41\n","Epoch 22-- Valid loss:1.41 accuracy:0.65 precision:0.53 recall:0.62 f1:0.52\n","Epoch 23-- Train loss:1.44 accuracy:0.58 precision:0.36 recall:0.48 f1:0.38\n","Epoch 23-- Valid loss:1.40 accuracy:0.65 precision:0.51 recall:0.62 f1:0.51\n","Epoch 24-- Train loss:1.43 accuracy:0.59 precision:0.37 recall:0.49 f1:0.39\n","Epoch 24-- Valid loss:1.38 accuracy:0.65 precision:0.50 recall:0.60 f1:0.50\n","Epoch 25-- Train loss:1.40 accuracy:0.59 precision:0.39 recall:0.52 f1:0.41\n","Epoch 25-- Valid loss:1.37 accuracy:0.64 precision:0.47 recall:0.58 f1:0.48\n","Epoch 26-- Train loss:1.40 accuracy:0.61 precision:0.40 recall:0.53 f1:0.42\n","Epoch 26-- Valid loss:1.36 accuracy:0.65 precision:0.48 recall:0.59 f1:0.49\n","Epoch 27-- Train loss:1.39 accuracy:0.60 precision:0.38 recall:0.51 f1:0.40\n","Epoch 27-- Valid loss:1.35 accuracy:0.65 precision:0.50 recall:0.61 f1:0.50\n","Epoch 28-- Train loss:1.37 accuracy:0.60 precision:0.38 recall:0.51 f1:0.40\n","Epoch 28-- Valid loss:1.33 accuracy:0.66 precision:0.50 recall:0.61 f1:0.51\n","Epoch 29-- Train loss:1.35 accuracy:0.61 precision:0.41 recall:0.55 f1:0.43\n","Epoch 29-- Valid loss:1.32 accuracy:0.66 precision:0.50 recall:0.61 f1:0.51\n","Epoch 30-- Train loss:1.34 accuracy:0.62 precision:0.41 recall:0.55 f1:0.44\n","Epoch 30-- Valid loss:1.31 accuracy:0.67 precision:0.50 recall:0.62 f1:0.52\n","Epoch 31-- Train loss:1.33 accuracy:0.62 precision:0.42 recall:0.57 f1:0.44\n","Epoch 31-- Valid loss:1.30 accuracy:0.67 precision:0.50 recall:0.62 f1:0.52\n","Epoch 32-- Train loss:1.30 accuracy:0.63 precision:0.42 recall:0.56 f1:0.44\n","Epoch 32-- Valid loss:1.28 accuracy:0.68 precision:0.51 recall:0.62 f1:0.52\n","Epoch 33-- Train loss:1.31 accuracy:0.64 precision:0.43 recall:0.57 f1:0.46\n","Epoch 33-- Valid loss:1.27 accuracy:0.68 precision:0.50 recall:0.61 f1:0.52\n","Epoch 34-- Train loss:1.29 accuracy:0.62 precision:0.40 recall:0.53 f1:0.42\n","Epoch 34-- Valid loss:1.26 accuracy:0.69 precision:0.50 recall:0.62 f1:0.52\n","Epoch 35-- Train loss:1.30 accuracy:0.61 precision:0.40 recall:0.53 f1:0.42\n","Epoch 35-- Valid loss:1.25 accuracy:0.69 precision:0.50 recall:0.63 f1:0.53\n","Epoch 36-- Train loss:1.29 accuracy:0.63 precision:0.43 recall:0.56 f1:0.46\n","Epoch 36-- Valid loss:1.24 accuracy:0.69 precision:0.50 recall:0.63 f1:0.53\n","Epoch 37-- Train loss:1.26 accuracy:0.64 precision:0.42 recall:0.58 f1:0.46\n","Epoch 37-- Valid loss:1.23 accuracy:0.69 precision:0.52 recall:0.64 f1:0.54\n","Epoch 38-- Train loss:1.25 accuracy:0.63 precision:0.41 recall:0.56 f1:0.45\n","Epoch 38-- Valid loss:1.22 accuracy:0.69 precision:0.52 recall:0.64 f1:0.54\n","Epoch 39-- Train loss:1.25 accuracy:0.65 precision:0.44 recall:0.58 f1:0.47\n","Epoch 39-- Valid loss:1.21 accuracy:0.70 precision:0.52 recall:0.64 f1:0.55\n","Epoch 40-- Train loss:1.24 accuracy:0.64 precision:0.43 recall:0.60 f1:0.47\n","Epoch 40-- Valid loss:1.19 accuracy:0.70 precision:0.52 recall:0.64 f1:0.55\n","Epoch 41-- Train loss:1.22 accuracy:0.66 precision:0.44 recall:0.60 f1:0.48\n","Epoch 41-- Valid loss:1.18 accuracy:0.72 precision:0.53 recall:0.64 f1:0.56\n","Epoch 42-- Train loss:1.21 accuracy:0.66 precision:0.45 recall:0.60 f1:0.48\n","Epoch 42-- Valid loss:1.17 accuracy:0.73 precision:0.55 recall:0.67 f1:0.58\n","Epoch 43-- Train loss:1.19 accuracy:0.65 precision:0.45 recall:0.61 f1:0.48\n","Epoch 43-- Valid loss:1.16 accuracy:0.73 precision:0.55 recall:0.68 f1:0.58\n","Epoch 44-- Train loss:1.22 accuracy:0.65 precision:0.43 recall:0.58 f1:0.47\n","Epoch 44-- Valid loss:1.15 accuracy:0.72 precision:0.54 recall:0.67 f1:0.58\n","Epoch 45-- Train loss:1.20 accuracy:0.67 precision:0.46 recall:0.62 f1:0.50\n","Epoch 45-- Valid loss:1.14 accuracy:0.73 precision:0.55 recall:0.67 f1:0.58\n","Epoch 46-- Train loss:1.18 accuracy:0.66 precision:0.44 recall:0.59 f1:0.48\n","Epoch 46-- Valid loss:1.14 accuracy:0.74 precision:0.55 recall:0.67 f1:0.58\n","Epoch 47-- Train loss:1.19 accuracy:0.67 precision:0.46 recall:0.61 f1:0.50\n","Epoch 47-- Valid loss:1.13 accuracy:0.74 precision:0.56 recall:0.69 f1:0.60\n","Epoch 48-- Train loss:1.16 accuracy:0.67 precision:0.45 recall:0.60 f1:0.49\n","Epoch 48-- Valid loss:1.12 accuracy:0.75 precision:0.57 recall:0.69 f1:0.61\n","Epoch 49-- Train loss:1.16 accuracy:0.68 precision:0.47 recall:0.61 f1:0.50\n","Epoch 49-- Valid loss:1.11 accuracy:0.74 precision:0.56 recall:0.68 f1:0.59\n","Epoch 50-- Train loss:1.15 accuracy:0.68 precision:0.46 recall:0.62 f1:0.50\n","Epoch 50-- Valid loss:1.10 accuracy:0.75 precision:0.56 recall:0.68 f1:0.60\n","Epoch 51-- Train loss:1.15 accuracy:0.68 precision:0.47 recall:0.64 f1:0.52\n","Epoch 51-- Valid loss:1.09 accuracy:0.76 precision:0.58 recall:0.70 f1:0.62\n","Epoch 52-- Train loss:1.13 accuracy:0.69 precision:0.48 recall:0.64 f1:0.52\n","Epoch 52-- Valid loss:1.09 accuracy:0.76 precision:0.58 recall:0.70 f1:0.61\n","Epoch 53-- Train loss:1.13 accuracy:0.68 precision:0.46 recall:0.60 f1:0.50\n","Epoch 53-- Valid loss:1.08 accuracy:0.76 precision:0.58 recall:0.70 f1:0.61\n","Epoch 54-- Train loss:1.12 accuracy:0.69 precision:0.47 recall:0.62 f1:0.51\n","Epoch 54-- Valid loss:1.07 accuracy:0.76 precision:0.59 recall:0.69 f1:0.61\n","Epoch 55-- Train loss:1.12 accuracy:0.70 precision:0.47 recall:0.62 f1:0.51\n","Epoch 55-- Valid loss:1.06 accuracy:0.75 precision:0.56 recall:0.67 f1:0.60\n","Epoch 56-- Train loss:1.11 accuracy:0.70 precision:0.48 recall:0.63 f1:0.52\n","Epoch 56-- Valid loss:1.05 accuracy:0.76 precision:0.58 recall:0.71 f1:0.62\n","Epoch 57-- Train loss:1.08 accuracy:0.68 precision:0.46 recall:0.61 f1:0.50\n","Epoch 57-- Valid loss:1.04 accuracy:0.76 precision:0.58 recall:0.71 f1:0.62\n","Epoch 58-- Train loss:1.09 accuracy:0.70 precision:0.49 recall:0.65 f1:0.53\n","Epoch 58-- Valid loss:1.04 accuracy:0.76 precision:0.58 recall:0.71 f1:0.62\n","Epoch 59-- Train loss:1.10 accuracy:0.70 precision:0.49 recall:0.63 f1:0.52\n","Epoch 59-- Valid loss:1.03 accuracy:0.77 precision:0.59 recall:0.71 f1:0.63\n","Epoch 60-- Train loss:1.07 accuracy:0.71 precision:0.49 recall:0.64 f1:0.53\n","Epoch 60-- Valid loss:1.02 accuracy:0.77 precision:0.59 recall:0.71 f1:0.63\n","Epoch 61-- Train loss:1.07 accuracy:0.71 precision:0.50 recall:0.66 f1:0.55\n","Epoch 61-- Valid loss:1.01 accuracy:0.75 precision:0.57 recall:0.69 f1:0.61\n","Epoch 62-- Train loss:1.09 accuracy:0.71 precision:0.49 recall:0.65 f1:0.53\n","Epoch 62-- Valid loss:1.01 accuracy:0.75 precision:0.57 recall:0.69 f1:0.60\n","Epoch 63-- Train loss:1.05 accuracy:0.71 precision:0.50 recall:0.66 f1:0.54\n","Epoch 63-- Valid loss:1.00 accuracy:0.77 precision:0.59 recall:0.71 f1:0.63\n","Epoch 64-- Train loss:1.07 accuracy:0.70 precision:0.48 recall:0.62 f1:0.52\n","Epoch 64-- Valid loss:1.00 accuracy:0.77 precision:0.60 recall:0.72 f1:0.64\n","Epoch 65-- Train loss:1.04 accuracy:0.73 precision:0.52 recall:0.67 f1:0.56\n","Epoch 65-- Valid loss:0.99 accuracy:0.77 precision:0.59 recall:0.71 f1:0.63\n","Epoch 66-- Train loss:1.03 accuracy:0.72 precision:0.51 recall:0.67 f1:0.55\n","Epoch 66-- Valid loss:0.98 accuracy:0.77 precision:0.59 recall:0.71 f1:0.63\n","Epoch 67-- Train loss:1.06 accuracy:0.71 precision:0.49 recall:0.64 f1:0.53\n","Epoch 67-- Valid loss:0.98 accuracy:0.77 precision:0.59 recall:0.71 f1:0.63\n","Epoch 68-- Train loss:1.03 accuracy:0.71 precision:0.49 recall:0.65 f1:0.54\n","Epoch 68-- Valid loss:0.97 accuracy:0.76 precision:0.59 recall:0.71 f1:0.63\n","Epoch 69-- Train loss:1.02 accuracy:0.73 precision:0.53 recall:0.69 f1:0.57\n","Epoch 69-- Valid loss:0.96 accuracy:0.76 precision:0.59 recall:0.71 f1:0.62\n","Epoch 70-- Train loss:1.04 accuracy:0.71 precision:0.48 recall:0.63 f1:0.52\n","Epoch 70-- Valid loss:0.96 accuracy:0.76 precision:0.59 recall:0.71 f1:0.63\n","Epoch 71-- Train loss:1.02 accuracy:0.71 precision:0.49 recall:0.65 f1:0.53\n","Epoch 71-- Valid loss:0.95 accuracy:0.77 precision:0.59 recall:0.71 f1:0.63\n","Epoch 72-- Train loss:1.01 accuracy:0.73 precision:0.51 recall:0.66 f1:0.56\n","Epoch 72-- Valid loss:0.95 accuracy:0.77 precision:0.60 recall:0.71 f1:0.63\n","Epoch 73-- Train loss:1.00 accuracy:0.73 precision:0.52 recall:0.67 f1:0.56\n","Epoch 73-- Valid loss:0.94 accuracy:0.78 precision:0.61 recall:0.71 f1:0.64\n","Epoch 74-- Train loss:1.01 accuracy:0.73 precision:0.52 recall:0.68 f1:0.56\n","Epoch 74-- Valid loss:0.94 accuracy:0.78 precision:0.61 recall:0.71 f1:0.64\n","Epoch 75-- Train loss:0.99 accuracy:0.73 precision:0.52 recall:0.67 f1:0.56\n","Epoch 75-- Valid loss:0.93 accuracy:0.78 precision:0.61 recall:0.72 f1:0.65\n","Epoch 76-- Train loss:0.98 accuracy:0.74 precision:0.53 recall:0.69 f1:0.58\n","Epoch 76-- Valid loss:0.93 accuracy:0.78 precision:0.61 recall:0.72 f1:0.65\n","Epoch 77-- Train loss:1.00 accuracy:0.73 precision:0.51 recall:0.66 f1:0.55\n","Epoch 77-- Valid loss:0.92 accuracy:0.78 precision:0.61 recall:0.72 f1:0.65\n","Epoch 78-- Train loss:0.95 accuracy:0.73 precision:0.52 recall:0.68 f1:0.57\n","Epoch 78-- Valid loss:0.91 accuracy:0.78 precision:0.60 recall:0.71 f1:0.64\n","Epoch 79-- Train loss:0.99 accuracy:0.73 precision:0.52 recall:0.67 f1:0.57\n","Epoch 79-- Valid loss:0.91 accuracy:0.79 precision:0.61 recall:0.72 f1:0.65\n","Epoch 80-- Train loss:0.98 accuracy:0.73 precision:0.52 recall:0.66 f1:0.55\n","Epoch 80-- Valid loss:0.90 accuracy:0.78 precision:0.61 recall:0.72 f1:0.64\n","Epoch 81-- Train loss:0.96 accuracy:0.75 precision:0.55 recall:0.71 f1:0.59\n","Epoch 81-- Valid loss:0.90 accuracy:0.78 precision:0.60 recall:0.71 f1:0.64\n","Epoch 82-- Train loss:0.94 accuracy:0.75 precision:0.54 recall:0.70 f1:0.59\n","Epoch 82-- Valid loss:0.89 accuracy:0.78 precision:0.61 recall:0.71 f1:0.64\n","Epoch 83-- Train loss:0.97 accuracy:0.74 precision:0.52 recall:0.67 f1:0.57\n","Epoch 83-- Valid loss:0.89 accuracy:0.78 precision:0.61 recall:0.72 f1:0.65\n","Epoch 84-- Train loss:0.93 accuracy:0.76 precision:0.56 recall:0.72 f1:0.61\n","Epoch 84-- Valid loss:0.88 accuracy:0.77 precision:0.62 recall:0.71 f1:0.64\n","Epoch 85-- Train loss:0.96 accuracy:0.73 precision:0.52 recall:0.67 f1:0.56\n","Epoch 85-- Valid loss:0.88 accuracy:0.78 precision:0.63 recall:0.72 f1:0.65\n","Epoch 86-- Train loss:0.93 accuracy:0.76 precision:0.55 recall:0.70 f1:0.60\n","Epoch 86-- Valid loss:0.88 accuracy:0.78 precision:0.62 recall:0.72 f1:0.65\n","Epoch 87-- Train loss:0.93 accuracy:0.75 precision:0.53 recall:0.68 f1:0.58\n","Epoch 87-- Valid loss:0.87 accuracy:0.79 precision:0.62 recall:0.72 f1:0.65\n","Epoch 88-- Train loss:0.96 accuracy:0.75 precision:0.54 recall:0.69 f1:0.59\n","Epoch 88-- Valid loss:0.87 accuracy:0.78 precision:0.61 recall:0.72 f1:0.65\n","Epoch 89-- Train loss:0.93 accuracy:0.75 precision:0.55 recall:0.69 f1:0.59\n","Epoch 89-- Valid loss:0.86 accuracy:0.79 precision:0.63 recall:0.73 f1:0.66\n","Epoch 90-- Train loss:0.94 accuracy:0.75 precision:0.55 recall:0.69 f1:0.59\n","Epoch 90-- Valid loss:0.86 accuracy:0.79 precision:0.63 recall:0.72 f1:0.66\n","Epoch 91-- Train loss:0.91 accuracy:0.76 precision:0.55 recall:0.70 f1:0.60\n","Epoch 91-- Valid loss:0.85 accuracy:0.78 precision:0.62 recall:0.72 f1:0.65\n","Epoch 92-- Train loss:0.91 accuracy:0.76 precision:0.56 recall:0.71 f1:0.61\n","Epoch 92-- Valid loss:0.85 accuracy:0.79 precision:0.62 recall:0.72 f1:0.65\n","Epoch 93-- Train loss:0.91 accuracy:0.75 precision:0.55 recall:0.70 f1:0.59\n","Epoch 93-- Valid loss:0.85 accuracy:0.79 precision:0.62 recall:0.72 f1:0.65\n","Epoch 94-- Train loss:0.88 accuracy:0.77 precision:0.57 recall:0.73 f1:0.62\n","Epoch 94-- Valid loss:0.84 accuracy:0.78 precision:0.62 recall:0.72 f1:0.65\n","Epoch 95-- Train loss:0.91 accuracy:0.75 precision:0.55 recall:0.71 f1:0.60\n","Epoch 95-- Valid loss:0.84 accuracy:0.78 precision:0.62 recall:0.72 f1:0.65\n","Epoch 96-- Train loss:0.89 accuracy:0.76 precision:0.57 recall:0.72 f1:0.61\n","Epoch 96-- Valid loss:0.83 accuracy:0.80 precision:0.63 recall:0.73 f1:0.67\n","Epoch 97-- Train loss:0.88 accuracy:0.77 precision:0.55 recall:0.70 f1:0.60\n","Epoch 97-- Valid loss:0.83 accuracy:0.80 precision:0.64 recall:0.73 f1:0.67\n","Epoch 98-- Train loss:0.91 accuracy:0.76 precision:0.55 recall:0.69 f1:0.59\n","Epoch 98-- Valid loss:0.83 accuracy:0.80 precision:0.64 recall:0.73 f1:0.67\n","Epoch 99-- Train loss:0.89 accuracy:0.75 precision:0.54 recall:0.69 f1:0.58\n","Epoch 99-- Valid loss:0.82 accuracy:0.80 precision:0.64 recall:0.74 f1:0.68\n","Epoch 100-- Train loss:0.90 accuracy:0.76 precision:0.56 recall:0.70 f1:0.60\n","Epoch 100-- Valid loss:0.82 accuracy:0.80 precision:0.65 recall:0.75 f1:0.68\n","Epoch 101-- Train loss:0.87 accuracy:0.78 precision:0.57 recall:0.72 f1:0.62\n","Epoch 101-- Valid loss:0.82 accuracy:0.80 precision:0.64 recall:0.74 f1:0.68\n","Epoch 102-- Train loss:0.88 accuracy:0.76 precision:0.55 recall:0.68 f1:0.59\n","Epoch 102-- Valid loss:0.81 accuracy:0.80 precision:0.64 recall:0.75 f1:0.68\n","Epoch 103-- Train loss:0.87 accuracy:0.76 precision:0.55 recall:0.71 f1:0.60\n","Epoch 103-- Valid loss:0.81 accuracy:0.80 precision:0.65 recall:0.75 f1:0.68\n","Epoch 104-- Train loss:0.88 accuracy:0.76 precision:0.56 recall:0.71 f1:0.61\n","Epoch 104-- Valid loss:0.81 accuracy:0.80 precision:0.64 recall:0.74 f1:0.68\n","Epoch 105-- Train loss:0.87 accuracy:0.76 precision:0.55 recall:0.69 f1:0.59\n","Epoch 105-- Valid loss:0.80 accuracy:0.80 precision:0.65 recall:0.75 f1:0.68\n","Epoch 106-- Train loss:0.84 accuracy:0.78 precision:0.59 recall:0.76 f1:0.65\n","Epoch 106-- Valid loss:0.80 accuracy:0.79 precision:0.64 recall:0.74 f1:0.67\n","Epoch 107-- Train loss:0.84 accuracy:0.76 precision:0.56 recall:0.72 f1:0.61\n","Epoch 107-- Valid loss:0.79 accuracy:0.80 precision:0.65 recall:0.75 f1:0.68\n","Epoch 108-- Train loss:0.84 accuracy:0.77 precision:0.58 recall:0.73 f1:0.63\n","Epoch 108-- Valid loss:0.79 accuracy:0.80 precision:0.66 recall:0.75 f1:0.69\n","Epoch 109-- Train loss:0.84 accuracy:0.78 precision:0.57 recall:0.72 f1:0.62\n","Epoch 109-- Valid loss:0.79 accuracy:0.80 precision:0.64 recall:0.74 f1:0.68\n","Epoch 110-- Train loss:0.84 accuracy:0.76 precision:0.56 recall:0.70 f1:0.60\n","Epoch 110-- Valid loss:0.78 accuracy:0.80 precision:0.64 recall:0.74 f1:0.68\n","Epoch 111-- Train loss:0.87 accuracy:0.78 precision:0.58 recall:0.73 f1:0.63\n","Epoch 111-- Valid loss:0.78 accuracy:0.79 precision:0.65 recall:0.74 f1:0.67\n","Epoch 112-- Train loss:0.84 accuracy:0.77 precision:0.56 recall:0.72 f1:0.61\n","Epoch 112-- Valid loss:0.78 accuracy:0.79 precision:0.66 recall:0.74 f1:0.68\n","Epoch 113-- Train loss:0.81 accuracy:0.78 precision:0.57 recall:0.72 f1:0.62\n","Epoch 113-- Valid loss:0.77 accuracy:0.80 precision:0.66 recall:0.75 f1:0.69\n","Epoch 114-- Train loss:0.83 accuracy:0.78 precision:0.58 recall:0.73 f1:0.63\n","Epoch 114-- Valid loss:0.77 accuracy:0.79 precision:0.64 recall:0.74 f1:0.67\n","Epoch 115-- Train loss:0.78 accuracy:0.79 precision:0.60 recall:0.75 f1:0.65\n","Epoch 115-- Valid loss:0.77 accuracy:0.79 precision:0.64 recall:0.74 f1:0.67\n","Epoch 116-- Train loss:0.84 accuracy:0.77 precision:0.56 recall:0.70 f1:0.60\n","Epoch 116-- Valid loss:0.76 accuracy:0.80 precision:0.65 recall:0.75 f1:0.69\n","Epoch 117-- Train loss:0.83 accuracy:0.78 precision:0.58 recall:0.73 f1:0.63\n","Epoch 117-- Valid loss:0.76 accuracy:0.79 precision:0.65 recall:0.74 f1:0.67\n","Epoch 118-- Train loss:0.82 accuracy:0.77 precision:0.57 recall:0.72 f1:0.61\n","Epoch 118-- Valid loss:0.76 accuracy:0.79 precision:0.65 recall:0.74 f1:0.67\n","Epoch 119-- Train loss:0.81 accuracy:0.78 precision:0.58 recall:0.72 f1:0.62\n","Epoch 119-- Valid loss:0.76 accuracy:0.80 precision:0.65 recall:0.75 f1:0.69\n","Epoch 120-- Train loss:0.80 accuracy:0.78 precision:0.58 recall:0.73 f1:0.63\n","Epoch 120-- Valid loss:0.75 accuracy:0.81 precision:0.65 recall:0.77 f1:0.69\n","Epoch 121-- Train loss:0.79 accuracy:0.78 precision:0.59 recall:0.74 f1:0.64\n","Epoch 121-- Valid loss:0.75 accuracy:0.81 precision:0.65 recall:0.77 f1:0.69\n","Epoch 122-- Train loss:0.79 accuracy:0.78 precision:0.59 recall:0.74 f1:0.64\n","Epoch 122-- Valid loss:0.75 accuracy:0.80 precision:0.65 recall:0.75 f1:0.69\n","Epoch 123-- Train loss:0.79 accuracy:0.79 precision:0.59 recall:0.74 f1:0.64\n","Epoch 123-- Valid loss:0.74 accuracy:0.80 precision:0.66 recall:0.76 f1:0.69\n","Epoch 124-- Train loss:0.82 accuracy:0.78 precision:0.57 recall:0.72 f1:0.62\n","Epoch 124-- Valid loss:0.74 accuracy:0.80 precision:0.66 recall:0.76 f1:0.69\n","Epoch 125-- Train loss:0.82 accuracy:0.76 precision:0.55 recall:0.69 f1:0.59\n","Epoch 125-- Valid loss:0.74 accuracy:0.80 precision:0.65 recall:0.76 f1:0.69\n","Epoch 126-- Train loss:0.76 accuracy:0.79 precision:0.59 recall:0.75 f1:0.64\n","Epoch 126-- Valid loss:0.74 accuracy:0.81 precision:0.67 recall:0.77 f1:0.70\n","Epoch 127-- Train loss:0.77 accuracy:0.79 precision:0.60 recall:0.75 f1:0.65\n","Epoch 127-- Valid loss:0.73 accuracy:0.81 precision:0.65 recall:0.76 f1:0.69\n","Epoch 128-- Train loss:0.80 accuracy:0.78 precision:0.58 recall:0.72 f1:0.62\n","Epoch 128-- Valid loss:0.73 accuracy:0.81 precision:0.65 recall:0.75 f1:0.69\n","Epoch 129-- Train loss:0.75 accuracy:0.80 precision:0.60 recall:0.75 f1:0.65\n","Epoch 129-- Valid loss:0.73 accuracy:0.80 precision:0.66 recall:0.75 f1:0.68\n","Epoch 130-- Train loss:0.80 accuracy:0.78 precision:0.59 recall:0.74 f1:0.64\n","Epoch 130-- Valid loss:0.72 accuracy:0.82 precision:0.67 recall:0.77 f1:0.71\n","Epoch 131-- Train loss:0.79 accuracy:0.80 precision:0.61 recall:0.76 f1:0.66\n","Epoch 131-- Valid loss:0.72 accuracy:0.80 precision:0.66 recall:0.76 f1:0.69\n","Epoch 132-- Train loss:0.78 accuracy:0.78 precision:0.58 recall:0.73 f1:0.63\n","Epoch 132-- Valid loss:0.72 accuracy:0.81 precision:0.66 recall:0.77 f1:0.70\n","Epoch 133-- Train loss:0.77 accuracy:0.78 precision:0.59 recall:0.74 f1:0.64\n","Epoch 133-- Valid loss:0.72 accuracy:0.82 precision:0.67 recall:0.78 f1:0.71\n","Epoch 134-- Train loss:0.77 accuracy:0.80 precision:0.61 recall:0.77 f1:0.67\n","Epoch 134-- Valid loss:0.71 accuracy:0.82 precision:0.67 recall:0.78 f1:0.71\n","Epoch 135-- Train loss:0.80 accuracy:0.78 precision:0.59 recall:0.74 f1:0.63\n","Epoch 135-- Valid loss:0.71 accuracy:0.82 precision:0.67 recall:0.77 f1:0.71\n","Epoch 136-- Train loss:0.74 accuracy:0.80 precision:0.62 recall:0.77 f1:0.67\n","Epoch 136-- Valid loss:0.71 accuracy:0.81 precision:0.67 recall:0.77 f1:0.70\n","Epoch 137-- Train loss:0.78 accuracy:0.79 precision:0.61 recall:0.76 f1:0.66\n","Epoch 137-- Valid loss:0.71 accuracy:0.80 precision:0.66 recall:0.76 f1:0.69\n","Epoch 138-- Train loss:0.75 accuracy:0.78 precision:0.59 recall:0.75 f1:0.64\n","Epoch 138-- Valid loss:0.70 accuracy:0.81 precision:0.66 recall:0.77 f1:0.70\n","Epoch 139-- Train loss:0.79 accuracy:0.79 precision:0.60 recall:0.74 f1:0.65\n","Epoch 139-- Valid loss:0.70 accuracy:0.81 precision:0.67 recall:0.77 f1:0.70\n","Epoch 140-- Train loss:0.77 accuracy:0.79 precision:0.60 recall:0.76 f1:0.66\n","Epoch 140-- Valid loss:0.70 accuracy:0.82 precision:0.67 recall:0.77 f1:0.71\n","Epoch 141-- Train loss:0.75 accuracy:0.79 precision:0.60 recall:0.75 f1:0.65\n","Epoch 141-- Valid loss:0.69 accuracy:0.81 precision:0.67 recall:0.77 f1:0.70\n","Epoch 142-- Train loss:0.74 accuracy:0.80 precision:0.61 recall:0.75 f1:0.65\n","Epoch 142-- Valid loss:0.69 accuracy:0.82 precision:0.68 recall:0.77 f1:0.71\n","Epoch 143-- Train loss:0.72 accuracy:0.81 precision:0.63 recall:0.78 f1:0.68\n","Epoch 143-- Valid loss:0.69 accuracy:0.82 precision:0.67 recall:0.78 f1:0.71\n","Epoch 144-- Train loss:0.76 accuracy:0.80 precision:0.60 recall:0.74 f1:0.65\n","Epoch 144-- Valid loss:0.69 accuracy:0.82 precision:0.67 recall:0.78 f1:0.71\n","Epoch 145-- Train loss:0.74 accuracy:0.80 precision:0.62 recall:0.76 f1:0.67\n","Epoch 145-- Valid loss:0.69 accuracy:0.81 precision:0.66 recall:0.77 f1:0.70\n","Epoch 146-- Train loss:0.74 accuracy:0.80 precision:0.62 recall:0.76 f1:0.66\n","Epoch 146-- Valid loss:0.68 accuracy:0.81 precision:0.67 recall:0.77 f1:0.70\n","Epoch 147-- Train loss:0.73 accuracy:0.81 precision:0.63 recall:0.79 f1:0.69\n","Epoch 147-- Valid loss:0.68 accuracy:0.82 precision:0.67 recall:0.77 f1:0.71\n","Epoch 148-- Train loss:0.75 accuracy:0.80 precision:0.61 recall:0.75 f1:0.65\n","Epoch 148-- Valid loss:0.68 accuracy:0.82 precision:0.68 recall:0.78 f1:0.71\n","Epoch 149-- Train loss:0.72 accuracy:0.80 precision:0.63 recall:0.78 f1:0.68\n","Epoch 149-- Valid loss:0.68 accuracy:0.82 precision:0.68 recall:0.78 f1:0.71\n","Epoch 150-- Train loss:0.73 accuracy:0.81 precision:0.63 recall:0.78 f1:0.68\n","Epoch 150-- Valid loss:0.67 accuracy:0.82 precision:0.67 recall:0.77 f1:0.71\n","Epoch 151-- Train loss:0.72 accuracy:0.80 precision:0.61 recall:0.76 f1:0.66\n","Epoch 151-- Valid loss:0.67 accuracy:0.82 precision:0.67 recall:0.77 f1:0.71\n","Epoch 152-- Train loss:0.75 accuracy:0.80 precision:0.61 recall:0.76 f1:0.66\n","Epoch 152-- Valid loss:0.67 accuracy:0.81 precision:0.67 recall:0.77 f1:0.71\n","Epoch 153-- Train loss:0.76 accuracy:0.80 precision:0.61 recall:0.75 f1:0.66\n","Epoch 153-- Valid loss:0.67 accuracy:0.81 precision:0.67 recall:0.77 f1:0.71\n","Epoch 154-- Train loss:0.70 accuracy:0.81 precision:0.62 recall:0.77 f1:0.68\n","Epoch 154-- Valid loss:0.66 accuracy:0.82 precision:0.67 recall:0.77 f1:0.71\n","Epoch 155-- Train loss:0.71 accuracy:0.80 precision:0.62 recall:0.77 f1:0.67\n","Epoch 155-- Valid loss:0.66 accuracy:0.82 precision:0.67 recall:0.78 f1:0.71\n","Epoch 156-- Train loss:0.73 accuracy:0.80 precision:0.61 recall:0.77 f1:0.67\n","Epoch 156-- Valid loss:0.66 accuracy:0.83 precision:0.68 recall:0.78 f1:0.72\n","Epoch 157-- Train loss:0.70 accuracy:0.80 precision:0.60 recall:0.76 f1:0.66\n","Epoch 157-- Valid loss:0.66 accuracy:0.82 precision:0.68 recall:0.78 f1:0.72\n","Epoch 158-- Train loss:0.73 accuracy:0.79 precision:0.60 recall:0.75 f1:0.65\n","Epoch 158-- Valid loss:0.66 accuracy:0.81 precision:0.68 recall:0.77 f1:0.71\n","Epoch 159-- Train loss:0.69 accuracy:0.81 precision:0.62 recall:0.78 f1:0.67\n","Epoch 159-- Valid loss:0.66 accuracy:0.82 precision:0.68 recall:0.78 f1:0.71\n","Epoch 160-- Train loss:0.69 accuracy:0.81 precision:0.64 recall:0.79 f1:0.69\n","Epoch 160-- Valid loss:0.65 accuracy:0.83 precision:0.69 recall:0.79 f1:0.73\n","Epoch 161-- Train loss:0.66 accuracy:0.81 precision:0.63 recall:0.78 f1:0.68\n","Epoch 161-- Valid loss:0.65 accuracy:0.84 precision:0.70 recall:0.81 f1:0.74\n","Epoch 162-- Train loss:0.70 accuracy:0.82 precision:0.64 recall:0.78 f1:0.69\n","Epoch 162-- Valid loss:0.65 accuracy:0.84 precision:0.71 recall:0.81 f1:0.75\n","Epoch 163-- Train loss:0.70 accuracy:0.81 precision:0.62 recall:0.76 f1:0.67\n","Epoch 163-- Valid loss:0.65 accuracy:0.83 precision:0.69 recall:0.78 f1:0.72\n","Epoch 164-- Train loss:0.70 accuracy:0.80 precision:0.62 recall:0.76 f1:0.66\n","Epoch 164-- Valid loss:0.65 accuracy:0.82 precision:0.68 recall:0.78 f1:0.71\n","Epoch 165-- Train loss:0.70 accuracy:0.80 precision:0.62 recall:0.76 f1:0.66\n","Epoch 165-- Valid loss:0.64 accuracy:0.82 precision:0.68 recall:0.79 f1:0.72\n","Epoch 166-- Train loss:0.68 accuracy:0.80 precision:0.62 recall:0.77 f1:0.67\n","Epoch 166-- Valid loss:0.64 accuracy:0.84 precision:0.70 recall:0.81 f1:0.74\n","Epoch 167-- Train loss:0.68 accuracy:0.81 precision:0.62 recall:0.78 f1:0.68\n","Epoch 167-- Valid loss:0.64 accuracy:0.84 precision:0.70 recall:0.80 f1:0.73\n","Epoch 168-- Train loss:0.72 accuracy:0.79 precision:0.61 recall:0.75 f1:0.66\n","Epoch 168-- Valid loss:0.64 accuracy:0.84 precision:0.70 recall:0.81 f1:0.74\n","Epoch 169-- Train loss:0.71 accuracy:0.79 precision:0.60 recall:0.75 f1:0.66\n","Epoch 169-- Valid loss:0.64 accuracy:0.81 precision:0.67 recall:0.78 f1:0.71\n","Epoch 170-- Train loss:0.70 accuracy:0.81 precision:0.62 recall:0.77 f1:0.67\n","Epoch 170-- Valid loss:0.63 accuracy:0.81 precision:0.68 recall:0.78 f1:0.71\n","Epoch 171-- Train loss:0.68 accuracy:0.80 precision:0.62 recall:0.77 f1:0.67\n","Epoch 171-- Valid loss:0.63 accuracy:0.82 precision:0.69 recall:0.79 f1:0.72\n","Epoch 172-- Train loss:0.67 accuracy:0.81 precision:0.63 recall:0.79 f1:0.69\n","Epoch 172-- Valid loss:0.63 accuracy:0.83 precision:0.69 recall:0.80 f1:0.73\n","Epoch 173-- Train loss:0.68 accuracy:0.80 precision:0.62 recall:0.77 f1:0.67\n","Epoch 173-- Valid loss:0.63 accuracy:0.83 precision:0.70 recall:0.80 f1:0.74\n","Epoch 174-- Train loss:0.66 accuracy:0.82 precision:0.65 recall:0.78 f1:0.70\n","Epoch 174-- Valid loss:0.63 accuracy:0.82 precision:0.69 recall:0.79 f1:0.73\n","Epoch 175-- Train loss:0.69 accuracy:0.81 precision:0.63 recall:0.78 f1:0.69\n","Epoch 175-- Valid loss:0.63 accuracy:0.82 precision:0.70 recall:0.79 f1:0.73\n","Epoch 176-- Train loss:0.70 accuracy:0.81 precision:0.63 recall:0.78 f1:0.68\n","Epoch 176-- Valid loss:0.62 accuracy:0.82 precision:0.69 recall:0.80 f1:0.73\n","Epoch 177-- Train loss:0.67 accuracy:0.80 precision:0.60 recall:0.76 f1:0.66\n","Epoch 177-- Valid loss:0.62 accuracy:0.83 precision:0.70 recall:0.80 f1:0.74\n","Epoch 178-- Train loss:0.71 accuracy:0.81 precision:0.63 recall:0.78 f1:0.68\n","Epoch 178-- Valid loss:0.62 accuracy:0.85 precision:0.72 recall:0.82 f1:0.75\n","Epoch 179-- Train loss:0.67 accuracy:0.81 precision:0.62 recall:0.76 f1:0.67\n","Epoch 179-- Valid loss:0.62 accuracy:0.85 precision:0.72 recall:0.82 f1:0.75\n","Epoch 180-- Train loss:0.68 accuracy:0.81 precision:0.63 recall:0.77 f1:0.68\n","Epoch 180-- Valid loss:0.62 accuracy:0.85 precision:0.72 recall:0.82 f1:0.75\n","Epoch 181-- Train loss:0.69 accuracy:0.82 precision:0.65 recall:0.79 f1:0.70\n","Epoch 181-- Valid loss:0.61 accuracy:0.83 precision:0.70 recall:0.80 f1:0.74\n","Epoch 182-- Train loss:0.63 accuracy:0.82 precision:0.66 recall:0.81 f1:0.71\n","Epoch 182-- Valid loss:0.61 accuracy:0.84 precision:0.71 recall:0.81 f1:0.75\n","Epoch 183-- Train loss:0.68 accuracy:0.81 precision:0.64 recall:0.79 f1:0.69\n","Epoch 183-- Valid loss:0.61 accuracy:0.84 precision:0.71 recall:0.81 f1:0.74\n","Epoch 184-- Train loss:0.65 accuracy:0.81 precision:0.63 recall:0.77 f1:0.68\n","Epoch 184-- Valid loss:0.61 accuracy:0.85 precision:0.72 recall:0.82 f1:0.75\n","Epoch 185-- Train loss:0.64 accuracy:0.83 precision:0.66 recall:0.79 f1:0.70\n","Epoch 185-- Valid loss:0.61 accuracy:0.84 precision:0.71 recall:0.81 f1:0.75\n","Epoch 186-- Train loss:0.66 accuracy:0.81 precision:0.63 recall:0.77 f1:0.68\n","Epoch 186-- Valid loss:0.61 accuracy:0.83 precision:0.70 recall:0.80 f1:0.74\n","Epoch 187-- Train loss:0.64 accuracy:0.81 precision:0.63 recall:0.79 f1:0.69\n","Epoch 187-- Valid loss:0.60 accuracy:0.83 precision:0.70 recall:0.80 f1:0.74\n","Epoch 188-- Train loss:0.66 accuracy:0.81 precision:0.64 recall:0.79 f1:0.69\n","Epoch 188-- Valid loss:0.60 accuracy:0.84 precision:0.70 recall:0.81 f1:0.74\n","Epoch 189-- Train loss:0.65 accuracy:0.82 precision:0.64 recall:0.78 f1:0.70\n","Epoch 189-- Valid loss:0.60 accuracy:0.85 precision:0.72 recall:0.82 f1:0.75\n","Epoch 190-- Train loss:0.62 accuracy:0.82 precision:0.65 recall:0.79 f1:0.70\n","Epoch 190-- Valid loss:0.60 accuracy:0.84 precision:0.71 recall:0.81 f1:0.75\n","Epoch 191-- Train loss:0.66 accuracy:0.81 precision:0.63 recall:0.77 f1:0.68\n","Epoch 191-- Valid loss:0.60 accuracy:0.82 precision:0.69 recall:0.80 f1:0.73\n","Epoch 192-- Train loss:0.63 accuracy:0.83 precision:0.65 recall:0.80 f1:0.70\n","Epoch 192-- Valid loss:0.60 accuracy:0.82 precision:0.70 recall:0.80 f1:0.73\n","Epoch 193-- Train loss:0.62 accuracy:0.82 precision:0.64 recall:0.80 f1:0.70\n","Epoch 193-- Valid loss:0.60 accuracy:0.84 precision:0.71 recall:0.81 f1:0.75\n","Epoch 194-- Train loss:0.63 accuracy:0.83 precision:0.66 recall:0.79 f1:0.71\n","Epoch 194-- Valid loss:0.60 accuracy:0.85 precision:0.72 recall:0.82 f1:0.75\n","Epoch 195-- Train loss:0.64 accuracy:0.82 precision:0.65 recall:0.80 f1:0.71\n","Epoch 195-- Valid loss:0.59 accuracy:0.84 precision:0.71 recall:0.81 f1:0.75\n","Epoch 196-- Train loss:0.61 accuracy:0.82 precision:0.65 recall:0.80 f1:0.70\n","Epoch 196-- Valid loss:0.59 accuracy:0.84 precision:0.70 recall:0.81 f1:0.74\n","Epoch 197-- Train loss:0.61 accuracy:0.82 precision:0.64 recall:0.78 f1:0.69\n","Epoch 197-- Valid loss:0.59 accuracy:0.84 precision:0.71 recall:0.81 f1:0.74\n","Epoch 198-- Train loss:0.63 accuracy:0.82 precision:0.65 recall:0.80 f1:0.70\n","Epoch 198-- Valid loss:0.59 accuracy:0.85 precision:0.72 recall:0.82 f1:0.75\n","Epoch 199-- Train loss:0.64 accuracy:0.81 precision:0.64 recall:0.79 f1:0.69\n","Epoch 199-- Valid loss:0.59 accuracy:0.85 precision:0.72 recall:0.82 f1:0.75\n","Epoch 200-- Train loss:0.62 accuracy:0.83 precision:0.67 recall:0.81 f1:0.72\n","Epoch 200-- Valid loss:0.59 accuracy:0.85 precision:0.72 recall:0.82 f1:0.75\n","Epoch 201-- Train loss:0.65 accuracy:0.83 precision:0.65 recall:0.80 f1:0.71\n","Epoch 201-- Valid loss:0.58 accuracy:0.84 precision:0.71 recall:0.81 f1:0.75\n","Epoch 202-- Train loss:0.64 accuracy:0.82 precision:0.64 recall:0.78 f1:0.69\n","Epoch 202-- Valid loss:0.58 accuracy:0.84 precision:0.71 recall:0.81 f1:0.75\n","Epoch 203-- Train loss:0.67 accuracy:0.81 precision:0.64 recall:0.79 f1:0.69\n","Epoch 203-- Valid loss:0.58 accuracy:0.83 precision:0.71 recall:0.80 f1:0.74\n","Epoch 204-- Train loss:0.63 accuracy:0.82 precision:0.65 recall:0.79 f1:0.70\n","Epoch 204-- Valid loss:0.58 accuracy:0.84 precision:0.71 recall:0.81 f1:0.75\n","Epoch 205-- Train loss:0.62 accuracy:0.83 precision:0.65 recall:0.80 f1:0.71\n","Epoch 205-- Valid loss:0.58 accuracy:0.84 precision:0.71 recall:0.81 f1:0.75\n","Epoch 206-- Train loss:0.61 accuracy:0.83 precision:0.66 recall:0.80 f1:0.71\n","Epoch 206-- Valid loss:0.58 accuracy:0.85 precision:0.72 recall:0.82 f1:0.76\n","Epoch 207-- Train loss:0.61 accuracy:0.83 precision:0.66 recall:0.81 f1:0.71\n","Epoch 207-- Valid loss:0.58 accuracy:0.84 precision:0.72 recall:0.81 f1:0.75\n","Epoch 208-- Train loss:0.59 accuracy:0.84 precision:0.68 recall:0.82 f1:0.73\n","Epoch 208-- Valid loss:0.57 accuracy:0.85 precision:0.72 recall:0.82 f1:0.75\n","Epoch 209-- Train loss:0.63 accuracy:0.83 precision:0.66 recall:0.79 f1:0.71\n","Epoch 209-- Valid loss:0.57 accuracy:0.84 precision:0.71 recall:0.81 f1:0.75\n","Epoch 210-- Train loss:0.62 accuracy:0.81 precision:0.63 recall:0.78 f1:0.69\n","Epoch 210-- Valid loss:0.57 accuracy:0.83 precision:0.70 recall:0.80 f1:0.73\n","Epoch 211-- Train loss:0.62 accuracy:0.82 precision:0.65 recall:0.80 f1:0.71\n","Epoch 211-- Valid loss:0.57 accuracy:0.84 precision:0.71 recall:0.82 f1:0.75\n","Epoch 212-- Train loss:0.59 accuracy:0.83 precision:0.67 recall:0.83 f1:0.73\n","Epoch 212-- Valid loss:0.57 accuracy:0.83 precision:0.71 recall:0.82 f1:0.74\n","Epoch 213-- Train loss:0.59 accuracy:0.83 precision:0.68 recall:0.82 f1:0.73\n","Epoch 213-- Valid loss:0.57 accuracy:0.83 precision:0.71 recall:0.82 f1:0.74\n","Epoch 214-- Train loss:0.62 accuracy:0.82 precision:0.65 recall:0.80 f1:0.70\n","Epoch 214-- Valid loss:0.57 accuracy:0.83 precision:0.71 recall:0.82 f1:0.75\n","Epoch 215-- Train loss:0.62 accuracy:0.82 precision:0.65 recall:0.80 f1:0.70\n","Epoch 215-- Valid loss:0.56 accuracy:0.84 precision:0.71 recall:0.82 f1:0.75\n","Epoch 216-- Train loss:0.64 accuracy:0.82 precision:0.64 recall:0.78 f1:0.69\n","Epoch 216-- Valid loss:0.56 accuracy:0.85 precision:0.72 recall:0.82 f1:0.76\n","Epoch 217-- Train loss:0.62 accuracy:0.82 precision:0.65 recall:0.80 f1:0.70\n","Epoch 217-- Valid loss:0.56 accuracy:0.84 precision:0.71 recall:0.82 f1:0.75\n","Epoch 218-- Train loss:0.63 accuracy:0.82 precision:0.64 recall:0.79 f1:0.69\n","Epoch 218-- Valid loss:0.56 accuracy:0.85 precision:0.72 recall:0.82 f1:0.76\n","Epoch 219-- Train loss:0.61 accuracy:0.83 precision:0.67 recall:0.81 f1:0.72\n","Epoch 219-- Valid loss:0.56 accuracy:0.85 precision:0.72 recall:0.83 f1:0.76\n","Epoch 220-- Train loss:0.62 accuracy:0.83 precision:0.66 recall:0.80 f1:0.71\n","Epoch 220-- Valid loss:0.56 accuracy:0.84 precision:0.72 recall:0.83 f1:0.76\n","Epoch 221-- Train loss:0.60 accuracy:0.83 precision:0.66 recall:0.81 f1:0.71\n","Epoch 221-- Valid loss:0.55 accuracy:0.84 precision:0.72 recall:0.83 f1:0.76\n","Epoch 222-- Train loss:0.61 accuracy:0.83 precision:0.66 recall:0.80 f1:0.71\n","Epoch 222-- Valid loss:0.55 accuracy:0.85 precision:0.73 recall:0.84 f1:0.77\n","Epoch 223-- Train loss:0.60 accuracy:0.82 precision:0.65 recall:0.79 f1:0.70\n","Epoch 223-- Valid loss:0.55 accuracy:0.85 precision:0.72 recall:0.83 f1:0.76\n","Epoch 224-- Train loss:0.57 accuracy:0.85 precision:0.70 recall:0.86 f1:0.76\n","Epoch 224-- Valid loss:0.55 accuracy:0.85 precision:0.72 recall:0.83 f1:0.76\n","Epoch 225-- Train loss:0.61 accuracy:0.83 precision:0.67 recall:0.81 f1:0.72\n","Epoch 225-- Valid loss:0.55 accuracy:0.85 precision:0.72 recall:0.83 f1:0.76\n","Epoch 226-- Train loss:0.56 accuracy:0.83 precision:0.67 recall:0.82 f1:0.72\n","Epoch 226-- Valid loss:0.55 accuracy:0.85 precision:0.72 recall:0.82 f1:0.76\n","Epoch 227-- Train loss:0.58 accuracy:0.84 precision:0.68 recall:0.82 f1:0.73\n","Epoch 227-- Valid loss:0.55 accuracy:0.85 precision:0.72 recall:0.82 f1:0.76\n","Epoch 228-- Train loss:0.59 accuracy:0.83 precision:0.66 recall:0.81 f1:0.72\n","Epoch 228-- Valid loss:0.54 accuracy:0.85 precision:0.72 recall:0.82 f1:0.76\n","Epoch 229-- Train loss:0.57 accuracy:0.83 precision:0.67 recall:0.81 f1:0.72\n","Epoch 229-- Valid loss:0.54 accuracy:0.85 precision:0.73 recall:0.84 f1:0.77\n","Epoch 230-- Train loss:0.58 accuracy:0.83 precision:0.67 recall:0.83 f1:0.73\n","Epoch 230-- Valid loss:0.54 accuracy:0.84 precision:0.72 recall:0.83 f1:0.76\n","Epoch 231-- Train loss:0.57 accuracy:0.82 precision:0.66 recall:0.82 f1:0.72\n","Epoch 231-- Valid loss:0.54 accuracy:0.85 precision:0.73 recall:0.84 f1:0.77\n","Epoch 232-- Train loss:0.61 accuracy:0.81 precision:0.64 recall:0.78 f1:0.69\n","Epoch 232-- Valid loss:0.54 accuracy:0.85 precision:0.74 recall:0.84 f1:0.77\n","Epoch 233-- Train loss:0.57 accuracy:0.83 precision:0.66 recall:0.81 f1:0.71\n","Epoch 233-- Valid loss:0.54 accuracy:0.85 precision:0.74 recall:0.84 f1:0.77\n","Epoch 234-- Train loss:0.58 accuracy:0.83 precision:0.66 recall:0.81 f1:0.71\n","Epoch 234-- Valid loss:0.54 accuracy:0.85 precision:0.74 recall:0.84 f1:0.77\n","Epoch 235-- Train loss:0.59 accuracy:0.81 precision:0.63 recall:0.78 f1:0.68\n","Epoch 235-- Valid loss:0.54 accuracy:0.86 precision:0.74 recall:0.84 f1:0.78\n","Epoch 236-- Train loss:0.57 accuracy:0.83 precision:0.67 recall:0.81 f1:0.72\n","Epoch 236-- Valid loss:0.54 accuracy:0.85 precision:0.73 recall:0.84 f1:0.77\n","Epoch 237-- Train loss:0.59 accuracy:0.83 precision:0.66 recall:0.81 f1:0.72\n","Epoch 237-- Valid loss:0.53 accuracy:0.84 precision:0.73 recall:0.84 f1:0.76\n","Epoch 238-- Train loss:0.56 accuracy:0.84 precision:0.68 recall:0.83 f1:0.73\n","Epoch 238-- Valid loss:0.53 accuracy:0.84 precision:0.73 recall:0.84 f1:0.76\n","Epoch 239-- Train loss:0.57 accuracy:0.83 precision:0.66 recall:0.81 f1:0.72\n","Epoch 239-- Valid loss:0.53 accuracy:0.84 precision:0.72 recall:0.84 f1:0.76\n","Epoch 240-- Train loss:0.57 accuracy:0.84 precision:0.67 recall:0.82 f1:0.72\n","Epoch 240-- Valid loss:0.53 accuracy:0.86 precision:0.74 recall:0.84 f1:0.78\n","Epoch 241-- Train loss:0.55 accuracy:0.83 precision:0.67 recall:0.82 f1:0.72\n","Epoch 241-- Valid loss:0.53 accuracy:0.86 precision:0.74 recall:0.83 f1:0.78\n","Epoch 242-- Train loss:0.57 accuracy:0.83 precision:0.66 recall:0.83 f1:0.71\n","Epoch 242-- Valid loss:0.53 accuracy:0.87 precision:0.75 recall:0.84 f1:0.79\n","Epoch 243-- Train loss:0.56 accuracy:0.83 precision:0.67 recall:0.82 f1:0.72\n","Epoch 243-- Valid loss:0.53 accuracy:0.87 precision:0.76 recall:0.85 f1:0.79\n","Epoch 244-- Train loss:0.55 accuracy:0.84 precision:0.69 recall:0.83 f1:0.74\n","Epoch 244-- Valid loss:0.53 accuracy:0.85 precision:0.74 recall:0.84 f1:0.78\n","Epoch 245-- Train loss:0.58 accuracy:0.84 precision:0.68 recall:0.82 f1:0.74\n","Epoch 245-- Valid loss:0.52 accuracy:0.85 precision:0.74 recall:0.84 f1:0.77\n","Epoch 246-- Train loss:0.55 accuracy:0.83 precision:0.67 recall:0.82 f1:0.73\n","Epoch 246-- Valid loss:0.52 accuracy:0.85 precision:0.74 recall:0.85 f1:0.78\n","Epoch 247-- Train loss:0.59 accuracy:0.83 precision:0.67 recall:0.81 f1:0.72\n","Epoch 247-- Valid loss:0.52 accuracy:0.86 precision:0.75 recall:0.85 f1:0.79\n","Epoch 248-- Train loss:0.55 accuracy:0.84 precision:0.68 recall:0.83 f1:0.73\n","Epoch 248-- Valid loss:0.52 accuracy:0.86 precision:0.75 recall:0.85 f1:0.78\n","Epoch 249-- Train loss:0.56 accuracy:0.83 precision:0.67 recall:0.81 f1:0.72\n","Epoch 249-- Valid loss:0.52 accuracy:0.86 precision:0.75 recall:0.85 f1:0.79\n","Epoch 0-- Train loss:1.79 accuracy:0.05 precision:0.02 recall:0.14 f1:0.03\n","Epoch 0-- Valid loss:1.79 accuracy:0.11 precision:0.02 recall:0.17 f1:0.03\n","Epoch 1-- Train loss:1.79 accuracy:0.04 precision:0.04 recall:0.14 f1:0.02\n","Epoch 1-- Valid loss:1.79 accuracy:0.11 precision:0.02 recall:0.17 f1:0.03\n","Epoch 2-- Train loss:1.79 accuracy:0.04 precision:0.02 recall:0.14 f1:0.03\n","Epoch 2-- Valid loss:1.79 accuracy:0.11 precision:0.02 recall:0.17 f1:0.03\n","Epoch 3-- Train loss:1.79 accuracy:0.04 precision:0.03 recall:0.14 f1:0.03\n","Epoch 3-- Valid loss:1.79 accuracy:0.07 precision:0.02 recall:0.21 f1:0.04\n","Epoch 4-- Train loss:1.79 accuracy:0.04 precision:0.03 recall:0.16 f1:0.03\n","Epoch 4-- Valid loss:1.79 accuracy:0.09 precision:0.03 recall:0.19 f1:0.05\n","Epoch 5-- Train loss:1.79 accuracy:0.05 precision:0.06 recall:0.17 f1:0.05\n","Epoch 5-- Valid loss:1.78 accuracy:0.08 precision:0.03 recall:0.21 f1:0.05\n","Epoch 6-- Train loss:1.78 accuracy:0.04 precision:0.04 recall:0.16 f1:0.03\n","Epoch 6-- Valid loss:1.78 accuracy:0.06 precision:0.02 recall:0.18 f1:0.03\n","Epoch 7-- Train loss:1.77 accuracy:0.11 precision:0.18 recall:0.19 f1:0.05\n","Epoch 7-- Valid loss:1.77 accuracy:0.28 precision:0.18 recall:0.23 f1:0.11\n","Epoch 8-- Train loss:1.76 accuracy:0.34 precision:0.18 recall:0.24 f1:0.12\n","Epoch 8-- Valid loss:1.75 accuracy:0.41 precision:0.17 recall:0.26 f1:0.14\n","Epoch 9-- Train loss:1.73 accuracy:0.43 precision:0.17 recall:0.26 f1:0.14\n","Epoch 9-- Valid loss:1.71 accuracy:0.49 precision:0.17 recall:0.28 f1:0.16\n","Epoch 10-- Train loss:1.68 accuracy:0.49 precision:0.17 recall:0.28 f1:0.15\n","Epoch 10-- Valid loss:1.66 accuracy:0.53 precision:0.17 recall:0.29 f1:0.17\n","Epoch 11-- Train loss:1.63 accuracy:0.53 precision:0.17 recall:0.28 f1:0.16\n","Epoch 11-- Valid loss:1.65 accuracy:0.55 precision:0.17 recall:0.29 f1:0.17\n","Epoch 12-- Train loss:1.60 accuracy:0.54 precision:0.17 recall:0.28 f1:0.16\n","Epoch 12-- Valid loss:1.63 accuracy:0.55 precision:0.17 recall:0.29 f1:0.17\n","Epoch 13-- Train loss:1.57 accuracy:0.55 precision:0.32 recall:0.32 f1:0.22\n","Epoch 13-- Valid loss:1.59 accuracy:0.55 precision:0.29 recall:0.30 f1:0.21\n","Epoch 14-- Train loss:1.56 accuracy:0.57 precision:0.26 recall:0.34 f1:0.26\n","Epoch 14-- Valid loss:1.57 accuracy:0.59 precision:0.29 recall:0.36 f1:0.30\n","Epoch 15-- Train loss:1.54 accuracy:0.57 precision:0.24 recall:0.33 f1:0.25\n","Epoch 15-- Valid loss:1.55 accuracy:0.61 precision:0.31 recall:0.41 f1:0.34\n","Epoch 16-- Train loss:1.52 accuracy:0.59 precision:0.30 recall:0.39 f1:0.32\n","Epoch 16-- Valid loss:1.53 accuracy:0.60 precision:0.29 recall:0.40 f1:0.31\n","Epoch 17-- Train loss:1.51 accuracy:0.59 precision:0.32 recall:0.39 f1:0.29\n","Epoch 17-- Valid loss:1.51 accuracy:0.60 precision:0.31 recall:0.43 f1:0.33\n","Epoch 18-- Train loss:1.49 accuracy:0.58 precision:0.32 recall:0.38 f1:0.29\n","Epoch 18-- Valid loss:1.49 accuracy:0.59 precision:0.30 recall:0.42 f1:0.32\n","Epoch 19-- Train loss:1.47 accuracy:0.59 precision:0.31 recall:0.40 f1:0.31\n","Epoch 19-- Valid loss:1.47 accuracy:0.59 precision:0.30 recall:0.41 f1:0.32\n","Epoch 20-- Train loss:1.46 accuracy:0.59 precision:0.32 recall:0.41 f1:0.32\n","Epoch 20-- Valid loss:1.45 accuracy:0.59 precision:0.30 recall:0.42 f1:0.33\n","Epoch 21-- Train loss:1.44 accuracy:0.59 precision:0.37 recall:0.40 f1:0.31\n","Epoch 21-- Valid loss:1.43 accuracy:0.59 precision:0.30 recall:0.42 f1:0.33\n","Epoch 22-- Train loss:1.42 accuracy:0.60 precision:0.39 recall:0.43 f1:0.34\n","Epoch 22-- Valid loss:1.41 accuracy:0.60 precision:0.47 recall:0.42 f1:0.34\n","Epoch 23-- Train loss:1.39 accuracy:0.60 precision:0.41 recall:0.45 f1:0.36\n","Epoch 23-- Valid loss:1.39 accuracy:0.62 precision:0.49 recall:0.45 f1:0.36\n","Epoch 24-- Train loss:1.37 accuracy:0.60 precision:0.39 recall:0.45 f1:0.36\n","Epoch 24-- Valid loss:1.37 accuracy:0.64 precision:0.42 recall:0.47 f1:0.41\n","Epoch 25-- Train loss:1.36 accuracy:0.60 precision:0.39 recall:0.45 f1:0.38\n","Epoch 25-- Valid loss:1.35 accuracy:0.66 precision:0.49 recall:0.53 f1:0.46\n","Epoch 26-- Train loss:1.33 accuracy:0.62 precision:0.44 recall:0.48 f1:0.40\n","Epoch 26-- Valid loss:1.33 accuracy:0.64 precision:0.45 recall:0.48 f1:0.42\n","Epoch 27-- Train loss:1.28 accuracy:0.63 precision:0.43 recall:0.51 f1:0.42\n","Epoch 27-- Valid loss:1.31 accuracy:0.64 precision:0.42 recall:0.47 f1:0.41\n","Epoch 28-- Train loss:1.25 accuracy:0.63 precision:0.45 recall:0.52 f1:0.44\n","Epoch 28-- Valid loss:1.29 accuracy:0.66 precision:0.47 recall:0.51 f1:0.46\n","Epoch 29-- Train loss:1.23 accuracy:0.64 precision:0.46 recall:0.54 f1:0.46\n","Epoch 29-- Valid loss:1.28 accuracy:0.68 precision:0.54 recall:0.54 f1:0.50\n","Epoch 30-- Train loss:1.23 accuracy:0.64 precision:0.45 recall:0.53 f1:0.45\n","Epoch 30-- Valid loss:1.27 accuracy:0.69 precision:0.56 recall:0.55 f1:0.51\n","Epoch 31-- Train loss:1.24 accuracy:0.62 precision:0.45 recall:0.51 f1:0.44\n","Epoch 31-- Valid loss:1.23 accuracy:0.68 precision:0.51 recall:0.56 f1:0.50\n","Epoch 32-- Train loss:1.21 accuracy:0.64 precision:0.47 recall:0.52 f1:0.46\n","Epoch 32-- Valid loss:1.23 accuracy:0.68 precision:0.49 recall:0.55 f1:0.49\n","Epoch 33-- Train loss:1.19 accuracy:0.65 precision:0.47 recall:0.54 f1:0.47\n","Epoch 33-- Valid loss:1.24 accuracy:0.67 precision:0.49 recall:0.53 f1:0.48\n","Epoch 34-- Train loss:1.14 accuracy:0.65 precision:0.49 recall:0.57 f1:0.49\n","Epoch 34-- Valid loss:1.20 accuracy:0.69 precision:0.51 recall:0.56 f1:0.50\n","Epoch 35-- Train loss:1.13 accuracy:0.65 precision:0.49 recall:0.57 f1:0.49\n","Epoch 35-- Valid loss:1.18 accuracy:0.69 precision:0.54 recall:0.57 f1:0.52\n","Epoch 36-- Train loss:1.13 accuracy:0.65 precision:0.49 recall:0.57 f1:0.49\n","Epoch 36-- Valid loss:1.18 accuracy:0.70 precision:0.56 recall:0.57 f1:0.52\n","Epoch 37-- Train loss:1.14 accuracy:0.65 precision:0.48 recall:0.55 f1:0.48\n","Epoch 37-- Valid loss:1.16 accuracy:0.68 precision:0.52 recall:0.58 f1:0.51\n","Epoch 38-- Train loss:1.11 accuracy:0.66 precision:0.49 recall:0.57 f1:0.49\n","Epoch 38-- Valid loss:1.18 accuracy:0.68 precision:0.49 recall:0.56 f1:0.49\n","Epoch 39-- Train loss:1.07 accuracy:0.66 precision:0.48 recall:0.58 f1:0.50\n","Epoch 39-- Valid loss:1.14 accuracy:0.68 precision:0.53 recall:0.55 f1:0.50\n","Epoch 40-- Train loss:1.06 accuracy:0.67 precision:0.51 recall:0.60 f1:0.52\n","Epoch 40-- Valid loss:1.12 accuracy:0.70 precision:0.56 recall:0.59 f1:0.54\n","Epoch 41-- Train loss:1.05 accuracy:0.67 precision:0.51 recall:0.62 f1:0.53\n","Epoch 41-- Valid loss:1.11 accuracy:0.71 precision:0.57 recall:0.60 f1:0.55\n","Epoch 42-- Train loss:1.06 accuracy:0.66 precision:0.48 recall:0.57 f1:0.49\n","Epoch 42-- Valid loss:1.10 accuracy:0.70 precision:0.56 recall:0.62 f1:0.55\n","Epoch 43-- Train loss:1.03 accuracy:0.67 precision:0.51 recall:0.61 f1:0.52\n","Epoch 43-- Valid loss:1.10 accuracy:0.72 precision:0.57 recall:0.62 f1:0.56\n","Epoch 44-- Train loss:1.02 accuracy:0.68 precision:0.51 recall:0.61 f1:0.52\n","Epoch 44-- Valid loss:1.08 accuracy:0.71 precision:0.57 recall:0.63 f1:0.56\n","Epoch 45-- Train loss:0.99 accuracy:0.69 precision:0.54 recall:0.64 f1:0.55\n","Epoch 45-- Valid loss:1.08 accuracy:0.72 precision:0.57 recall:0.63 f1:0.56\n","Epoch 46-- Train loss:0.97 accuracy:0.69 precision:0.54 recall:0.65 f1:0.56\n","Epoch 46-- Valid loss:1.06 accuracy:0.72 precision:0.57 recall:0.62 f1:0.56\n","Epoch 47-- Train loss:0.96 accuracy:0.69 precision:0.54 recall:0.66 f1:0.56\n","Epoch 47-- Valid loss:1.05 accuracy:0.72 precision:0.57 recall:0.61 f1:0.55\n","Epoch 48-- Train loss:0.94 accuracy:0.69 precision:0.53 recall:0.65 f1:0.56\n","Epoch 48-- Valid loss:1.04 accuracy:0.73 precision:0.57 recall:0.62 f1:0.56\n","Epoch 49-- Train loss:0.93 accuracy:0.71 precision:0.55 recall:0.68 f1:0.58\n","Epoch 49-- Valid loss:1.03 accuracy:0.73 precision:0.59 recall:0.64 f1:0.58\n","Epoch 50-- Train loss:0.93 accuracy:0.71 precision:0.56 recall:0.69 f1:0.59\n","Epoch 50-- Valid loss:1.01 accuracy:0.73 precision:0.58 recall:0.64 f1:0.58\n","Epoch 51-- Train loss:0.90 accuracy:0.71 precision:0.57 recall:0.70 f1:0.59\n","Epoch 51-- Valid loss:1.00 accuracy:0.74 precision:0.59 recall:0.64 f1:0.58\n","Epoch 52-- Train loss:0.89 accuracy:0.72 precision:0.56 recall:0.69 f1:0.59\n","Epoch 52-- Valid loss:0.99 accuracy:0.74 precision:0.59 recall:0.65 f1:0.58\n","Epoch 53-- Train loss:0.86 accuracy:0.73 precision:0.59 recall:0.73 f1:0.62\n","Epoch 53-- Valid loss:0.98 accuracy:0.75 precision:0.60 recall:0.65 f1:0.59\n","Epoch 54-- Train loss:0.85 accuracy:0.73 precision:0.58 recall:0.72 f1:0.61\n","Epoch 54-- Valid loss:0.98 accuracy:0.76 precision:0.61 recall:0.66 f1:0.60\n","Epoch 55-- Train loss:0.84 accuracy:0.73 precision:0.59 recall:0.72 f1:0.62\n","Epoch 55-- Valid loss:0.96 accuracy:0.76 precision:0.61 recall:0.67 f1:0.60\n","Epoch 56-- Train loss:0.83 accuracy:0.73 precision:0.58 recall:0.72 f1:0.61\n","Epoch 56-- Valid loss:0.95 accuracy:0.77 precision:0.63 recall:0.69 f1:0.63\n","Epoch 57-- Train loss:0.82 accuracy:0.73 precision:0.57 recall:0.72 f1:0.61\n","Epoch 57-- Valid loss:0.94 accuracy:0.77 precision:0.64 recall:0.70 f1:0.63\n","Epoch 58-- Train loss:0.80 accuracy:0.74 precision:0.58 recall:0.72 f1:0.62\n","Epoch 58-- Valid loss:0.93 accuracy:0.77 precision:0.63 recall:0.70 f1:0.63\n","Epoch 59-- Train loss:0.78 accuracy:0.74 precision:0.58 recall:0.73 f1:0.62\n","Epoch 59-- Valid loss:0.92 accuracy:0.78 precision:0.65 recall:0.71 f1:0.65\n","Epoch 60-- Train loss:0.77 accuracy:0.74 precision:0.58 recall:0.73 f1:0.62\n","Epoch 60-- Valid loss:0.91 accuracy:0.78 precision:0.65 recall:0.73 f1:0.66\n","Epoch 61-- Train loss:0.76 accuracy:0.74 precision:0.58 recall:0.74 f1:0.62\n","Epoch 61-- Valid loss:0.90 accuracy:0.79 precision:0.66 recall:0.75 f1:0.68\n","Epoch 62-- Train loss:0.74 accuracy:0.75 precision:0.59 recall:0.74 f1:0.63\n","Epoch 62-- Valid loss:0.89 accuracy:0.79 precision:0.66 recall:0.75 f1:0.68\n","Epoch 63-- Train loss:0.72 accuracy:0.75 precision:0.59 recall:0.75 f1:0.63\n","Epoch 63-- Valid loss:0.88 accuracy:0.80 precision:0.67 recall:0.75 f1:0.68\n","Epoch 64-- Train loss:0.70 accuracy:0.76 precision:0.60 recall:0.76 f1:0.65\n","Epoch 64-- Valid loss:0.87 accuracy:0.81 precision:0.68 recall:0.76 f1:0.69\n","Epoch 65-- Train loss:0.69 accuracy:0.76 precision:0.61 recall:0.77 f1:0.66\n","Epoch 65-- Valid loss:0.87 accuracy:0.82 precision:0.69 recall:0.77 f1:0.70\n","Epoch 66-- Train loss:0.67 accuracy:0.76 precision:0.61 recall:0.78 f1:0.66\n","Epoch 66-- Valid loss:0.85 accuracy:0.79 precision:0.67 recall:0.75 f1:0.68\n","Epoch 67-- Train loss:0.64 accuracy:0.78 precision:0.63 recall:0.80 f1:0.68\n","Epoch 67-- Valid loss:0.85 accuracy:0.82 precision:0.68 recall:0.76 f1:0.69\n","Epoch 68-- Train loss:0.63 accuracy:0.76 precision:0.62 recall:0.80 f1:0.67\n","Epoch 68-- Valid loss:0.83 accuracy:0.80 precision:0.68 recall:0.76 f1:0.69\n","Epoch 69-- Train loss:0.63 accuracy:0.78 precision:0.62 recall:0.80 f1:0.67\n","Epoch 69-- Valid loss:0.82 accuracy:0.81 precision:0.68 recall:0.76 f1:0.69\n","Epoch 70-- Train loss:0.60 accuracy:0.77 precision:0.62 recall:0.81 f1:0.68\n","Epoch 70-- Valid loss:0.82 accuracy:0.82 precision:0.68 recall:0.76 f1:0.70\n","Epoch 71-- Train loss:0.58 accuracy:0.78 precision:0.63 recall:0.82 f1:0.69\n","Epoch 71-- Valid loss:0.80 accuracy:0.80 precision:0.65 recall:0.75 f1:0.68\n","Epoch 72-- Train loss:0.56 accuracy:0.79 precision:0.65 recall:0.83 f1:0.71\n","Epoch 72-- Valid loss:0.80 accuracy:0.82 precision:0.68 recall:0.76 f1:0.70\n","Epoch 73-- Train loss:0.56 accuracy:0.78 precision:0.64 recall:0.82 f1:0.70\n","Epoch 73-- Valid loss:0.78 accuracy:0.81 precision:0.67 recall:0.75 f1:0.69\n","Epoch 74-- Train loss:0.53 accuracy:0.79 precision:0.65 recall:0.85 f1:0.71\n","Epoch 74-- Valid loss:0.79 accuracy:0.82 precision:0.67 recall:0.75 f1:0.69\n","Epoch 75-- Train loss:0.52 accuracy:0.80 precision:0.65 recall:0.83 f1:0.71\n","Epoch 75-- Valid loss:0.77 accuracy:0.80 precision:0.65 recall:0.75 f1:0.68\n","Epoch 76-- Train loss:0.49 accuracy:0.80 precision:0.66 recall:0.85 f1:0.72\n","Epoch 76-- Valid loss:0.78 accuracy:0.82 precision:0.67 recall:0.76 f1:0.70\n","Epoch 77-- Train loss:0.49 accuracy:0.79 precision:0.65 recall:0.85 f1:0.72\n","Epoch 77-- Valid loss:0.75 accuracy:0.82 precision:0.67 recall:0.76 f1:0.70\n","Epoch 78-- Train loss:0.47 accuracy:0.81 precision:0.68 recall:0.86 f1:0.75\n","Epoch 78-- Valid loss:0.75 accuracy:0.82 precision:0.67 recall:0.76 f1:0.70\n","Epoch 79-- Train loss:0.45 accuracy:0.82 precision:0.68 recall:0.87 f1:0.75\n","Epoch 79-- Valid loss:0.74 accuracy:0.82 precision:0.67 recall:0.76 f1:0.69\n","Epoch 80-- Train loss:0.44 accuracy:0.81 precision:0.67 recall:0.85 f1:0.74\n","Epoch 80-- Valid loss:0.73 accuracy:0.84 precision:0.69 recall:0.78 f1:0.72\n","Epoch 81-- Train loss:0.43 accuracy:0.82 precision:0.69 recall:0.87 f1:0.75\n","Epoch 81-- Valid loss:0.72 accuracy:0.84 precision:0.71 recall:0.79 f1:0.72\n","Epoch 82-- Train loss:0.41 accuracy:0.83 precision:0.70 recall:0.88 f1:0.76\n","Epoch 82-- Valid loss:0.72 accuracy:0.85 precision:0.72 recall:0.80 f1:0.74\n","Epoch 83-- Train loss:0.40 accuracy:0.83 precision:0.70 recall:0.88 f1:0.77\n","Epoch 83-- Valid loss:0.71 accuracy:0.83 precision:0.71 recall:0.77 f1:0.72\n","Epoch 84-- Train loss:0.39 accuracy:0.83 precision:0.71 recall:0.88 f1:0.77\n","Epoch 84-- Valid loss:0.73 accuracy:0.83 precision:0.71 recall:0.76 f1:0.72\n","Epoch 85-- Train loss:0.40 accuracy:0.83 precision:0.69 recall:0.86 f1:0.75\n","Epoch 85-- Valid loss:0.73 accuracy:0.82 precision:0.70 recall:0.76 f1:0.71\n","Epoch 86-- Train loss:0.37 accuracy:0.84 precision:0.70 recall:0.88 f1:0.77\n","Epoch 86-- Valid loss:0.70 accuracy:0.82 precision:0.71 recall:0.76 f1:0.72\n","Epoch 87-- Train loss:0.37 accuracy:0.85 precision:0.72 recall:0.89 f1:0.79\n","Epoch 87-- Valid loss:0.69 accuracy:0.84 precision:0.72 recall:0.79 f1:0.74\n","Epoch 88-- Train loss:0.34 accuracy:0.84 precision:0.72 recall:0.90 f1:0.79\n","Epoch 88-- Valid loss:0.68 accuracy:0.84 precision:0.72 recall:0.81 f1:0.75\n","Epoch 89-- Train loss:0.35 accuracy:0.85 precision:0.72 recall:0.89 f1:0.78\n","Epoch 89-- Valid loss:0.72 accuracy:0.84 precision:0.70 recall:0.77 f1:0.73\n","Epoch 90-- Train loss:0.33 accuracy:0.84 precision:0.71 recall:0.89 f1:0.78\n","Epoch 90-- Valid loss:0.71 accuracy:0.83 precision:0.69 recall:0.78 f1:0.73\n","Epoch 91-- Train loss:0.33 accuracy:0.85 precision:0.72 recall:0.90 f1:0.79\n","Epoch 91-- Valid loss:0.70 accuracy:0.85 precision:0.72 recall:0.79 f1:0.74\n","Epoch 92-- Train loss:0.29 accuracy:0.85 precision:0.74 recall:0.91 f1:0.80\n","Epoch 92-- Valid loss:0.68 accuracy:0.85 precision:0.74 recall:0.80 f1:0.76\n","Epoch 93-- Train loss:0.28 accuracy:0.85 precision:0.73 recall:0.90 f1:0.80\n","Epoch 93-- Valid loss:0.68 accuracy:0.85 precision:0.74 recall:0.80 f1:0.75\n","Epoch 94-- Train loss:0.28 accuracy:0.86 precision:0.75 recall:0.92 f1:0.82\n","Epoch 94-- Valid loss:0.69 accuracy:0.84 precision:0.74 recall:0.79 f1:0.75\n","Epoch 95-- Train loss:0.26 accuracy:0.88 precision:0.77 recall:0.93 f1:0.83\n","Epoch 95-- Valid loss:0.70 accuracy:0.84 precision:0.73 recall:0.78 f1:0.75\n","Epoch 96-- Train loss:0.27 accuracy:0.87 precision:0.76 recall:0.92 f1:0.82\n","Epoch 96-- Valid loss:0.69 accuracy:0.85 precision:0.74 recall:0.79 f1:0.76\n","Epoch 97-- Train loss:0.26 accuracy:0.86 precision:0.75 recall:0.92 f1:0.82\n","Epoch 97-- Valid loss:0.70 accuracy:0.84 precision:0.71 recall:0.80 f1:0.75\n","Epoch 98-- Train loss:0.25 accuracy:0.87 precision:0.75 recall:0.92 f1:0.82\n","Epoch 98-- Valid loss:0.71 accuracy:0.85 precision:0.73 recall:0.80 f1:0.76\n","Epoch 99-- Train loss:0.25 accuracy:0.87 precision:0.76 recall:0.92 f1:0.82\n","Epoch 99-- Valid loss:0.70 accuracy:0.85 precision:0.74 recall:0.79 f1:0.76\n","Epoch 100-- Train loss:0.23 accuracy:0.88 precision:0.76 recall:0.93 f1:0.83\n","Epoch 100-- Valid loss:0.70 accuracy:0.85 precision:0.75 recall:0.81 f1:0.78\n","Epoch 101-- Train loss:0.23 accuracy:0.87 precision:0.76 recall:0.93 f1:0.83\n","Epoch 101-- Valid loss:0.71 accuracy:0.85 precision:0.75 recall:0.81 f1:0.77\n","Epoch 102-- Train loss:0.22 accuracy:0.89 precision:0.78 recall:0.93 f1:0.84\n","Epoch 102-- Valid loss:0.71 accuracy:0.85 precision:0.76 recall:0.81 f1:0.78\n","Epoch 103-- Train loss:0.22 accuracy:0.88 precision:0.78 recall:0.93 f1:0.84\n","Epoch 103-- Valid loss:0.69 accuracy:0.85 precision:0.74 recall:0.82 f1:0.78\n","Epoch 104-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 104-- Valid loss:0.70 accuracy:0.86 precision:0.76 recall:0.82 f1:0.79\n","Epoch 105-- Train loss:0.21 accuracy:0.88 precision:0.77 recall:0.93 f1:0.83\n","Epoch 105-- Valid loss:0.71 accuracy:0.86 precision:0.75 recall:0.82 f1:0.78\n","Epoch 106-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.85\n","Epoch 106-- Valid loss:0.71 accuracy:0.86 precision:0.76 recall:0.82 f1:0.79\n","Epoch 107-- Train loss:0.20 accuracy:0.88 precision:0.77 recall:0.94 f1:0.84\n","Epoch 107-- Valid loss:0.71 accuracy:0.86 precision:0.76 recall:0.82 f1:0.79\n","Epoch 108-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.86\n","Epoch 108-- Valid loss:0.70 accuracy:0.86 precision:0.76 recall:0.82 f1:0.78\n","Epoch 109-- Train loss:0.20 accuracy:0.89 precision:0.78 recall:0.94 f1:0.84\n","Epoch 109-- Valid loss:0.70 accuracy:0.86 precision:0.76 recall:0.82 f1:0.79\n","Epoch 110-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.85\n","Epoch 110-- Valid loss:0.71 accuracy:0.86 precision:0.76 recall:0.82 f1:0.79\n","Epoch 111-- Train loss:0.20 accuracy:0.88 precision:0.77 recall:0.93 f1:0.83\n","Epoch 111-- Valid loss:0.71 accuracy:0.86 precision:0.76 recall:0.82 f1:0.79\n","Epoch 112-- Train loss:0.21 accuracy:0.88 precision:0.78 recall:0.93 f1:0.84\n","Epoch 112-- Valid loss:0.71 accuracy:0.86 precision:0.76 recall:0.82 f1:0.79\n","Epoch 113-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.85\n","Epoch 113-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 114-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.85\n","Epoch 114-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 115-- Train loss:0.20 accuracy:0.88 precision:0.77 recall:0.93 f1:0.84\n","Epoch 115-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 116-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.94 f1:0.85\n","Epoch 116-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 117-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.95 f1:0.85\n","Epoch 117-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 118-- Train loss:0.20 accuracy:0.88 precision:0.77 recall:0.94 f1:0.84\n","Epoch 118-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 119-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 119-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 120-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 120-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 121-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 121-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 122-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.95 f1:0.84\n","Epoch 122-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 123-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.85\n","Epoch 123-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 124-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.94 f1:0.85\n","Epoch 124-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 125-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 125-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 126-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.95 f1:0.85\n","Epoch 126-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 127-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.85\n","Epoch 127-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 128-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.85\n","Epoch 128-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 129-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 129-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 130-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 130-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 131-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.85\n","Epoch 131-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 132-- Train loss:0.20 accuracy:0.88 precision:0.77 recall:0.93 f1:0.84\n","Epoch 132-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 133-- Train loss:0.20 accuracy:0.89 precision:0.79 recall:0.94 f1:0.85\n","Epoch 133-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 134-- Train loss:0.20 accuracy:0.88 precision:0.77 recall:0.93 f1:0.84\n","Epoch 134-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 135-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.85\n","Epoch 135-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 136-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 136-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 137-- Train loss:0.19 accuracy:0.89 precision:0.80 recall:0.95 f1:0.86\n","Epoch 137-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 138-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.86\n","Epoch 138-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 139-- Train loss:0.19 accuracy:0.89 precision:0.80 recall:0.95 f1:0.86\n","Epoch 139-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 140-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.95 f1:0.85\n","Epoch 140-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 141-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 141-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 142-- Train loss:0.19 accuracy:0.88 precision:0.77 recall:0.94 f1:0.84\n","Epoch 142-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 143-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.84\n","Epoch 143-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 144-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 144-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 145-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.85\n","Epoch 145-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 146-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.85\n","Epoch 146-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 147-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 147-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 148-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 148-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 149-- Train loss:0.20 accuracy:0.89 precision:0.79 recall:0.95 f1:0.86\n","Epoch 149-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 150-- Train loss:0.19 accuracy:0.89 precision:0.80 recall:0.95 f1:0.86\n","Epoch 150-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 151-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 151-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 152-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 152-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 153-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 153-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 154-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.94 f1:0.85\n","Epoch 154-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 155-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.95 f1:0.85\n","Epoch 155-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 156-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.85\n","Epoch 156-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 157-- Train loss:0.20 accuracy:0.88 precision:0.77 recall:0.94 f1:0.84\n","Epoch 157-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 158-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 158-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 159-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 159-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 160-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.95 f1:0.85\n","Epoch 160-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 161-- Train loss:0.20 accuracy:0.89 precision:0.79 recall:0.94 f1:0.85\n","Epoch 161-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 162-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 162-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 163-- Train loss:0.18 accuracy:0.89 precision:0.78 recall:0.95 f1:0.85\n","Epoch 163-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 164-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.95 f1:0.85\n","Epoch 164-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 165-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 165-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 166-- Train loss:0.18 accuracy:0.89 precision:0.79 recall:0.95 f1:0.85\n","Epoch 166-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 167-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.95 f1:0.85\n","Epoch 167-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 168-- Train loss:0.20 accuracy:0.89 precision:0.79 recall:0.95 f1:0.85\n","Epoch 168-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 169-- Train loss:0.18 accuracy:0.89 precision:0.79 recall:0.95 f1:0.86\n","Epoch 169-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 170-- Train loss:0.18 accuracy:0.89 precision:0.79 recall:0.95 f1:0.86\n","Epoch 170-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 171-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 171-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 172-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.95 f1:0.85\n","Epoch 172-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 173-- Train loss:0.20 accuracy:0.89 precision:0.79 recall:0.94 f1:0.85\n","Epoch 173-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 174-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.86\n","Epoch 174-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 175-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.85\n","Epoch 175-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 176-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 176-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 177-- Train loss:0.18 accuracy:0.90 precision:0.79 recall:0.95 f1:0.86\n","Epoch 177-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 178-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 178-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 179-- Train loss:0.20 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 179-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 180-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.95 f1:0.85\n","Epoch 180-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 181-- Train loss:0.20 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 181-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 182-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.85\n","Epoch 182-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 183-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.86\n","Epoch 183-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 184-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 184-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 185-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.94 f1:0.85\n","Epoch 185-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 186-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.95 f1:0.85\n","Epoch 186-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 187-- Train loss:0.18 accuracy:0.88 precision:0.78 recall:0.95 f1:0.85\n","Epoch 187-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 188-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.85\n","Epoch 188-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 189-- Train loss:0.18 accuracy:0.89 precision:0.79 recall:0.95 f1:0.86\n","Epoch 189-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 190-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 190-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 191-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.95 f1:0.85\n","Epoch 191-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 192-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.85\n","Epoch 192-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 193-- Train loss:0.20 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 193-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 194-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.95 f1:0.85\n","Epoch 194-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 195-- Train loss:0.18 accuracy:0.89 precision:0.78 recall:0.95 f1:0.85\n","Epoch 195-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 196-- Train loss:0.20 accuracy:0.89 precision:0.79 recall:0.94 f1:0.85\n","Epoch 196-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 197-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.85\n","Epoch 197-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 198-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 198-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 199-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.95 f1:0.85\n","Epoch 199-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 200-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 200-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 201-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 201-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 202-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 202-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 203-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.86\n","Epoch 203-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 204-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.94 f1:0.85\n","Epoch 204-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 205-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 205-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 206-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.85\n","Epoch 206-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 207-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.94 f1:0.85\n","Epoch 207-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 208-- Train loss:0.20 accuracy:0.89 precision:0.79 recall:0.94 f1:0.85\n","Epoch 208-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 209-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.86\n","Epoch 209-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 210-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 210-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 211-- Train loss:0.20 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 211-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 212-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.86\n","Epoch 212-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 213-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.85\n","Epoch 213-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 214-- Train loss:0.19 accuracy:0.89 precision:0.80 recall:0.95 f1:0.86\n","Epoch 214-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 215-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 215-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 216-- Train loss:0.19 accuracy:0.88 precision:0.79 recall:0.94 f1:0.85\n","Epoch 216-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 217-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 217-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 218-- Train loss:0.20 accuracy:0.89 precision:0.78 recall:0.94 f1:0.84\n","Epoch 218-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 219-- Train loss:0.20 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 219-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 220-- Train loss:0.20 accuracy:0.89 precision:0.79 recall:0.95 f1:0.85\n","Epoch 220-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 221-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 221-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 222-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 222-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 223-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.85\n","Epoch 223-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 224-- Train loss:0.20 accuracy:0.88 precision:0.77 recall:0.93 f1:0.84\n","Epoch 224-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 225-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.94 f1:0.85\n","Epoch 225-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 226-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.95 f1:0.85\n","Epoch 226-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 227-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 227-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 228-- Train loss:0.20 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 228-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 229-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.85\n","Epoch 229-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 230-- Train loss:0.19 accuracy:0.88 precision:0.77 recall:0.94 f1:0.84\n","Epoch 230-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 231-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.86\n","Epoch 231-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 232-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.85\n","Epoch 232-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 233-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 233-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 234-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.94 f1:0.84\n","Epoch 234-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 235-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 235-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 236-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 236-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 237-- Train loss:0.20 accuracy:0.89 precision:0.79 recall:0.95 f1:0.85\n","Epoch 237-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 238-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 238-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 239-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.85\n","Epoch 239-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 240-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.85\n","Epoch 240-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 241-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.85\n","Epoch 241-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 242-- Train loss:0.19 accuracy:0.89 precision:0.78 recall:0.95 f1:0.85\n","Epoch 242-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 243-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.85\n","Epoch 243-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 244-- Train loss:0.20 accuracy:0.88 precision:0.77 recall:0.93 f1:0.83\n","Epoch 244-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 245-- Train loss:0.19 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 245-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 246-- Train loss:0.20 accuracy:0.89 precision:0.78 recall:0.95 f1:0.85\n","Epoch 246-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 247-- Train loss:0.19 accuracy:0.89 precision:0.79 recall:0.95 f1:0.85\n","Epoch 247-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 248-- Train loss:0.20 accuracy:0.88 precision:0.78 recall:0.94 f1:0.84\n","Epoch 248-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n","Epoch 249-- Train loss:0.20 accuracy:0.89 precision:0.78 recall:0.94 f1:0.85\n","Epoch 249-- Valid loss:0.71 accuracy:0.86 precision:0.77 recall:0.83 f1:0.79\n"]}]},{"cell_type":"code","source":["import pickle\n","baseline_model = BaselineLSTM().to(device)\n","baseline_model.load_state_dict(torch.load('baselineLSTM_0122.pth'))"],"metadata":{"id":"b1B_um_OWc6e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705947911663,"user_tz":0,"elapsed":23,"user":{"displayName":"Xiaochen Zoey Tan","userId":"02061312605743465288"}},"outputId":"d4c2b886-018c-417a-9831-15d3a87b0554"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":87}]},{"cell_type":"code","source":["def train_knowledge_distillation(teacher, student, epochs, learning_rate, soft_target_loss_weight, ce_loss_weight, device):\n","    ce_loss = nn.CrossEntropyLoss()\n","    kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n","    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n","\n","    teacher.eval()  # Teacher set to evaluation mode\n","    student.train() # Student to train mode\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for inputs, labels in train_dataloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            with torch.no_grad():\n","                teacher_out = teacher(inputs)\n","\n","            student_out = student(inputs)\n","\n","            soft_targets_loss = ce_loss(student_out, teacher_out)\n","\n","            label_loss = ce_loss(student_out, labels)\n","            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_dataloader)}\")\n","\n","def train_knowledge_distillation_naive(teacher, student, epochs, learning_rate, soft_target_loss_weight, ce_loss_weight, device):\n","    ce_loss = nn.CrossEntropyLoss()\n","    kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n","    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n","\n","    teacher.eval()  # Teacher set to evaluation mode\n","    student.train() # Student to train mode\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for inputs, labels in train_dataloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            with torch.no_grad():\n","                teacher_out = teacher(inputs)\n","\n","            student_out = torch.log(student(inputs))\n","            print(student(inputs),torch.log(student(inputs)))\n","\n","            soft_targets_loss = -torch.sum(teacher_out * student_out) / student_out.size()[0]\n","\n","            label_loss = ce_loss(student_out, labels)\n","            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_dataloader)}\")"],"metadata":{"id":"XOz7KHIeS78G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import copy\n","\n","teacher_model = BaselineLSTM().to(device)\n","teacher_model.load_state_dict(torch.load('baselineLSTM_0122.pth'))\n","results_conv = []\n","results_convlstm = []\n","for weight in np.linspace(0,1,11):\n","    student_model_conv_copy = copy.deepcopy(student_model_conv)\n","    student_model_convlstm_copy = copy.deepcopy(student_model_convlstm)\n","    train_knowledge_distillation(teacher=teacher_model, student=student_model_conv_copy, epochs=100, learning_rate=1e-3, soft_target_loss_weight=weight, ce_loss_weight=1-weight, device=device)\n","    train_knowledge_distillation(teacher=teacher_model, student=student_model_convlstm_copy, epochs=100, learning_rate=1e-3, soft_target_loss_weight=weight, ce_loss_weight=1-weight, device=device)\n","    print(model_evaluate(student_model_conv_copy)[\"f1_score\"], model_evaluate(student_model_convlstm_copy)[\"f1_score\"])\n","    results_conv.append(model_evaluate(student_model_conv_copy))\n","    results_convlstm.append(model_evaluate(student_model_convlstm_copy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ANlyoqxIj7AN","outputId":"44303e13-2a17-4a9f-805f-ba1a27618736","executionInfo":{"status":"ok","timestamp":1705954166044,"user_tz":0,"elapsed":352112,"user":{"displayName":"Xiaochen Zoey Tan","userId":"02061312605743465288"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100, Loss: 1.2322076857089996\n","Epoch 2/100, Loss: 1.2253742814064026\n","Epoch 3/100, Loss: 1.2219525128602982\n","Epoch 4/100, Loss: 1.2083457261323929\n","Epoch 5/100, Loss: 1.2049484252929688\n","Epoch 6/100, Loss: 1.1978644579648972\n","Epoch 7/100, Loss: 1.204502671957016\n","Epoch 8/100, Loss: 1.1981400549411774\n","Epoch 9/100, Loss: 1.2013391554355621\n","Epoch 10/100, Loss: 1.1954323798418045\n","Epoch 11/100, Loss: 1.1883406192064285\n","Epoch 12/100, Loss: 1.1938782185316086\n","Epoch 13/100, Loss: 1.1979675441980362\n","Epoch 14/100, Loss: 1.191600725054741\n","Epoch 15/100, Loss: 1.1926616877317429\n","Epoch 16/100, Loss: 1.187788650393486\n","Epoch 17/100, Loss: 1.1899406611919403\n","Epoch 18/100, Loss: 1.183540791273117\n","Epoch 19/100, Loss: 1.1868154853582382\n","Epoch 20/100, Loss: 1.1826448887586594\n","Epoch 21/100, Loss: 1.1892844289541245\n","Epoch 22/100, Loss: 1.1781978011131287\n","Epoch 23/100, Loss: 1.1755514740943909\n","Epoch 24/100, Loss: 1.1801898926496506\n","Epoch 25/100, Loss: 1.1776583194732666\n","Epoch 26/100, Loss: 1.1769748479127884\n","Epoch 27/100, Loss: 1.17083640396595\n","Epoch 28/100, Loss: 1.177662268280983\n","Epoch 29/100, Loss: 1.1702258437871933\n","Epoch 30/100, Loss: 1.1721546053886414\n","Epoch 31/100, Loss: 1.1731136739253998\n","Epoch 32/100, Loss: 1.1666923612356186\n","Epoch 33/100, Loss: 1.164383590221405\n","Epoch 34/100, Loss: 1.16457799077034\n","Epoch 35/100, Loss: 1.160786435008049\n","Epoch 36/100, Loss: 1.163062036037445\n","Epoch 37/100, Loss: 1.1641395390033722\n","Epoch 38/100, Loss: 1.16231469810009\n","Epoch 39/100, Loss: 1.1574133038520813\n","Epoch 40/100, Loss: 1.1498305350542068\n","Epoch 41/100, Loss: 1.1532175093889236\n","Epoch 42/100, Loss: 1.161781221628189\n","Epoch 43/100, Loss: 1.163210242986679\n","Epoch 44/100, Loss: 1.1631585359573364\n","Epoch 45/100, Loss: 1.153341829776764\n","Epoch 46/100, Loss: 1.1532809436321259\n","Epoch 47/100, Loss: 1.1662769615650177\n","Epoch 48/100, Loss: 1.15929214656353\n","Epoch 49/100, Loss: 1.1556594967842102\n","Epoch 50/100, Loss: 1.1516831368207932\n","Epoch 51/100, Loss: 1.146795704960823\n","Epoch 52/100, Loss: 1.1442684680223465\n","Epoch 53/100, Loss: 1.1476710736751556\n","Epoch 54/100, Loss: 1.1426514983177185\n","Epoch 55/100, Loss: 1.1463962942361832\n","Epoch 56/100, Loss: 1.1453888714313507\n","Epoch 57/100, Loss: 1.1431395560503006\n","Epoch 58/100, Loss: 1.139014109969139\n","Epoch 59/100, Loss: 1.145336538553238\n","Epoch 60/100, Loss: 1.1326769143342972\n","Epoch 61/100, Loss: 1.1448343247175217\n","Epoch 62/100, Loss: 1.1408365815877914\n","Epoch 63/100, Loss: 1.1435638517141342\n","Epoch 64/100, Loss: 1.1291489005088806\n","Epoch 65/100, Loss: 1.139284297823906\n","Epoch 66/100, Loss: 1.126290500164032\n","Epoch 67/100, Loss: 1.1311918795108795\n","Epoch 68/100, Loss: 1.1340650916099548\n","Epoch 69/100, Loss: 1.1322242319583893\n","Epoch 70/100, Loss: 1.1292350143194199\n","Epoch 71/100, Loss: 1.1257613450288773\n","Epoch 72/100, Loss: 1.1281137615442276\n","Epoch 73/100, Loss: 1.1255255490541458\n","Epoch 74/100, Loss: 1.1305947750806808\n","Epoch 75/100, Loss: 1.1227078586816788\n","Epoch 76/100, Loss: 1.122308924794197\n","Epoch 77/100, Loss: 1.1195735037326813\n","Epoch 78/100, Loss: 1.1233041286468506\n","Epoch 79/100, Loss: 1.1261872798204422\n","Epoch 80/100, Loss: 1.1264694929122925\n","Epoch 81/100, Loss: 1.1190611124038696\n","Epoch 82/100, Loss: 1.118338018655777\n","Epoch 83/100, Loss: 1.1170215010643005\n","Epoch 84/100, Loss: 1.1153374314308167\n","Epoch 85/100, Loss: 1.1205113381147385\n","Epoch 86/100, Loss: 1.1100124716758728\n","Epoch 87/100, Loss: 1.1201585978269577\n","Epoch 88/100, Loss: 1.116037830710411\n","Epoch 89/100, Loss: 1.1156919747591019\n","Epoch 90/100, Loss: 1.1159776151180267\n","Epoch 91/100, Loss: 1.1138159334659576\n","Epoch 92/100, Loss: 1.1136661767959595\n","Epoch 93/100, Loss: 1.1134191453456879\n","Epoch 94/100, Loss: 1.1115160882472992\n","Epoch 95/100, Loss: 1.1119744926691055\n","Epoch 96/100, Loss: 1.1120119541883469\n","Epoch 97/100, Loss: 1.1069101244211197\n","Epoch 98/100, Loss: 1.1107233166694641\n","Epoch 99/100, Loss: 1.1091150492429733\n","Epoch 100/100, Loss: 1.101488620042801\n","Epoch 1/100, Loss: 1.3460321724414825\n","Epoch 2/100, Loss: 1.3616979867219925\n","Epoch 3/100, Loss: 1.3616979867219925\n","Epoch 4/100, Loss: 1.3616979867219925\n","Epoch 5/100, Loss: 1.3616979867219925\n","Epoch 6/100, Loss: 1.3616979867219925\n","Epoch 7/100, Loss: 1.3616979867219925\n","Epoch 8/100, Loss: 1.3616979867219925\n","Epoch 9/100, Loss: 1.3616979867219925\n","Epoch 10/100, Loss: 1.3616979867219925\n","Epoch 11/100, Loss: 1.3616979867219925\n","Epoch 12/100, Loss: 1.3616979867219925\n","Epoch 13/100, Loss: 1.3616979867219925\n","Epoch 14/100, Loss: 1.3616979867219925\n","Epoch 15/100, Loss: 1.3616979867219925\n","Epoch 16/100, Loss: 1.3616979867219925\n","Epoch 17/100, Loss: 1.3616979867219925\n","Epoch 18/100, Loss: 1.3616979867219925\n","Epoch 19/100, Loss: 1.3616979867219925\n","Epoch 20/100, Loss: 1.3616979867219925\n","Epoch 21/100, Loss: 1.3616979867219925\n","Epoch 22/100, Loss: 1.3616979867219925\n","Epoch 23/100, Loss: 1.3616979867219925\n","Epoch 24/100, Loss: 1.3616979867219925\n","Epoch 25/100, Loss: 1.3616979867219925\n","Epoch 26/100, Loss: 1.3616979867219925\n","Epoch 27/100, Loss: 1.3616979867219925\n","Epoch 28/100, Loss: 1.3616979867219925\n","Epoch 29/100, Loss: 1.3616979867219925\n","Epoch 30/100, Loss: 1.3616979867219925\n","Epoch 31/100, Loss: 1.3616979867219925\n","Epoch 32/100, Loss: 1.3616979867219925\n","Epoch 33/100, Loss: 1.3616979867219925\n","Epoch 34/100, Loss: 1.3616979867219925\n","Epoch 35/100, Loss: 1.3616979867219925\n","Epoch 36/100, Loss: 1.3616979867219925\n","Epoch 37/100, Loss: 1.3616979867219925\n","Epoch 38/100, Loss: 1.3616979867219925\n","Epoch 39/100, Loss: 1.3616979867219925\n","Epoch 40/100, Loss: 1.3616979867219925\n","Epoch 41/100, Loss: 1.3616979867219925\n","Epoch 42/100, Loss: 1.3616979867219925\n","Epoch 43/100, Loss: 1.3616979867219925\n","Epoch 44/100, Loss: 1.3616979867219925\n","Epoch 45/100, Loss: 1.3616979867219925\n","Epoch 46/100, Loss: 1.3616979867219925\n","Epoch 47/100, Loss: 1.3616979867219925\n","Epoch 48/100, Loss: 1.3616979867219925\n","Epoch 49/100, Loss: 1.3616979867219925\n","Epoch 50/100, Loss: 1.3616979867219925\n","Epoch 51/100, Loss: 1.3616979867219925\n","Epoch 52/100, Loss: 1.3616979867219925\n","Epoch 53/100, Loss: 1.3616979867219925\n","Epoch 54/100, Loss: 1.3616979867219925\n","Epoch 55/100, Loss: 1.3616979867219925\n","Epoch 56/100, Loss: 1.3616979867219925\n","Epoch 57/100, Loss: 1.3616979867219925\n","Epoch 58/100, Loss: 1.3616979867219925\n","Epoch 59/100, Loss: 1.3616979867219925\n","Epoch 60/100, Loss: 1.3616979867219925\n","Epoch 61/100, Loss: 1.3616979867219925\n","Epoch 62/100, Loss: 1.3616979867219925\n","Epoch 63/100, Loss: 1.3616979867219925\n","Epoch 64/100, Loss: 1.3616979867219925\n","Epoch 65/100, Loss: 1.3616979867219925\n","Epoch 66/100, Loss: 1.3616979867219925\n","Epoch 67/100, Loss: 1.3616979867219925\n","Epoch 68/100, Loss: 1.3616979867219925\n","Epoch 69/100, Loss: 1.3616979867219925\n","Epoch 70/100, Loss: 1.3616979867219925\n","Epoch 71/100, Loss: 1.3616979867219925\n","Epoch 72/100, Loss: 1.3616979867219925\n","Epoch 73/100, Loss: 1.3616979867219925\n","Epoch 74/100, Loss: 1.3616979867219925\n","Epoch 75/100, Loss: 1.3616979867219925\n","Epoch 76/100, Loss: 1.3616979867219925\n","Epoch 77/100, Loss: 1.3616979867219925\n","Epoch 78/100, Loss: 1.3616979867219925\n","Epoch 79/100, Loss: 1.3616979867219925\n","Epoch 80/100, Loss: 1.3616979867219925\n","Epoch 81/100, Loss: 1.3616979867219925\n","Epoch 82/100, Loss: 1.3616979867219925\n","Epoch 83/100, Loss: 1.3616979867219925\n","Epoch 84/100, Loss: 1.3616979867219925\n","Epoch 85/100, Loss: 1.3616979867219925\n","Epoch 86/100, Loss: 1.3616979867219925\n","Epoch 87/100, Loss: 1.3616979867219925\n","Epoch 88/100, Loss: 1.3616979867219925\n","Epoch 89/100, Loss: 1.3616979867219925\n","Epoch 90/100, Loss: 1.3616979867219925\n","Epoch 91/100, Loss: 1.3616979867219925\n","Epoch 92/100, Loss: 1.3616979867219925\n","Epoch 93/100, Loss: 1.3616979867219925\n","Epoch 94/100, Loss: 1.3616979867219925\n","Epoch 95/100, Loss: 1.3616979867219925\n","Epoch 96/100, Loss: 1.3616979867219925\n","Epoch 97/100, Loss: 1.3616979867219925\n","Epoch 98/100, Loss: 1.3616979867219925\n","Epoch 99/100, Loss: 1.3616979867219925\n","Epoch 100/100, Loss: 1.3616979867219925\n","tensor(0.8926, device='cuda:0') tensor(0.1313, device='cuda:0')\n","Epoch 1/100, Loss: 1.2364710569381714\n","Epoch 2/100, Loss: 1.2240381389856339\n","Epoch 3/100, Loss: 1.2130002081394196\n","Epoch 4/100, Loss: 1.2186038494110107\n","Epoch 5/100, Loss: 1.2106237560510635\n","Epoch 6/100, Loss: 1.215171456336975\n","Epoch 7/100, Loss: 1.203706756234169\n","Epoch 8/100, Loss: 1.2069832682609558\n","Epoch 9/100, Loss: 1.2020977139472961\n","Epoch 10/100, Loss: 1.196075662970543\n","Epoch 11/100, Loss: 1.193761020898819\n","Epoch 12/100, Loss: 1.1913661658763885\n","Epoch 13/100, Loss: 1.1910804957151413\n","Epoch 14/100, Loss: 1.1923354417085648\n","Epoch 15/100, Loss: 1.1829237341880798\n","Epoch 16/100, Loss: 1.183201640844345\n","Epoch 17/100, Loss: 1.1928787976503372\n","Epoch 18/100, Loss: 1.1908010840415955\n","Epoch 19/100, Loss: 1.1860504448413849\n","Epoch 20/100, Loss: 1.1796988546848297\n","Epoch 21/100, Loss: 1.1707478612661362\n","Epoch 22/100, Loss: 1.1791315376758575\n","Epoch 23/100, Loss: 1.1822174936532974\n","Epoch 24/100, Loss: 1.181851014494896\n","Epoch 25/100, Loss: 1.1732293963432312\n","Epoch 26/100, Loss: 1.1758654415607452\n","Epoch 27/100, Loss: 1.1673744320869446\n","Epoch 28/100, Loss: 1.1739574670791626\n","Epoch 29/100, Loss: 1.1684610694646835\n","Epoch 30/100, Loss: 1.1725283116102219\n","Epoch 31/100, Loss: 1.1743497997522354\n","Epoch 32/100, Loss: 1.1629886627197266\n","Epoch 33/100, Loss: 1.1683235615491867\n","Epoch 34/100, Loss: 1.1735677421092987\n","Epoch 35/100, Loss: 1.1656748205423355\n","Epoch 36/100, Loss: 1.1630768328905106\n","Epoch 37/100, Loss: 1.157972365617752\n","Epoch 38/100, Loss: 1.15787073969841\n","Epoch 39/100, Loss: 1.1571438312530518\n","Epoch 40/100, Loss: 1.1580080389976501\n","Epoch 41/100, Loss: 1.1543326824903488\n","Epoch 42/100, Loss: 1.1571771651506424\n","Epoch 43/100, Loss: 1.157849371433258\n","Epoch 44/100, Loss: 1.153687208890915\n","Epoch 45/100, Loss: 1.1522864252328873\n","Epoch 46/100, Loss: 1.1494761258363724\n","Epoch 47/100, Loss: 1.1536564975976944\n","Epoch 48/100, Loss: 1.1507252305746078\n","Epoch 49/100, Loss: 1.150347739458084\n","Epoch 50/100, Loss: 1.146186962723732\n","Epoch 51/100, Loss: 1.1363414376974106\n","Epoch 52/100, Loss: 1.1497973203659058\n","Epoch 53/100, Loss: 1.1449582874774933\n","Epoch 54/100, Loss: 1.1492963582277298\n","Epoch 55/100, Loss: 1.144598826766014\n","Epoch 56/100, Loss: 1.1445596367120743\n","Epoch 57/100, Loss: 1.1416980922222137\n","Epoch 58/100, Loss: 1.1370043754577637\n","Epoch 59/100, Loss: 1.1393049210309982\n","Epoch 60/100, Loss: 1.1356033831834793\n","Epoch 61/100, Loss: 1.1383571922779083\n","Epoch 62/100, Loss: 1.13692906498909\n","Epoch 63/100, Loss: 1.1348300874233246\n","Epoch 64/100, Loss: 1.1321675777435303\n","Epoch 65/100, Loss: 1.1323988437652588\n","Epoch 66/100, Loss: 1.141822636127472\n","Epoch 67/100, Loss: 1.1259694993495941\n","Epoch 68/100, Loss: 1.1294010430574417\n","Epoch 69/100, Loss: 1.1364544928073883\n","Epoch 70/100, Loss: 1.1308869123458862\n","Epoch 71/100, Loss: 1.1366572827100754\n","Epoch 72/100, Loss: 1.1235965341329575\n","Epoch 73/100, Loss: 1.1234900057315826\n","Epoch 74/100, Loss: 1.1200105398893356\n","Epoch 75/100, Loss: 1.1246102005243301\n","Epoch 76/100, Loss: 1.1243656128644943\n","Epoch 77/100, Loss: 1.117641806602478\n","Epoch 78/100, Loss: 1.132888749241829\n","Epoch 79/100, Loss: 1.1247595846652985\n","Epoch 80/100, Loss: 1.119529202580452\n","Epoch 81/100, Loss: 1.1160357594490051\n","Epoch 82/100, Loss: 1.123742699623108\n","Epoch 83/100, Loss: 1.1228083968162537\n","Epoch 84/100, Loss: 1.1200308203697205\n","Epoch 85/100, Loss: 1.1116504669189453\n","Epoch 86/100, Loss: 1.1164091676473618\n","Epoch 87/100, Loss: 1.109972059726715\n","Epoch 88/100, Loss: 1.114833191037178\n","Epoch 89/100, Loss: 1.1054562628269196\n","Epoch 90/100, Loss: 1.1155275255441666\n","Epoch 91/100, Loss: 1.1076089590787888\n","Epoch 92/100, Loss: 1.1125771701335907\n","Epoch 93/100, Loss: 1.1050382107496262\n","Epoch 94/100, Loss: 1.1030230671167374\n","Epoch 95/100, Loss: 1.1071650385856628\n","Epoch 96/100, Loss: 1.105184257030487\n","Epoch 97/100, Loss: 1.1057405322790146\n","Epoch 98/100, Loss: 1.1021747142076492\n","Epoch 99/100, Loss: 1.1104765385389328\n","Epoch 100/100, Loss: 1.106550619006157\n","Epoch 1/100, Loss: 1.3456721901893616\n","Epoch 2/100, Loss: 1.3618646562099457\n","Epoch 3/100, Loss: 1.3618646562099457\n","Epoch 4/100, Loss: 1.3618646562099457\n","Epoch 5/100, Loss: 1.3618646562099457\n","Epoch 6/100, Loss: 1.3618646562099457\n","Epoch 7/100, Loss: 1.3618646562099457\n","Epoch 8/100, Loss: 1.3618646562099457\n","Epoch 9/100, Loss: 1.3618646562099457\n","Epoch 10/100, Loss: 1.3618646562099457\n","Epoch 11/100, Loss: 1.3618646562099457\n","Epoch 12/100, Loss: 1.3618646562099457\n","Epoch 13/100, Loss: 1.3618646562099457\n","Epoch 14/100, Loss: 1.3618646562099457\n","Epoch 15/100, Loss: 1.3618646562099457\n","Epoch 16/100, Loss: 1.3618646562099457\n","Epoch 17/100, Loss: 1.3618646562099457\n","Epoch 18/100, Loss: 1.3618646562099457\n","Epoch 19/100, Loss: 1.3618646562099457\n","Epoch 20/100, Loss: 1.3618646562099457\n","Epoch 21/100, Loss: 1.3618646562099457\n","Epoch 22/100, Loss: 1.3618646562099457\n","Epoch 23/100, Loss: 1.3618646562099457\n","Epoch 24/100, Loss: 1.3618646562099457\n","Epoch 25/100, Loss: 1.3618646562099457\n","Epoch 26/100, Loss: 1.3618646562099457\n","Epoch 27/100, Loss: 1.3618646562099457\n","Epoch 28/100, Loss: 1.3618646562099457\n","Epoch 29/100, Loss: 1.3618646562099457\n","Epoch 30/100, Loss: 1.3618646562099457\n","Epoch 31/100, Loss: 1.3618646562099457\n","Epoch 32/100, Loss: 1.3618646562099457\n","Epoch 33/100, Loss: 1.3618646562099457\n","Epoch 34/100, Loss: 1.3618646562099457\n","Epoch 35/100, Loss: 1.3618646562099457\n","Epoch 36/100, Loss: 1.3618646562099457\n","Epoch 37/100, Loss: 1.3618646562099457\n","Epoch 38/100, Loss: 1.3618646562099457\n","Epoch 39/100, Loss: 1.3618646562099457\n","Epoch 40/100, Loss: 1.3618646562099457\n","Epoch 41/100, Loss: 1.3618646562099457\n","Epoch 42/100, Loss: 1.3618646562099457\n","Epoch 43/100, Loss: 1.3618646562099457\n","Epoch 44/100, Loss: 1.3618646562099457\n","Epoch 45/100, Loss: 1.3618646562099457\n","Epoch 46/100, Loss: 1.3618646562099457\n","Epoch 47/100, Loss: 1.3618646562099457\n","Epoch 48/100, Loss: 1.3618646562099457\n","Epoch 49/100, Loss: 1.3618646562099457\n","Epoch 50/100, Loss: 1.3618646562099457\n","Epoch 51/100, Loss: 1.3618646562099457\n","Epoch 52/100, Loss: 1.3618646562099457\n","Epoch 53/100, Loss: 1.3618646562099457\n","Epoch 54/100, Loss: 1.3618646562099457\n","Epoch 55/100, Loss: 1.3618646562099457\n","Epoch 56/100, Loss: 1.3618646562099457\n","Epoch 57/100, Loss: 1.3618646562099457\n","Epoch 58/100, Loss: 1.3618646562099457\n","Epoch 59/100, Loss: 1.3618646562099457\n","Epoch 60/100, Loss: 1.3618646562099457\n","Epoch 61/100, Loss: 1.3618646562099457\n","Epoch 62/100, Loss: 1.3618646562099457\n","Epoch 63/100, Loss: 1.3618646562099457\n","Epoch 64/100, Loss: 1.3618646562099457\n","Epoch 65/100, Loss: 1.3618646562099457\n","Epoch 66/100, Loss: 1.3618646562099457\n","Epoch 67/100, Loss: 1.3618646562099457\n","Epoch 68/100, Loss: 1.3618646562099457\n","Epoch 69/100, Loss: 1.3618646562099457\n","Epoch 70/100, Loss: 1.3618646562099457\n","Epoch 71/100, Loss: 1.3618646562099457\n","Epoch 72/100, Loss: 1.3618646562099457\n","Epoch 73/100, Loss: 1.3618646562099457\n","Epoch 74/100, Loss: 1.3618646562099457\n","Epoch 75/100, Loss: 1.3618646562099457\n","Epoch 76/100, Loss: 1.3618646562099457\n","Epoch 77/100, Loss: 1.3618646562099457\n","Epoch 78/100, Loss: 1.3618646562099457\n","Epoch 79/100, Loss: 1.3618646562099457\n","Epoch 80/100, Loss: 1.3618646562099457\n","Epoch 81/100, Loss: 1.3618646562099457\n","Epoch 82/100, Loss: 1.3618646562099457\n","Epoch 83/100, Loss: 1.3618646562099457\n","Epoch 84/100, Loss: 1.3618646562099457\n","Epoch 85/100, Loss: 1.3618646562099457\n","Epoch 86/100, Loss: 1.3618646562099457\n","Epoch 87/100, Loss: 1.3618646562099457\n","Epoch 88/100, Loss: 1.3618646562099457\n","Epoch 89/100, Loss: 1.3618646562099457\n","Epoch 90/100, Loss: 1.3618646562099457\n","Epoch 91/100, Loss: 1.3618646562099457\n","Epoch 92/100, Loss: 1.3618646562099457\n","Epoch 93/100, Loss: 1.3618646562099457\n","Epoch 94/100, Loss: 1.3618646562099457\n","Epoch 95/100, Loss: 1.3618646562099457\n","Epoch 96/100, Loss: 1.3618646562099457\n","Epoch 97/100, Loss: 1.3618646562099457\n","Epoch 98/100, Loss: 1.3618646562099457\n","Epoch 99/100, Loss: 1.3618646562099457\n","Epoch 100/100, Loss: 1.3618646562099457\n","tensor(0.9020, device='cuda:0') tensor(0.1313, device='cuda:0')\n","Epoch 1/100, Loss: 1.231894999742508\n","Epoch 2/100, Loss: 1.2193211317062378\n","Epoch 3/100, Loss: 1.216806337237358\n","Epoch 4/100, Loss: 1.217407077550888\n","Epoch 5/100, Loss: 1.2150927931070328\n","Epoch 6/100, Loss: 1.205900028347969\n","Epoch 7/100, Loss: 1.2126311659812927\n","Epoch 8/100, Loss: 1.197751984000206\n","Epoch 9/100, Loss: 1.2053814679384232\n","Epoch 10/100, Loss: 1.1989623308181763\n","Epoch 11/100, Loss: 1.1997348815202713\n","Epoch 12/100, Loss: 1.1904713809490204\n","Epoch 13/100, Loss: 1.196715161204338\n","Epoch 14/100, Loss: 1.1992423981428146\n","Epoch 15/100, Loss: 1.1922562271356583\n","Epoch 16/100, Loss: 1.1893497854471207\n","Epoch 17/100, Loss: 1.184614509344101\n","Epoch 18/100, Loss: 1.184722125530243\n","Epoch 19/100, Loss: 1.1834541708230972\n","Epoch 20/100, Loss: 1.1854179203510284\n","Epoch 21/100, Loss: 1.186102956533432\n","Epoch 22/100, Loss: 1.1830978393554688\n","Epoch 23/100, Loss: 1.1842933893203735\n","Epoch 24/100, Loss: 1.1792661845684052\n","Epoch 25/100, Loss: 1.1798612922430038\n","Epoch 26/100, Loss: 1.1718763411045074\n","Epoch 27/100, Loss: 1.1722274869680405\n","Epoch 28/100, Loss: 1.1728914082050323\n","Epoch 29/100, Loss: 1.164331093430519\n","Epoch 30/100, Loss: 1.1656821221113205\n","Epoch 31/100, Loss: 1.1616369485855103\n","Epoch 32/100, Loss: 1.166234314441681\n","Epoch 33/100, Loss: 1.1647067219018936\n","Epoch 34/100, Loss: 1.1683712750673294\n","Epoch 35/100, Loss: 1.163181722164154\n","Epoch 36/100, Loss: 1.1536624282598495\n","Epoch 37/100, Loss: 1.1687646061182022\n","Epoch 38/100, Loss: 1.161913439631462\n","Epoch 39/100, Loss: 1.1652492433786392\n","Epoch 40/100, Loss: 1.167524203658104\n","Epoch 41/100, Loss: 1.164038434624672\n","Epoch 42/100, Loss: 1.1652655452489853\n","Epoch 43/100, Loss: 1.15529765188694\n","Epoch 44/100, Loss: 1.1637034714221954\n","Epoch 45/100, Loss: 1.1512785255908966\n","Epoch 46/100, Loss: 1.159740298986435\n","Epoch 47/100, Loss: 1.1526239067316055\n","Epoch 48/100, Loss: 1.151083067059517\n","Epoch 49/100, Loss: 1.1525450199842453\n","Epoch 50/100, Loss: 1.146325707435608\n","Epoch 51/100, Loss: 1.1472969204187393\n","Epoch 52/100, Loss: 1.150633454322815\n","Epoch 53/100, Loss: 1.1479041576385498\n","Epoch 54/100, Loss: 1.1520315259695053\n","Epoch 55/100, Loss: 1.1417128443717957\n","Epoch 56/100, Loss: 1.1480261385440826\n","Epoch 57/100, Loss: 1.1426349133253098\n","Epoch 58/100, Loss: 1.1423331499099731\n","Epoch 59/100, Loss: 1.1397497057914734\n","Epoch 60/100, Loss: 1.133156344294548\n","Epoch 61/100, Loss: 1.1359939277172089\n","Epoch 62/100, Loss: 1.1315660774707794\n","Epoch 63/100, Loss: 1.1396144479513168\n","Epoch 64/100, Loss: 1.133739784359932\n","Epoch 65/100, Loss: 1.1324985027313232\n","Epoch 66/100, Loss: 1.1330855190753937\n","Epoch 67/100, Loss: 1.1387590318918228\n","Epoch 68/100, Loss: 1.1287591308355331\n","Epoch 69/100, Loss: 1.1325974762439728\n","Epoch 70/100, Loss: 1.1270939111709595\n","Epoch 71/100, Loss: 1.1314928084611893\n","Epoch 72/100, Loss: 1.1300950348377228\n","Epoch 73/100, Loss: 1.1310313045978546\n","Epoch 74/100, Loss: 1.1241454184055328\n","Epoch 75/100, Loss: 1.1211519688367844\n","Epoch 76/100, Loss: 1.1257622092962265\n","Epoch 77/100, Loss: 1.1190480887889862\n","Epoch 78/100, Loss: 1.1264245063066483\n","Epoch 79/100, Loss: 1.1137472540140152\n","Epoch 80/100, Loss: 1.1158626973628998\n","Epoch 81/100, Loss: 1.11823470890522\n","Epoch 82/100, Loss: 1.1177118718624115\n","Epoch 83/100, Loss: 1.121827945113182\n","Epoch 84/100, Loss: 1.1170921176671982\n","Epoch 85/100, Loss: 1.1113632172346115\n","Epoch 86/100, Loss: 1.1112934350967407\n","Epoch 87/100, Loss: 1.1140173375606537\n","Epoch 88/100, Loss: 1.1181077510118484\n","Epoch 89/100, Loss: 1.11482834815979\n","Epoch 90/100, Loss: 1.1116597056388855\n","Epoch 91/100, Loss: 1.1136170029640198\n","Epoch 92/100, Loss: 1.115606352686882\n","Epoch 93/100, Loss: 1.1105697304010391\n","Epoch 94/100, Loss: 1.1124815344810486\n","Epoch 95/100, Loss: 1.1114435344934464\n","Epoch 96/100, Loss: 1.1118213534355164\n","Epoch 97/100, Loss: 1.102847009897232\n","Epoch 98/100, Loss: 1.100204959511757\n","Epoch 99/100, Loss: 1.1032147109508514\n","Epoch 100/100, Loss: 1.1085922122001648\n","Epoch 1/100, Loss: 1.3455347567796707\n","Epoch 2/100, Loss: 1.3620313853025436\n","Epoch 3/100, Loss: 1.3620313853025436\n","Epoch 4/100, Loss: 1.3620313853025436\n","Epoch 5/100, Loss: 1.3620313853025436\n","Epoch 6/100, Loss: 1.3620313853025436\n","Epoch 7/100, Loss: 1.3620313853025436\n","Epoch 8/100, Loss: 1.3620313853025436\n","Epoch 9/100, Loss: 1.3620313853025436\n","Epoch 10/100, Loss: 1.3620313853025436\n","Epoch 11/100, Loss: 1.3620313853025436\n","Epoch 12/100, Loss: 1.3620313853025436\n","Epoch 13/100, Loss: 1.3620313853025436\n","Epoch 14/100, Loss: 1.3620313853025436\n","Epoch 15/100, Loss: 1.3620313853025436\n","Epoch 16/100, Loss: 1.3620313853025436\n","Epoch 17/100, Loss: 1.3620313853025436\n","Epoch 18/100, Loss: 1.3620313853025436\n","Epoch 19/100, Loss: 1.3620313853025436\n","Epoch 20/100, Loss: 1.3620313853025436\n","Epoch 21/100, Loss: 1.3620313853025436\n","Epoch 22/100, Loss: 1.3620313853025436\n","Epoch 23/100, Loss: 1.3620313853025436\n","Epoch 24/100, Loss: 1.3620313853025436\n","Epoch 25/100, Loss: 1.3620313853025436\n","Epoch 26/100, Loss: 1.3620313853025436\n","Epoch 27/100, Loss: 1.3620313853025436\n","Epoch 28/100, Loss: 1.3620313853025436\n","Epoch 29/100, Loss: 1.3620313853025436\n","Epoch 30/100, Loss: 1.3620313853025436\n","Epoch 31/100, Loss: 1.3620313853025436\n","Epoch 32/100, Loss: 1.3620313853025436\n","Epoch 33/100, Loss: 1.3620313853025436\n","Epoch 34/100, Loss: 1.3620313853025436\n","Epoch 35/100, Loss: 1.3620313853025436\n","Epoch 36/100, Loss: 1.3620313853025436\n","Epoch 37/100, Loss: 1.3620313853025436\n","Epoch 38/100, Loss: 1.3620313853025436\n","Epoch 39/100, Loss: 1.3620313853025436\n","Epoch 40/100, Loss: 1.3620313853025436\n","Epoch 41/100, Loss: 1.3620313853025436\n","Epoch 42/100, Loss: 1.3620313853025436\n","Epoch 43/100, Loss: 1.3620313853025436\n","Epoch 44/100, Loss: 1.3620313853025436\n","Epoch 45/100, Loss: 1.3620313853025436\n","Epoch 46/100, Loss: 1.3620313853025436\n","Epoch 47/100, Loss: 1.3620313853025436\n","Epoch 48/100, Loss: 1.3620313853025436\n","Epoch 49/100, Loss: 1.3620313853025436\n","Epoch 50/100, Loss: 1.3620313853025436\n","Epoch 51/100, Loss: 1.3620313853025436\n","Epoch 52/100, Loss: 1.3620313853025436\n","Epoch 53/100, Loss: 1.3620313853025436\n","Epoch 54/100, Loss: 1.3620313853025436\n","Epoch 55/100, Loss: 1.3620313853025436\n","Epoch 56/100, Loss: 1.3620313853025436\n","Epoch 57/100, Loss: 1.3620313853025436\n","Epoch 58/100, Loss: 1.3620313853025436\n","Epoch 59/100, Loss: 1.3620313853025436\n","Epoch 60/100, Loss: 1.3620313853025436\n","Epoch 61/100, Loss: 1.3620313853025436\n","Epoch 62/100, Loss: 1.3620313853025436\n","Epoch 63/100, Loss: 1.3620313853025436\n","Epoch 64/100, Loss: 1.3620313853025436\n","Epoch 65/100, Loss: 1.3620313853025436\n","Epoch 66/100, Loss: 1.3620313853025436\n","Epoch 67/100, Loss: 1.3620313853025436\n","Epoch 68/100, Loss: 1.3620313853025436\n","Epoch 69/100, Loss: 1.3620313853025436\n","Epoch 70/100, Loss: 1.3620313853025436\n","Epoch 71/100, Loss: 1.3620313853025436\n","Epoch 72/100, Loss: 1.3620313853025436\n","Epoch 73/100, Loss: 1.3620313853025436\n","Epoch 74/100, Loss: 1.3620313853025436\n","Epoch 75/100, Loss: 1.3620313853025436\n","Epoch 76/100, Loss: 1.3620313853025436\n","Epoch 77/100, Loss: 1.3620313853025436\n","Epoch 78/100, Loss: 1.3620313853025436\n","Epoch 79/100, Loss: 1.3620313853025436\n","Epoch 80/100, Loss: 1.3620313853025436\n","Epoch 81/100, Loss: 1.3620313853025436\n","Epoch 82/100, Loss: 1.3620313853025436\n","Epoch 83/100, Loss: 1.3620313853025436\n","Epoch 84/100, Loss: 1.3620313853025436\n","Epoch 85/100, Loss: 1.3620313853025436\n","Epoch 86/100, Loss: 1.3620313853025436\n","Epoch 87/100, Loss: 1.3620313853025436\n","Epoch 88/100, Loss: 1.3620313853025436\n","Epoch 89/100, Loss: 1.3620313853025436\n","Epoch 90/100, Loss: 1.3620313853025436\n","Epoch 91/100, Loss: 1.3620313853025436\n","Epoch 92/100, Loss: 1.3620313853025436\n","Epoch 93/100, Loss: 1.3620313853025436\n","Epoch 94/100, Loss: 1.3620313853025436\n","Epoch 95/100, Loss: 1.3620313853025436\n","Epoch 96/100, Loss: 1.3620313853025436\n","Epoch 97/100, Loss: 1.3620313853025436\n","Epoch 98/100, Loss: 1.3620313853025436\n","Epoch 99/100, Loss: 1.3620313853025436\n","Epoch 100/100, Loss: 1.3620313853025436\n","tensor(0.9021, device='cuda:0') tensor(0.1313, device='cuda:0')\n","Epoch 1/100, Loss: 1.234950140118599\n","Epoch 2/100, Loss: 1.219828113913536\n","Epoch 3/100, Loss: 1.2084283232688904\n","Epoch 4/100, Loss: 1.2096672803163528\n","Epoch 5/100, Loss: 1.2048594057559967\n","Epoch 6/100, Loss: 1.2073115110397339\n","Epoch 7/100, Loss: 1.1979638636112213\n","Epoch 8/100, Loss: 1.2070399075746536\n","Epoch 9/100, Loss: 1.2028844058513641\n","Epoch 10/100, Loss: 1.195011630654335\n","Epoch 11/100, Loss: 1.1961524188518524\n","Epoch 12/100, Loss: 1.1974626034498215\n","Epoch 13/100, Loss: 1.1905616223812103\n","Epoch 14/100, Loss: 1.1970850676298141\n","Epoch 15/100, Loss: 1.1949618756771088\n","Epoch 16/100, Loss: 1.1852332651615143\n","Epoch 17/100, Loss: 1.1902673840522766\n","Epoch 18/100, Loss: 1.191851407289505\n","Epoch 19/100, Loss: 1.1906224489212036\n","Epoch 20/100, Loss: 1.1816295683383942\n","Epoch 21/100, Loss: 1.1845643669366837\n","Epoch 22/100, Loss: 1.1825588792562485\n","Epoch 23/100, Loss: 1.1747494786977768\n","Epoch 24/100, Loss: 1.1803551614284515\n","Epoch 25/100, Loss: 1.17354054749012\n","Epoch 26/100, Loss: 1.1736811250448227\n","Epoch 27/100, Loss: 1.1804006397724152\n","Epoch 28/100, Loss: 1.1748032718896866\n","Epoch 29/100, Loss: 1.1756330132484436\n","Epoch 30/100, Loss: 1.1727024167776108\n","Epoch 31/100, Loss: 1.1767559349536896\n","Epoch 32/100, Loss: 1.1708008795976639\n","Epoch 33/100, Loss: 1.1709854304790497\n","Epoch 34/100, Loss: 1.165190801024437\n","Epoch 35/100, Loss: 1.1615299582481384\n","Epoch 36/100, Loss: 1.1611902564764023\n","Epoch 37/100, Loss: 1.1724065393209457\n","Epoch 38/100, Loss: 1.1632487177848816\n","Epoch 39/100, Loss: 1.1585828363895416\n","Epoch 40/100, Loss: 1.157238930463791\n","Epoch 41/100, Loss: 1.1587045788764954\n","Epoch 42/100, Loss: 1.1593715250492096\n","Epoch 43/100, Loss: 1.1546221226453781\n","Epoch 44/100, Loss: 1.1535073220729828\n","Epoch 45/100, Loss: 1.1570272147655487\n","Epoch 46/100, Loss: 1.149595558643341\n","Epoch 47/100, Loss: 1.1503351032733917\n","Epoch 48/100, Loss: 1.155542016029358\n","Epoch 49/100, Loss: 1.1543657779693604\n","Epoch 50/100, Loss: 1.1512019634246826\n","Epoch 51/100, Loss: 1.1527646332979202\n","Epoch 52/100, Loss: 1.140669658780098\n","Epoch 53/100, Loss: 1.1503839194774628\n","Epoch 54/100, Loss: 1.1415271311998367\n","Epoch 55/100, Loss: 1.1444738507270813\n","Epoch 56/100, Loss: 1.1429427415132523\n","Epoch 57/100, Loss: 1.1432466059923172\n","Epoch 58/100, Loss: 1.1451915204524994\n","Epoch 59/100, Loss: 1.138632521033287\n","Epoch 60/100, Loss: 1.1409249603748322\n","Epoch 61/100, Loss: 1.1321107298135757\n","Epoch 62/100, Loss: 1.1408319920301437\n","Epoch 63/100, Loss: 1.1358301788568497\n","Epoch 64/100, Loss: 1.1373007893562317\n","Epoch 65/100, Loss: 1.1310245245695114\n","Epoch 66/100, Loss: 1.140933632850647\n","Epoch 67/100, Loss: 1.1380472928285599\n","Epoch 68/100, Loss: 1.1329584419727325\n","Epoch 69/100, Loss: 1.130422756075859\n","Epoch 70/100, Loss: 1.1273427605628967\n","Epoch 71/100, Loss: 1.1306509375572205\n","Epoch 72/100, Loss: 1.1290653198957443\n","Epoch 73/100, Loss: 1.1276533007621765\n","Epoch 74/100, Loss: 1.1230579614639282\n","Epoch 75/100, Loss: 1.1297871768474579\n","Epoch 76/100, Loss: 1.119614839553833\n","Epoch 77/100, Loss: 1.1244639605283737\n","Epoch 78/100, Loss: 1.1219904869794846\n","Epoch 79/100, Loss: 1.1261965334415436\n","Epoch 80/100, Loss: 1.1204511672258377\n","Epoch 81/100, Loss: 1.1219104528427124\n","Epoch 82/100, Loss: 1.1140583455562592\n","Epoch 83/100, Loss: 1.1234403848648071\n","Epoch 84/100, Loss: 1.1168846189975739\n","Epoch 85/100, Loss: 1.1181662678718567\n","Epoch 86/100, Loss: 1.1218229234218597\n","Epoch 87/100, Loss: 1.1107733249664307\n","Epoch 88/100, Loss: 1.1153292953968048\n","Epoch 89/100, Loss: 1.1069334149360657\n","Epoch 90/100, Loss: 1.1052537560462952\n","Epoch 91/100, Loss: 1.111324429512024\n","Epoch 92/100, Loss: 1.1069594770669937\n","Epoch 93/100, Loss: 1.1139156818389893\n","Epoch 94/100, Loss: 1.1131022274494171\n","Epoch 95/100, Loss: 1.110215812921524\n","Epoch 96/100, Loss: 1.1113940328359604\n","Epoch 97/100, Loss: 1.1102661490440369\n","Epoch 98/100, Loss: 1.1000668555498123\n","Epoch 99/100, Loss: 1.10751973092556\n","Epoch 100/100, Loss: 1.0978947579860687\n","Epoch 1/100, Loss: 1.3458627462387085\n","Epoch 2/100, Loss: 1.3621980398893356\n","Epoch 3/100, Loss: 1.3621980398893356\n","Epoch 4/100, Loss: 1.3621980398893356\n","Epoch 5/100, Loss: 1.3621980398893356\n","Epoch 6/100, Loss: 1.3621980398893356\n","Epoch 7/100, Loss: 1.3621980398893356\n","Epoch 8/100, Loss: 1.3621980398893356\n","Epoch 9/100, Loss: 1.3621980398893356\n","Epoch 10/100, Loss: 1.3621980398893356\n","Epoch 11/100, Loss: 1.3621980398893356\n","Epoch 12/100, Loss: 1.3621980398893356\n","Epoch 13/100, Loss: 1.3621980398893356\n","Epoch 14/100, Loss: 1.3621980398893356\n","Epoch 15/100, Loss: 1.3621980398893356\n","Epoch 16/100, Loss: 1.3621980398893356\n","Epoch 17/100, Loss: 1.3621980398893356\n","Epoch 18/100, Loss: 1.3621980398893356\n","Epoch 19/100, Loss: 1.3621980398893356\n","Epoch 20/100, Loss: 1.3621980398893356\n","Epoch 21/100, Loss: 1.3621980398893356\n","Epoch 22/100, Loss: 1.3621980398893356\n","Epoch 23/100, Loss: 1.3621980398893356\n","Epoch 24/100, Loss: 1.3621980398893356\n","Epoch 25/100, Loss: 1.3621980398893356\n","Epoch 26/100, Loss: 1.3621980398893356\n","Epoch 27/100, Loss: 1.3621980398893356\n","Epoch 28/100, Loss: 1.3621980398893356\n","Epoch 29/100, Loss: 1.3621980398893356\n","Epoch 30/100, Loss: 1.3621980398893356\n","Epoch 31/100, Loss: 1.3621980398893356\n","Epoch 32/100, Loss: 1.3621980398893356\n","Epoch 33/100, Loss: 1.3621980398893356\n","Epoch 34/100, Loss: 1.3621980398893356\n","Epoch 35/100, Loss: 1.3621980398893356\n","Epoch 36/100, Loss: 1.3621980398893356\n","Epoch 37/100, Loss: 1.3621980398893356\n","Epoch 38/100, Loss: 1.3621980398893356\n","Epoch 39/100, Loss: 1.3621980398893356\n","Epoch 40/100, Loss: 1.3621980398893356\n","Epoch 41/100, Loss: 1.3621980398893356\n","Epoch 42/100, Loss: 1.3621980398893356\n","Epoch 43/100, Loss: 1.3621980398893356\n","Epoch 44/100, Loss: 1.3621980398893356\n","Epoch 45/100, Loss: 1.3621980398893356\n","Epoch 46/100, Loss: 1.3621980398893356\n","Epoch 47/100, Loss: 1.3621980398893356\n","Epoch 48/100, Loss: 1.3621980398893356\n","Epoch 49/100, Loss: 1.3621980398893356\n","Epoch 50/100, Loss: 1.3621980398893356\n","Epoch 51/100, Loss: 1.3621980398893356\n","Epoch 52/100, Loss: 1.3621980398893356\n","Epoch 53/100, Loss: 1.3621980398893356\n","Epoch 54/100, Loss: 1.3621980398893356\n","Epoch 55/100, Loss: 1.3621980398893356\n","Epoch 56/100, Loss: 1.3621980398893356\n","Epoch 57/100, Loss: 1.3621980398893356\n","Epoch 58/100, Loss: 1.3621980398893356\n","Epoch 59/100, Loss: 1.3621980398893356\n","Epoch 60/100, Loss: 1.3621980398893356\n","Epoch 61/100, Loss: 1.3621980398893356\n","Epoch 62/100, Loss: 1.3621980398893356\n","Epoch 63/100, Loss: 1.3621980398893356\n","Epoch 64/100, Loss: 1.3621980398893356\n","Epoch 65/100, Loss: 1.3621980398893356\n","Epoch 66/100, Loss: 1.3621980398893356\n","Epoch 67/100, Loss: 1.3621980398893356\n","Epoch 68/100, Loss: 1.3621980398893356\n","Epoch 69/100, Loss: 1.3621980398893356\n","Epoch 70/100, Loss: 1.3621980398893356\n","Epoch 71/100, Loss: 1.3621980398893356\n","Epoch 72/100, Loss: 1.3621980398893356\n","Epoch 73/100, Loss: 1.3621980398893356\n","Epoch 74/100, Loss: 1.3621980398893356\n","Epoch 75/100, Loss: 1.3621980398893356\n","Epoch 76/100, Loss: 1.3621980398893356\n","Epoch 77/100, Loss: 1.3621980398893356\n","Epoch 78/100, Loss: 1.3621980398893356\n","Epoch 79/100, Loss: 1.3621980398893356\n","Epoch 80/100, Loss: 1.3621980398893356\n","Epoch 81/100, Loss: 1.3621980398893356\n","Epoch 82/100, Loss: 1.3621980398893356\n","Epoch 83/100, Loss: 1.3621980398893356\n","Epoch 84/100, Loss: 1.3621980398893356\n","Epoch 85/100, Loss: 1.3621980398893356\n","Epoch 86/100, Loss: 1.3621980398893356\n","Epoch 87/100, Loss: 1.3621980398893356\n","Epoch 88/100, Loss: 1.3621980398893356\n","Epoch 89/100, Loss: 1.3621980398893356\n","Epoch 90/100, Loss: 1.3621980398893356\n","Epoch 91/100, Loss: 1.3621980398893356\n","Epoch 92/100, Loss: 1.3621980398893356\n","Epoch 93/100, Loss: 1.3621980398893356\n","Epoch 94/100, Loss: 1.3621980398893356\n","Epoch 95/100, Loss: 1.3621980398893356\n","Epoch 96/100, Loss: 1.3621980398893356\n","Epoch 97/100, Loss: 1.3621980398893356\n","Epoch 98/100, Loss: 1.3621980398893356\n","Epoch 99/100, Loss: 1.3621980398893356\n","Epoch 100/100, Loss: 1.3621980398893356\n","tensor(0.8874, device='cuda:0') tensor(0.1313, device='cuda:0')\n","Epoch 1/100, Loss: 1.2288947999477386\n","Epoch 2/100, Loss: 1.2207306027412415\n","Epoch 3/100, Loss: 1.2114178389310837\n","Epoch 4/100, Loss: 1.2122103720903397\n","Epoch 5/100, Loss: 1.2127297967672348\n","Epoch 6/100, Loss: 1.2020627409219742\n","Epoch 7/100, Loss: 1.1962390542030334\n","Epoch 8/100, Loss: 1.2075888812541962\n","Epoch 9/100, Loss: 1.205354928970337\n","Epoch 10/100, Loss: 1.1941099911928177\n","Epoch 11/100, Loss: 1.1936146169900894\n","Epoch 12/100, Loss: 1.1960605680942535\n","Epoch 13/100, Loss: 1.1905546635389328\n","Epoch 14/100, Loss: 1.192027136683464\n","Epoch 15/100, Loss: 1.1916428059339523\n","Epoch 16/100, Loss: 1.1868471056222916\n","Epoch 17/100, Loss: 1.1840213537216187\n","Epoch 18/100, Loss: 1.1882746815681458\n","Epoch 19/100, Loss: 1.200359731912613\n","Epoch 20/100, Loss: 1.1911438256502151\n","Epoch 21/100, Loss: 1.1856269389390945\n","Epoch 22/100, Loss: 1.1760527044534683\n","Epoch 23/100, Loss: 1.172749862074852\n","Epoch 24/100, Loss: 1.1816078275442123\n","Epoch 25/100, Loss: 1.1686369478702545\n","Epoch 26/100, Loss: 1.1794865727424622\n","Epoch 27/100, Loss: 1.1719316840171814\n","Epoch 28/100, Loss: 1.1658723801374435\n","Epoch 29/100, Loss: 1.1781029850244522\n","Epoch 30/100, Loss: 1.16782146692276\n","Epoch 31/100, Loss: 1.176236480474472\n","Epoch 32/100, Loss: 1.1772970408201218\n","Epoch 33/100, Loss: 1.166530892252922\n","Epoch 34/100, Loss: 1.1753456741571426\n","Epoch 35/100, Loss: 1.178049460053444\n","Epoch 36/100, Loss: 1.1654175817966461\n","Epoch 37/100, Loss: 1.1572277396917343\n","Epoch 38/100, Loss: 1.1681538820266724\n","Epoch 39/100, Loss: 1.1612819284200668\n","Epoch 40/100, Loss: 1.1569833904504776\n","Epoch 41/100, Loss: 1.15922212600708\n","Epoch 42/100, Loss: 1.1572359204292297\n","Epoch 43/100, Loss: 1.156365916132927\n","Epoch 44/100, Loss: 1.151108592748642\n","Epoch 45/100, Loss: 1.1507541686296463\n","Epoch 46/100, Loss: 1.148343801498413\n","Epoch 47/100, Loss: 1.1584697365760803\n","Epoch 48/100, Loss: 1.1551196426153183\n","Epoch 49/100, Loss: 1.1465214937925339\n","Epoch 50/100, Loss: 1.1466743052005768\n","Epoch 51/100, Loss: 1.1460477262735367\n","Epoch 52/100, Loss: 1.1560963094234467\n","Epoch 53/100, Loss: 1.1367491483688354\n","Epoch 54/100, Loss: 1.1436393857002258\n","Epoch 55/100, Loss: 1.1359381079673767\n","Epoch 56/100, Loss: 1.1460909843444824\n","Epoch 57/100, Loss: 1.1427921056747437\n","Epoch 58/100, Loss: 1.14136803150177\n","Epoch 59/100, Loss: 1.133168250322342\n","Epoch 60/100, Loss: 1.138743743300438\n","Epoch 61/100, Loss: 1.1412025392055511\n","Epoch 62/100, Loss: 1.1359850615262985\n","Epoch 63/100, Loss: 1.1422069221735\n","Epoch 64/100, Loss: 1.1380296796560287\n","Epoch 65/100, Loss: 1.1390689313411713\n","Epoch 66/100, Loss: 1.139316514134407\n","Epoch 67/100, Loss: 1.1390756964683533\n","Epoch 68/100, Loss: 1.1347452700138092\n","Epoch 69/100, Loss: 1.1319416612386703\n","Epoch 70/100, Loss: 1.131694883108139\n","Epoch 71/100, Loss: 1.1237752884626389\n","Epoch 72/100, Loss: 1.1232218444347382\n","Epoch 73/100, Loss: 1.130938172340393\n","Epoch 74/100, Loss: 1.1220766603946686\n","Epoch 75/100, Loss: 1.117733746767044\n","Epoch 76/100, Loss: 1.1222651153802872\n","Epoch 77/100, Loss: 1.1253017038106918\n","Epoch 78/100, Loss: 1.127414733171463\n","Epoch 79/100, Loss: 1.121994987130165\n","Epoch 80/100, Loss: 1.1184220016002655\n","Epoch 81/100, Loss: 1.1193247586488724\n","Epoch 82/100, Loss: 1.117879331111908\n","Epoch 83/100, Loss: 1.1198287904262543\n","Epoch 84/100, Loss: 1.1245441138744354\n","Epoch 85/100, Loss: 1.1189725548028946\n","Epoch 86/100, Loss: 1.1129843592643738\n","Epoch 87/100, Loss: 1.1135054230690002\n","Epoch 88/100, Loss: 1.1201662123203278\n","Epoch 89/100, Loss: 1.1073700040578842\n","Epoch 90/100, Loss: 1.1185179203748703\n","Epoch 91/100, Loss: 1.123536080121994\n","Epoch 92/100, Loss: 1.1201255023479462\n","Epoch 93/100, Loss: 1.1115846037864685\n","Epoch 94/100, Loss: 1.104909986257553\n","Epoch 95/100, Loss: 1.110247403383255\n","Epoch 96/100, Loss: 1.107791781425476\n","Epoch 97/100, Loss: 1.1072816997766495\n","Epoch 98/100, Loss: 1.10418239235878\n","Epoch 99/100, Loss: 1.106752261519432\n","Epoch 100/100, Loss: 1.1027543097734451\n","Epoch 1/100, Loss: 1.3470195829868317\n","Epoch 2/100, Loss: 1.3623647689819336\n","Epoch 3/100, Loss: 1.3623647689819336\n","Epoch 4/100, Loss: 1.3623647689819336\n","Epoch 5/100, Loss: 1.3623647689819336\n","Epoch 6/100, Loss: 1.3623647689819336\n","Epoch 7/100, Loss: 1.3623647689819336\n","Epoch 8/100, Loss: 1.3623647689819336\n","Epoch 9/100, Loss: 1.3623647689819336\n","Epoch 10/100, Loss: 1.3623647689819336\n","Epoch 11/100, Loss: 1.3623647689819336\n","Epoch 12/100, Loss: 1.3623647689819336\n","Epoch 13/100, Loss: 1.3623647689819336\n","Epoch 14/100, Loss: 1.3623647689819336\n","Epoch 15/100, Loss: 1.3623647689819336\n","Epoch 16/100, Loss: 1.3623647689819336\n","Epoch 17/100, Loss: 1.3623647689819336\n","Epoch 18/100, Loss: 1.3623647689819336\n","Epoch 19/100, Loss: 1.3623647689819336\n","Epoch 20/100, Loss: 1.3623647689819336\n","Epoch 21/100, Loss: 1.3623647689819336\n","Epoch 22/100, Loss: 1.3623647689819336\n","Epoch 23/100, Loss: 1.3623647689819336\n","Epoch 24/100, Loss: 1.3623647689819336\n","Epoch 25/100, Loss: 1.3623647689819336\n","Epoch 26/100, Loss: 1.3623647689819336\n","Epoch 27/100, Loss: 1.3623647689819336\n","Epoch 28/100, Loss: 1.3623647689819336\n","Epoch 29/100, Loss: 1.3623647689819336\n","Epoch 30/100, Loss: 1.3623647689819336\n","Epoch 31/100, Loss: 1.3623647689819336\n","Epoch 32/100, Loss: 1.3623647689819336\n","Epoch 33/100, Loss: 1.3623647689819336\n","Epoch 34/100, Loss: 1.3623647689819336\n","Epoch 35/100, Loss: 1.3623647689819336\n","Epoch 36/100, Loss: 1.3623647689819336\n","Epoch 37/100, Loss: 1.3623647689819336\n","Epoch 38/100, Loss: 1.3623647689819336\n","Epoch 39/100, Loss: 1.3623647689819336\n","Epoch 40/100, Loss: 1.3623647689819336\n","Epoch 41/100, Loss: 1.3623647689819336\n","Epoch 42/100, Loss: 1.3623647689819336\n","Epoch 43/100, Loss: 1.3623647689819336\n","Epoch 44/100, Loss: 1.3623647689819336\n","Epoch 45/100, Loss: 1.3623647689819336\n","Epoch 46/100, Loss: 1.3623647689819336\n","Epoch 47/100, Loss: 1.3623647689819336\n","Epoch 48/100, Loss: 1.3623647689819336\n","Epoch 49/100, Loss: 1.3623647689819336\n","Epoch 50/100, Loss: 1.3623647689819336\n","Epoch 51/100, Loss: 1.3623647689819336\n","Epoch 52/100, Loss: 1.3623647689819336\n","Epoch 53/100, Loss: 1.3623647689819336\n","Epoch 54/100, Loss: 1.3623647689819336\n","Epoch 55/100, Loss: 1.3623647689819336\n","Epoch 56/100, Loss: 1.3623647689819336\n","Epoch 57/100, Loss: 1.3623647689819336\n","Epoch 58/100, Loss: 1.3623647689819336\n","Epoch 59/100, Loss: 1.3623647689819336\n","Epoch 60/100, Loss: 1.3623647689819336\n","Epoch 61/100, Loss: 1.3623647689819336\n","Epoch 62/100, Loss: 1.3623647689819336\n","Epoch 63/100, Loss: 1.3623647689819336\n","Epoch 64/100, Loss: 1.3623647689819336\n","Epoch 65/100, Loss: 1.3623647689819336\n","Epoch 66/100, Loss: 1.3623647689819336\n","Epoch 67/100, Loss: 1.3623647689819336\n","Epoch 68/100, Loss: 1.3623647689819336\n","Epoch 69/100, Loss: 1.3623647689819336\n","Epoch 70/100, Loss: 1.3623647689819336\n","Epoch 71/100, Loss: 1.3623647689819336\n","Epoch 72/100, Loss: 1.3623647689819336\n","Epoch 73/100, Loss: 1.3623647689819336\n","Epoch 74/100, Loss: 1.3623647689819336\n","Epoch 75/100, Loss: 1.3623647689819336\n","Epoch 76/100, Loss: 1.3623647689819336\n","Epoch 77/100, Loss: 1.3623647689819336\n","Epoch 78/100, Loss: 1.3623647689819336\n","Epoch 79/100, Loss: 1.3623647689819336\n","Epoch 80/100, Loss: 1.3623647689819336\n","Epoch 81/100, Loss: 1.3623647689819336\n","Epoch 82/100, Loss: 1.3623647689819336\n","Epoch 83/100, Loss: 1.3623647689819336\n","Epoch 84/100, Loss: 1.3623647689819336\n","Epoch 85/100, Loss: 1.3623647689819336\n","Epoch 86/100, Loss: 1.3623647689819336\n","Epoch 87/100, Loss: 1.3623647689819336\n","Epoch 88/100, Loss: 1.3623647689819336\n","Epoch 89/100, Loss: 1.3623647689819336\n","Epoch 90/100, Loss: 1.3623647689819336\n","Epoch 91/100, Loss: 1.3623647689819336\n","Epoch 92/100, Loss: 1.3623647689819336\n","Epoch 93/100, Loss: 1.3623647689819336\n","Epoch 94/100, Loss: 1.3623647689819336\n","Epoch 95/100, Loss: 1.3623647689819336\n","Epoch 96/100, Loss: 1.3623647689819336\n","Epoch 97/100, Loss: 1.3623647689819336\n","Epoch 98/100, Loss: 1.3623647689819336\n","Epoch 99/100, Loss: 1.3623647689819336\n","Epoch 100/100, Loss: 1.3623647689819336\n","tensor(0.8953, device='cuda:0') tensor(0.1313, device='cuda:0')\n","Epoch 1/100, Loss: 1.2318690419197083\n","Epoch 2/100, Loss: 1.215196669101715\n","Epoch 3/100, Loss: 1.2169612795114517\n","Epoch 4/100, Loss: 1.204367145895958\n","Epoch 5/100, Loss: 1.2084565162658691\n","Epoch 6/100, Loss: 1.210273414850235\n","Epoch 7/100, Loss: 1.2106277346611023\n","Epoch 8/100, Loss: 1.2049469202756882\n","Epoch 9/100, Loss: 1.1991659998893738\n","Epoch 10/100, Loss: 1.2031848281621933\n","Epoch 11/100, Loss: 1.1940279453992844\n","Epoch 12/100, Loss: 1.2050669491291046\n","Epoch 13/100, Loss: 1.1938803493976593\n","Epoch 14/100, Loss: 1.1942248493432999\n","Epoch 15/100, Loss: 1.199507787823677\n","Epoch 16/100, Loss: 1.1908198148012161\n","Epoch 17/100, Loss: 1.186118707060814\n","Epoch 18/100, Loss: 1.1889513283967972\n","Epoch 19/100, Loss: 1.1838541477918625\n","Epoch 20/100, Loss: 1.180869609117508\n","Epoch 21/100, Loss: 1.1821641325950623\n","Epoch 22/100, Loss: 1.1829358786344528\n","Epoch 23/100, Loss: 1.1795788556337357\n","Epoch 24/100, Loss: 1.1752735078334808\n","Epoch 25/100, Loss: 1.1801811456680298\n","Epoch 26/100, Loss: 1.1786225140094757\n","Epoch 27/100, Loss: 1.1776211261749268\n","Epoch 28/100, Loss: 1.1889001429080963\n","Epoch 29/100, Loss: 1.1796760857105255\n","Epoch 30/100, Loss: 1.1801276355981827\n","Epoch 31/100, Loss: 1.1716879457235336\n","Epoch 32/100, Loss: 1.175147831439972\n","Epoch 33/100, Loss: 1.17556893825531\n","Epoch 34/100, Loss: 1.171418473124504\n","Epoch 35/100, Loss: 1.1653850078582764\n","Epoch 36/100, Loss: 1.174090951681137\n","Epoch 37/100, Loss: 1.1607885658740997\n","Epoch 38/100, Loss: 1.1526524871587753\n","Epoch 39/100, Loss: 1.1603505462408066\n","Epoch 40/100, Loss: 1.1646147519350052\n","Epoch 41/100, Loss: 1.1630497425794601\n","Epoch 42/100, Loss: 1.1558000445365906\n","Epoch 43/100, Loss: 1.1633960902690887\n","Epoch 44/100, Loss: 1.1581561118364334\n","Epoch 45/100, Loss: 1.152789980173111\n","Epoch 46/100, Loss: 1.153140366077423\n","Epoch 47/100, Loss: 1.1588341444730759\n","Epoch 48/100, Loss: 1.1482162773609161\n","Epoch 49/100, Loss: 1.1539964079856873\n","Epoch 50/100, Loss: 1.1464938074350357\n","Epoch 51/100, Loss: 1.1498951017856598\n","Epoch 52/100, Loss: 1.1568625718355179\n","Epoch 53/100, Loss: 1.1495168954133987\n","Epoch 54/100, Loss: 1.1454723924398422\n","Epoch 55/100, Loss: 1.1489797979593277\n","Epoch 56/100, Loss: 1.1458669304847717\n","Epoch 57/100, Loss: 1.1446814984083176\n","Epoch 58/100, Loss: 1.1459727883338928\n","Epoch 59/100, Loss: 1.1376720517873764\n","Epoch 60/100, Loss: 1.1372549831867218\n","Epoch 61/100, Loss: 1.1383868157863617\n","Epoch 62/100, Loss: 1.1382799297571182\n","Epoch 63/100, Loss: 1.1371149867773056\n","Epoch 64/100, Loss: 1.1362278461456299\n","Epoch 65/100, Loss: 1.1372604817152023\n","Epoch 66/100, Loss: 1.1398910582065582\n","Epoch 67/100, Loss: 1.1286195814609528\n","Epoch 68/100, Loss: 1.131202146410942\n","Epoch 69/100, Loss: 1.1294798702001572\n","Epoch 70/100, Loss: 1.1278686225414276\n","Epoch 71/100, Loss: 1.1337563693523407\n","Epoch 72/100, Loss: 1.1263175159692764\n","Epoch 73/100, Loss: 1.1303691864013672\n","Epoch 74/100, Loss: 1.1244964897632599\n","Epoch 75/100, Loss: 1.1261606514453888\n","Epoch 76/100, Loss: 1.1273880898952484\n","Epoch 77/100, Loss: 1.1148678213357925\n","Epoch 78/100, Loss: 1.12835393846035\n","Epoch 79/100, Loss: 1.1223863065242767\n","Epoch 80/100, Loss: 1.1202967166900635\n","Epoch 81/100, Loss: 1.1168523579835892\n","Epoch 82/100, Loss: 1.1228827983140945\n","Epoch 83/100, Loss: 1.1204289495944977\n","Epoch 84/100, Loss: 1.123822957277298\n","Epoch 85/100, Loss: 1.1225559413433075\n","Epoch 86/100, Loss: 1.1133923828601837\n","Epoch 87/100, Loss: 1.1133716851472855\n","Epoch 88/100, Loss: 1.1161132603883743\n","Epoch 89/100, Loss: 1.120484247803688\n","Epoch 90/100, Loss: 1.1080301105976105\n","Epoch 91/100, Loss: 1.1142731606960297\n","Epoch 92/100, Loss: 1.1142795085906982\n","Epoch 93/100, Loss: 1.1112676858901978\n","Epoch 94/100, Loss: 1.1068244874477386\n","Epoch 95/100, Loss: 1.1027154326438904\n","Epoch 96/100, Loss: 1.1143174320459366\n","Epoch 97/100, Loss: 1.1106617748737335\n","Epoch 98/100, Loss: 1.1101750582456589\n","Epoch 99/100, Loss: 1.1094887852668762\n","Epoch 100/100, Loss: 1.10910302400589\n","Epoch 1/100, Loss: 1.3465338051319122\n","Epoch 2/100, Loss: 1.3625313937664032\n","Epoch 3/100, Loss: 1.3625313937664032\n","Epoch 4/100, Loss: 1.3625313937664032\n","Epoch 5/100, Loss: 1.3625313937664032\n","Epoch 6/100, Loss: 1.3625313937664032\n","Epoch 7/100, Loss: 1.3625313937664032\n","Epoch 8/100, Loss: 1.3625313937664032\n","Epoch 9/100, Loss: 1.3625313937664032\n","Epoch 10/100, Loss: 1.3625313937664032\n","Epoch 11/100, Loss: 1.3625313937664032\n","Epoch 12/100, Loss: 1.3625313937664032\n","Epoch 13/100, Loss: 1.3625313937664032\n","Epoch 14/100, Loss: 1.3625313937664032\n","Epoch 15/100, Loss: 1.3625313937664032\n","Epoch 16/100, Loss: 1.3625313937664032\n","Epoch 17/100, Loss: 1.3625313937664032\n","Epoch 18/100, Loss: 1.3625313937664032\n","Epoch 19/100, Loss: 1.3625313937664032\n","Epoch 20/100, Loss: 1.3625313937664032\n","Epoch 21/100, Loss: 1.3625313937664032\n","Epoch 22/100, Loss: 1.3625313937664032\n","Epoch 23/100, Loss: 1.3625313937664032\n","Epoch 24/100, Loss: 1.3625313937664032\n","Epoch 25/100, Loss: 1.3625313937664032\n","Epoch 26/100, Loss: 1.3625313937664032\n","Epoch 27/100, Loss: 1.3625313937664032\n","Epoch 28/100, Loss: 1.3625313937664032\n","Epoch 29/100, Loss: 1.3625313937664032\n","Epoch 30/100, Loss: 1.3625313937664032\n","Epoch 31/100, Loss: 1.3625313937664032\n","Epoch 32/100, Loss: 1.3625313937664032\n","Epoch 33/100, Loss: 1.3625313937664032\n","Epoch 34/100, Loss: 1.3625313937664032\n","Epoch 35/100, Loss: 1.3625313937664032\n","Epoch 36/100, Loss: 1.3625313937664032\n","Epoch 37/100, Loss: 1.3625313937664032\n","Epoch 38/100, Loss: 1.3625313937664032\n","Epoch 39/100, Loss: 1.3625313937664032\n","Epoch 40/100, Loss: 1.3625313937664032\n","Epoch 41/100, Loss: 1.3625313937664032\n","Epoch 42/100, Loss: 1.3625313937664032\n","Epoch 43/100, Loss: 1.3625313937664032\n","Epoch 44/100, Loss: 1.3625313937664032\n","Epoch 45/100, Loss: 1.3625313937664032\n","Epoch 46/100, Loss: 1.3625313937664032\n","Epoch 47/100, Loss: 1.3625313937664032\n","Epoch 48/100, Loss: 1.3625313937664032\n","Epoch 49/100, Loss: 1.3625313937664032\n","Epoch 50/100, Loss: 1.3625313937664032\n","Epoch 51/100, Loss: 1.3625313937664032\n","Epoch 52/100, Loss: 1.3625313937664032\n","Epoch 53/100, Loss: 1.3625313937664032\n","Epoch 54/100, Loss: 1.3625313937664032\n","Epoch 55/100, Loss: 1.3625313937664032\n","Epoch 56/100, Loss: 1.3625313937664032\n","Epoch 57/100, Loss: 1.3625313937664032\n","Epoch 58/100, Loss: 1.3625313937664032\n","Epoch 59/100, Loss: 1.3625313937664032\n","Epoch 60/100, Loss: 1.3625313937664032\n","Epoch 61/100, Loss: 1.3625313937664032\n","Epoch 62/100, Loss: 1.3625313937664032\n","Epoch 63/100, Loss: 1.3625313937664032\n","Epoch 64/100, Loss: 1.3625313937664032\n","Epoch 65/100, Loss: 1.3625313937664032\n","Epoch 66/100, Loss: 1.3625313937664032\n","Epoch 67/100, Loss: 1.3625313937664032\n","Epoch 68/100, Loss: 1.3625313937664032\n","Epoch 69/100, Loss: 1.3625313937664032\n","Epoch 70/100, Loss: 1.3625313937664032\n","Epoch 71/100, Loss: 1.3625313937664032\n","Epoch 72/100, Loss: 1.3625313937664032\n","Epoch 73/100, Loss: 1.3625313937664032\n","Epoch 74/100, Loss: 1.3625313937664032\n","Epoch 75/100, Loss: 1.3625313937664032\n","Epoch 76/100, Loss: 1.3625313937664032\n","Epoch 77/100, Loss: 1.3625313937664032\n","Epoch 78/100, Loss: 1.3625313937664032\n","Epoch 79/100, Loss: 1.3625313937664032\n","Epoch 80/100, Loss: 1.3625313937664032\n","Epoch 81/100, Loss: 1.3625313937664032\n","Epoch 82/100, Loss: 1.3625313937664032\n","Epoch 83/100, Loss: 1.3625313937664032\n","Epoch 84/100, Loss: 1.3625313937664032\n","Epoch 85/100, Loss: 1.3625313937664032\n","Epoch 86/100, Loss: 1.3625313937664032\n","Epoch 87/100, Loss: 1.3625313937664032\n","Epoch 88/100, Loss: 1.3625313937664032\n","Epoch 89/100, Loss: 1.3625313937664032\n","Epoch 90/100, Loss: 1.3625313937664032\n","Epoch 91/100, Loss: 1.3625313937664032\n","Epoch 92/100, Loss: 1.3625313937664032\n","Epoch 93/100, Loss: 1.3625313937664032\n","Epoch 94/100, Loss: 1.3625313937664032\n","Epoch 95/100, Loss: 1.3625313937664032\n","Epoch 96/100, Loss: 1.3625313937664032\n","Epoch 97/100, Loss: 1.3625313937664032\n","Epoch 98/100, Loss: 1.3625313937664032\n","Epoch 99/100, Loss: 1.3625313937664032\n","Epoch 100/100, Loss: 1.3625313937664032\n","tensor(0.8878, device='cuda:0') tensor(0.1313, device='cuda:0')\n","Epoch 1/100, Loss: 1.2372861504554749\n","Epoch 2/100, Loss: 1.2201695144176483\n","Epoch 3/100, Loss: 1.2180134952068329\n","Epoch 4/100, Loss: 1.2081126421689987\n","Epoch 5/100, Loss: 1.2117146849632263\n","Epoch 6/100, Loss: 1.2141457051038742\n","Epoch 7/100, Loss: 1.2030379921197891\n","Epoch 8/100, Loss: 1.1999147534370422\n","Epoch 9/100, Loss: 1.203765094280243\n","Epoch 10/100, Loss: 1.2029148638248444\n","Epoch 11/100, Loss: 1.2054913192987442\n","Epoch 12/100, Loss: 1.1956516206264496\n","Epoch 13/100, Loss: 1.1977164447307587\n","Epoch 14/100, Loss: 1.182258278131485\n","Epoch 15/100, Loss: 1.1878324151039124\n","Epoch 16/100, Loss: 1.1838339865207672\n","Epoch 17/100, Loss: 1.1825215220451355\n","Epoch 18/100, Loss: 1.1886476427316666\n","Epoch 19/100, Loss: 1.1867864578962326\n","Epoch 20/100, Loss: 1.1894854754209518\n","Epoch 21/100, Loss: 1.172605261206627\n","Epoch 22/100, Loss: 1.1856607645750046\n","Epoch 23/100, Loss: 1.1745254695415497\n","Epoch 24/100, Loss: 1.1838977634906769\n","Epoch 25/100, Loss: 1.177157536149025\n","Epoch 26/100, Loss: 1.1758254617452621\n","Epoch 27/100, Loss: 1.163879930973053\n","Epoch 28/100, Loss: 1.1818602532148361\n","Epoch 29/100, Loss: 1.1692016869783401\n","Epoch 30/100, Loss: 1.177229717373848\n","Epoch 31/100, Loss: 1.1708897054195404\n","Epoch 32/100, Loss: 1.1656321436166763\n","Epoch 33/100, Loss: 1.169010415673256\n","Epoch 34/100, Loss: 1.1646788418293\n","Epoch 35/100, Loss: 1.163549080491066\n","Epoch 36/100, Loss: 1.1526635438203812\n","Epoch 37/100, Loss: 1.1616450250148773\n","Epoch 38/100, Loss: 1.1655453145503998\n","Epoch 39/100, Loss: 1.171276330947876\n","Epoch 40/100, Loss: 1.169446423649788\n","Epoch 41/100, Loss: 1.1607172191143036\n","Epoch 42/100, Loss: 1.155948430299759\n","Epoch 43/100, Loss: 1.154925674200058\n","Epoch 44/100, Loss: 1.1615089625120163\n","Epoch 45/100, Loss: 1.1456461399793625\n","Epoch 46/100, Loss: 1.1614457815885544\n","Epoch 47/100, Loss: 1.152685061097145\n","Epoch 48/100, Loss: 1.1579295992851257\n","Epoch 49/100, Loss: 1.153045266866684\n","Epoch 50/100, Loss: 1.1480555981397629\n","Epoch 51/100, Loss: 1.1448295265436172\n","Epoch 52/100, Loss: 1.144545242190361\n","Epoch 53/100, Loss: 1.1536735743284225\n","Epoch 54/100, Loss: 1.1404796093702316\n","Epoch 55/100, Loss: 1.1474842578172684\n","Epoch 56/100, Loss: 1.1513297110795975\n","Epoch 57/100, Loss: 1.1369766294956207\n","Epoch 58/100, Loss: 1.1362744569778442\n","Epoch 59/100, Loss: 1.1420214176177979\n","Epoch 60/100, Loss: 1.1458545625209808\n","Epoch 61/100, Loss: 1.1438729763031006\n","Epoch 62/100, Loss: 1.1377433687448502\n","Epoch 63/100, Loss: 1.1365002691745758\n","Epoch 64/100, Loss: 1.1505164802074432\n","Epoch 65/100, Loss: 1.128232553601265\n","Epoch 66/100, Loss: 1.1356241255998611\n","Epoch 67/100, Loss: 1.140555590391159\n","Epoch 68/100, Loss: 1.1269261240959167\n","Epoch 69/100, Loss: 1.1287018954753876\n","Epoch 70/100, Loss: 1.1345719397068024\n","Epoch 71/100, Loss: 1.1260157078504562\n","Epoch 72/100, Loss: 1.1413422673940659\n","Epoch 73/100, Loss: 1.1331701874732971\n","Epoch 74/100, Loss: 1.1259559988975525\n","Epoch 75/100, Loss: 1.1295691132545471\n","Epoch 76/100, Loss: 1.1214497089385986\n","Epoch 77/100, Loss: 1.1251140087842941\n","Epoch 78/100, Loss: 1.1306867003440857\n","Epoch 79/100, Loss: 1.1254435926675797\n","Epoch 80/100, Loss: 1.1209109723567963\n","Epoch 81/100, Loss: 1.1221629083156586\n","Epoch 82/100, Loss: 1.124367132782936\n","Epoch 83/100, Loss: 1.1215637028217316\n","Epoch 84/100, Loss: 1.1142211258411407\n","Epoch 85/100, Loss: 1.1140315383672714\n","Epoch 86/100, Loss: 1.1147541254758835\n","Epoch 87/100, Loss: 1.1135518103837967\n","Epoch 88/100, Loss: 1.1173767298460007\n","Epoch 89/100, Loss: 1.1092137545347214\n","Epoch 90/100, Loss: 1.1129501014947891\n","Epoch 91/100, Loss: 1.1135922372341156\n","Epoch 92/100, Loss: 1.1108238995075226\n","Epoch 93/100, Loss: 1.112303614616394\n","Epoch 94/100, Loss: 1.1034813970327377\n","Epoch 95/100, Loss: 1.106606125831604\n","Epoch 96/100, Loss: 1.1099025160074234\n","Epoch 97/100, Loss: 1.1094595193862915\n","Epoch 98/100, Loss: 1.1138856261968613\n","Epoch 99/100, Loss: 1.1131559759378433\n","Epoch 100/100, Loss: 1.1066229045391083\n","Epoch 1/100, Loss: 1.346516728401184\n","Epoch 2/100, Loss: 1.3626981228590012\n","Epoch 3/100, Loss: 1.3626981228590012\n","Epoch 4/100, Loss: 1.3626981228590012\n","Epoch 5/100, Loss: 1.3626981228590012\n","Epoch 6/100, Loss: 1.3626981228590012\n","Epoch 7/100, Loss: 1.3626981228590012\n","Epoch 8/100, Loss: 1.3626981228590012\n","Epoch 9/100, Loss: 1.3626981228590012\n","Epoch 10/100, Loss: 1.3626981228590012\n","Epoch 11/100, Loss: 1.3626981228590012\n","Epoch 12/100, Loss: 1.3626981228590012\n","Epoch 13/100, Loss: 1.3626981228590012\n","Epoch 14/100, Loss: 1.3626981228590012\n","Epoch 15/100, Loss: 1.3626981228590012\n","Epoch 16/100, Loss: 1.3626981228590012\n","Epoch 17/100, Loss: 1.3626981228590012\n","Epoch 18/100, Loss: 1.3626981228590012\n","Epoch 19/100, Loss: 1.3626981228590012\n","Epoch 20/100, Loss: 1.3626981228590012\n","Epoch 21/100, Loss: 1.3626981228590012\n","Epoch 22/100, Loss: 1.3626981228590012\n","Epoch 23/100, Loss: 1.3626981228590012\n","Epoch 24/100, Loss: 1.3626981228590012\n","Epoch 25/100, Loss: 1.3626981228590012\n","Epoch 26/100, Loss: 1.3626981228590012\n","Epoch 27/100, Loss: 1.3626981228590012\n","Epoch 28/100, Loss: 1.3626981228590012\n","Epoch 29/100, Loss: 1.3626981228590012\n","Epoch 30/100, Loss: 1.3626981228590012\n","Epoch 31/100, Loss: 1.3626981228590012\n","Epoch 32/100, Loss: 1.3626981228590012\n","Epoch 33/100, Loss: 1.3626981228590012\n","Epoch 34/100, Loss: 1.3626981228590012\n","Epoch 35/100, Loss: 1.3626981228590012\n","Epoch 36/100, Loss: 1.3626981228590012\n","Epoch 37/100, Loss: 1.3626981228590012\n","Epoch 38/100, Loss: 1.3626981228590012\n","Epoch 39/100, Loss: 1.3626981228590012\n","Epoch 40/100, Loss: 1.3626981228590012\n","Epoch 41/100, Loss: 1.3626981228590012\n","Epoch 42/100, Loss: 1.3626981228590012\n","Epoch 43/100, Loss: 1.3626981228590012\n","Epoch 44/100, Loss: 1.3626981228590012\n","Epoch 45/100, Loss: 1.3626981228590012\n","Epoch 46/100, Loss: 1.3626981228590012\n","Epoch 47/100, Loss: 1.3626981228590012\n","Epoch 48/100, Loss: 1.3626981228590012\n","Epoch 49/100, Loss: 1.3626981228590012\n","Epoch 50/100, Loss: 1.3626981228590012\n","Epoch 51/100, Loss: 1.3626981228590012\n","Epoch 52/100, Loss: 1.3626981228590012\n","Epoch 53/100, Loss: 1.3626981228590012\n","Epoch 54/100, Loss: 1.3626981228590012\n","Epoch 55/100, Loss: 1.3626981228590012\n","Epoch 56/100, Loss: 1.3626981228590012\n","Epoch 57/100, Loss: 1.3626981228590012\n","Epoch 58/100, Loss: 1.3626981228590012\n","Epoch 59/100, Loss: 1.3626981228590012\n","Epoch 60/100, Loss: 1.3626981228590012\n","Epoch 61/100, Loss: 1.3626981228590012\n","Epoch 62/100, Loss: 1.3626981228590012\n","Epoch 63/100, Loss: 1.3626981228590012\n","Epoch 64/100, Loss: 1.3626981228590012\n","Epoch 65/100, Loss: 1.3626981228590012\n","Epoch 66/100, Loss: 1.3626981228590012\n","Epoch 67/100, Loss: 1.3626981228590012\n","Epoch 68/100, Loss: 1.3626981228590012\n","Epoch 69/100, Loss: 1.3626981228590012\n","Epoch 70/100, Loss: 1.3626981228590012\n","Epoch 71/100, Loss: 1.3626981228590012\n","Epoch 72/100, Loss: 1.3626981228590012\n","Epoch 73/100, Loss: 1.3626981228590012\n","Epoch 74/100, Loss: 1.3626981228590012\n","Epoch 75/100, Loss: 1.3626981228590012\n","Epoch 76/100, Loss: 1.3626981228590012\n","Epoch 77/100, Loss: 1.3626981228590012\n","Epoch 78/100, Loss: 1.3626981228590012\n","Epoch 79/100, Loss: 1.3626981228590012\n","Epoch 80/100, Loss: 1.3626981228590012\n","Epoch 81/100, Loss: 1.3626981228590012\n","Epoch 82/100, Loss: 1.3626981228590012\n","Epoch 83/100, Loss: 1.3626981228590012\n","Epoch 84/100, Loss: 1.3626981228590012\n","Epoch 85/100, Loss: 1.3626981228590012\n","Epoch 86/100, Loss: 1.3626981228590012\n","Epoch 87/100, Loss: 1.3626981228590012\n","Epoch 88/100, Loss: 1.3626981228590012\n","Epoch 89/100, Loss: 1.3626981228590012\n","Epoch 90/100, Loss: 1.3626981228590012\n","Epoch 91/100, Loss: 1.3626981228590012\n","Epoch 92/100, Loss: 1.3626981228590012\n","Epoch 93/100, Loss: 1.3626981228590012\n","Epoch 94/100, Loss: 1.3626981228590012\n","Epoch 95/100, Loss: 1.3626981228590012\n","Epoch 96/100, Loss: 1.3626981228590012\n","Epoch 97/100, Loss: 1.3626981228590012\n","Epoch 98/100, Loss: 1.3626981228590012\n","Epoch 99/100, Loss: 1.3626981228590012\n","Epoch 100/100, Loss: 1.3626981228590012\n","tensor(0.8928, device='cuda:0') tensor(0.1313, device='cuda:0')\n","Epoch 1/100, Loss: 1.2372599393129349\n","Epoch 2/100, Loss: 1.2233732640743256\n","Epoch 3/100, Loss: 1.2206849157810211\n","Epoch 4/100, Loss: 1.208674892783165\n","Epoch 5/100, Loss: 1.2040733993053436\n","Epoch 6/100, Loss: 1.2053473144769669\n","Epoch 7/100, Loss: 1.2031428068876266\n","Epoch 8/100, Loss: 1.201919049024582\n","Epoch 9/100, Loss: 1.200926348567009\n","Epoch 10/100, Loss: 1.1987340450286865\n","Epoch 11/100, Loss: 1.1994417011737823\n","Epoch 12/100, Loss: 1.1932132542133331\n","Epoch 13/100, Loss: 1.1953180432319641\n","Epoch 14/100, Loss: 1.1833286881446838\n","Epoch 15/100, Loss: 1.1879085451364517\n","Epoch 16/100, Loss: 1.1904682964086533\n","Epoch 17/100, Loss: 1.190243661403656\n","Epoch 18/100, Loss: 1.1843123733997345\n","Epoch 19/100, Loss: 1.1840786635875702\n","Epoch 20/100, Loss: 1.180570811033249\n","Epoch 21/100, Loss: 1.1828923225402832\n","Epoch 22/100, Loss: 1.177773728966713\n","Epoch 23/100, Loss: 1.1757092475891113\n","Epoch 24/100, Loss: 1.1801339238882065\n","Epoch 25/100, Loss: 1.1730452477931976\n","Epoch 26/100, Loss: 1.1710026264190674\n","Epoch 27/100, Loss: 1.1657580584287643\n","Epoch 28/100, Loss: 1.1696771532297134\n","Epoch 29/100, Loss: 1.1727012395858765\n","Epoch 30/100, Loss: 1.1622585952281952\n","Epoch 31/100, Loss: 1.1666378378868103\n","Epoch 32/100, Loss: 1.164849266409874\n","Epoch 33/100, Loss: 1.1676304638385773\n","Epoch 34/100, Loss: 1.1740868538618088\n","Epoch 35/100, Loss: 1.1659119725227356\n","Epoch 36/100, Loss: 1.172138825058937\n","Epoch 37/100, Loss: 1.160648226737976\n","Epoch 38/100, Loss: 1.1609911769628525\n","Epoch 39/100, Loss: 1.165170669555664\n","Epoch 40/100, Loss: 1.1611483693122864\n","Epoch 41/100, Loss: 1.142140582203865\n","Epoch 42/100, Loss: 1.162012666463852\n","Epoch 43/100, Loss: 1.1527734249830246\n","Epoch 44/100, Loss: 1.1488537341356277\n","Epoch 45/100, Loss: 1.1567644774913788\n","Epoch 46/100, Loss: 1.1511903405189514\n","Epoch 47/100, Loss: 1.1570027023553848\n","Epoch 48/100, Loss: 1.1512154340744019\n","Epoch 49/100, Loss: 1.1531315743923187\n","Epoch 50/100, Loss: 1.1528429836034775\n","Epoch 51/100, Loss: 1.1462703347206116\n","Epoch 52/100, Loss: 1.154007613658905\n","Epoch 53/100, Loss: 1.1508329659700394\n","Epoch 54/100, Loss: 1.1469142585992813\n","Epoch 55/100, Loss: 1.1398538649082184\n","Epoch 56/100, Loss: 1.1463809311389923\n","Epoch 57/100, Loss: 1.1394763886928558\n","Epoch 58/100, Loss: 1.1423284858465195\n","Epoch 59/100, Loss: 1.1426321864128113\n","Epoch 60/100, Loss: 1.132804125547409\n","Epoch 61/100, Loss: 1.146467924118042\n","Epoch 62/100, Loss: 1.1431868523359299\n","Epoch 63/100, Loss: 1.1344892084598541\n","Epoch 64/100, Loss: 1.132402703166008\n","Epoch 65/100, Loss: 1.1272981315851212\n","Epoch 66/100, Loss: 1.1300909966230392\n","Epoch 67/100, Loss: 1.1292360723018646\n","Epoch 68/100, Loss: 1.1266984939575195\n","Epoch 69/100, Loss: 1.1321174055337906\n","Epoch 70/100, Loss: 1.1217885315418243\n","Epoch 71/100, Loss: 1.1284181326627731\n","Epoch 72/100, Loss: 1.1311140209436417\n","Epoch 73/100, Loss: 1.1267889589071274\n","Epoch 74/100, Loss: 1.1335874795913696\n","Epoch 75/100, Loss: 1.1274356245994568\n","Epoch 76/100, Loss: 1.120964989066124\n","Epoch 77/100, Loss: 1.1250160038471222\n","Epoch 78/100, Loss: 1.122516691684723\n","Epoch 79/100, Loss: 1.1283473074436188\n","Epoch 80/100, Loss: 1.1190068870782852\n","Epoch 81/100, Loss: 1.1225432753562927\n","Epoch 82/100, Loss: 1.118604987859726\n","Epoch 83/100, Loss: 1.1217432171106339\n","Epoch 84/100, Loss: 1.1171692460775375\n","Epoch 85/100, Loss: 1.114108756184578\n","Epoch 86/100, Loss: 1.1132738441228867\n","Epoch 87/100, Loss: 1.1128339171409607\n","Epoch 88/100, Loss: 1.1115502417087555\n","Epoch 89/100, Loss: 1.109464704990387\n","Epoch 90/100, Loss: 1.1101146936416626\n","Epoch 91/100, Loss: 1.1107796877622604\n","Epoch 92/100, Loss: 1.116463541984558\n","Epoch 93/100, Loss: 1.112745314836502\n","Epoch 94/100, Loss: 1.1168192625045776\n","Epoch 95/100, Loss: 1.1072285920381546\n","Epoch 96/100, Loss: 1.1137799173593521\n","Epoch 97/100, Loss: 1.1117839217185974\n","Epoch 98/100, Loss: 1.1020842045545578\n","Epoch 99/100, Loss: 1.1144639253616333\n","Epoch 100/100, Loss: 1.1066962331533432\n","Epoch 1/100, Loss: 1.3463948369026184\n","Epoch 2/100, Loss: 1.3628647774457932\n","Epoch 3/100, Loss: 1.3628647774457932\n","Epoch 4/100, Loss: 1.3628647774457932\n","Epoch 5/100, Loss: 1.3628647774457932\n","Epoch 6/100, Loss: 1.3628647774457932\n","Epoch 7/100, Loss: 1.3628647774457932\n","Epoch 8/100, Loss: 1.3628647774457932\n","Epoch 9/100, Loss: 1.3628647774457932\n","Epoch 10/100, Loss: 1.3628647774457932\n","Epoch 11/100, Loss: 1.3628647774457932\n","Epoch 12/100, Loss: 1.3628647774457932\n","Epoch 13/100, Loss: 1.3628647774457932\n","Epoch 14/100, Loss: 1.3628647774457932\n","Epoch 15/100, Loss: 1.3628647774457932\n","Epoch 16/100, Loss: 1.3628647774457932\n","Epoch 17/100, Loss: 1.3628647774457932\n","Epoch 18/100, Loss: 1.3628647774457932\n","Epoch 19/100, Loss: 1.3628647774457932\n","Epoch 20/100, Loss: 1.3628647774457932\n","Epoch 21/100, Loss: 1.3628647774457932\n","Epoch 22/100, Loss: 1.3628647774457932\n","Epoch 23/100, Loss: 1.3628647774457932\n","Epoch 24/100, Loss: 1.3628647774457932\n","Epoch 25/100, Loss: 1.3628647774457932\n","Epoch 26/100, Loss: 1.3628647774457932\n","Epoch 27/100, Loss: 1.3628647774457932\n","Epoch 28/100, Loss: 1.3628647774457932\n","Epoch 29/100, Loss: 1.3628647774457932\n","Epoch 30/100, Loss: 1.3628647774457932\n","Epoch 31/100, Loss: 1.3628647774457932\n","Epoch 32/100, Loss: 1.3628647774457932\n","Epoch 33/100, Loss: 1.3628647774457932\n","Epoch 34/100, Loss: 1.3628647774457932\n","Epoch 35/100, Loss: 1.3628647774457932\n","Epoch 36/100, Loss: 1.3628647774457932\n","Epoch 37/100, Loss: 1.3628647774457932\n","Epoch 38/100, Loss: 1.3628647774457932\n","Epoch 39/100, Loss: 1.3628647774457932\n","Epoch 40/100, Loss: 1.3628647774457932\n","Epoch 41/100, Loss: 1.3628647774457932\n","Epoch 42/100, Loss: 1.3628647774457932\n","Epoch 43/100, Loss: 1.3628647774457932\n","Epoch 44/100, Loss: 1.3628647774457932\n","Epoch 45/100, Loss: 1.3628647774457932\n","Epoch 46/100, Loss: 1.3628647774457932\n","Epoch 47/100, Loss: 1.3628647774457932\n","Epoch 48/100, Loss: 1.3628647774457932\n","Epoch 49/100, Loss: 1.3628647774457932\n","Epoch 50/100, Loss: 1.3628647774457932\n","Epoch 51/100, Loss: 1.3628647774457932\n","Epoch 52/100, Loss: 1.3628647774457932\n","Epoch 53/100, Loss: 1.3628647774457932\n","Epoch 54/100, Loss: 1.3628647774457932\n","Epoch 55/100, Loss: 1.3628647774457932\n","Epoch 56/100, Loss: 1.3628647774457932\n","Epoch 57/100, Loss: 1.3628647774457932\n","Epoch 58/100, Loss: 1.3628647774457932\n","Epoch 59/100, Loss: 1.3628647774457932\n","Epoch 60/100, Loss: 1.3628647774457932\n","Epoch 61/100, Loss: 1.3628647774457932\n","Epoch 62/100, Loss: 1.3628647774457932\n","Epoch 63/100, Loss: 1.3628647774457932\n","Epoch 64/100, Loss: 1.3628647774457932\n","Epoch 65/100, Loss: 1.3628647774457932\n","Epoch 66/100, Loss: 1.3628647774457932\n","Epoch 67/100, Loss: 1.3628647774457932\n","Epoch 68/100, Loss: 1.3628647774457932\n","Epoch 69/100, Loss: 1.3628647774457932\n","Epoch 70/100, Loss: 1.3628647774457932\n","Epoch 71/100, Loss: 1.3628647774457932\n","Epoch 72/100, Loss: 1.3628647774457932\n","Epoch 73/100, Loss: 1.3628647774457932\n","Epoch 74/100, Loss: 1.3628647774457932\n","Epoch 75/100, Loss: 1.3628647774457932\n","Epoch 76/100, Loss: 1.3628647774457932\n","Epoch 77/100, Loss: 1.3628647774457932\n","Epoch 78/100, Loss: 1.3628647774457932\n","Epoch 79/100, Loss: 1.3628647774457932\n","Epoch 80/100, Loss: 1.3628647774457932\n","Epoch 81/100, Loss: 1.3628647774457932\n","Epoch 82/100, Loss: 1.3628647774457932\n","Epoch 83/100, Loss: 1.3628647774457932\n","Epoch 84/100, Loss: 1.3628647774457932\n","Epoch 85/100, Loss: 1.3628647774457932\n","Epoch 86/100, Loss: 1.3628647774457932\n","Epoch 87/100, Loss: 1.3628647774457932\n","Epoch 88/100, Loss: 1.3628647774457932\n","Epoch 89/100, Loss: 1.3628647774457932\n","Epoch 90/100, Loss: 1.3628647774457932\n","Epoch 91/100, Loss: 1.3628647774457932\n","Epoch 92/100, Loss: 1.3628647774457932\n","Epoch 93/100, Loss: 1.3628647774457932\n","Epoch 94/100, Loss: 1.3628647774457932\n","Epoch 95/100, Loss: 1.3628647774457932\n","Epoch 96/100, Loss: 1.3628647774457932\n","Epoch 97/100, Loss: 1.3628647774457932\n","Epoch 98/100, Loss: 1.3628647774457932\n","Epoch 99/100, Loss: 1.3628647774457932\n","Epoch 100/100, Loss: 1.3628647774457932\n","tensor(0.9060, device='cuda:0') tensor(0.1313, device='cuda:0')\n","Epoch 1/100, Loss: 1.2312069833278656\n","Epoch 2/100, Loss: 1.2228476703166962\n","Epoch 3/100, Loss: 1.2142380326986313\n","Epoch 4/100, Loss: 1.2140668034553528\n","Epoch 5/100, Loss: 1.2134961932897568\n","Epoch 6/100, Loss: 1.2103255987167358\n","Epoch 7/100, Loss: 1.2109281569719315\n","Epoch 8/100, Loss: 1.2108266949653625\n","Epoch 9/100, Loss: 1.20230133831501\n","Epoch 10/100, Loss: 1.1969954073429108\n","Epoch 11/100, Loss: 1.1964558213949203\n","Epoch 12/100, Loss: 1.1963206678628922\n","Epoch 13/100, Loss: 1.1874148398637772\n","Epoch 14/100, Loss: 1.190554454922676\n","Epoch 15/100, Loss: 1.1944621056318283\n","Epoch 16/100, Loss: 1.1931865215301514\n","Epoch 17/100, Loss: 1.1861771494150162\n","Epoch 18/100, Loss: 1.1871345192193985\n","Epoch 19/100, Loss: 1.1842290759086609\n","Epoch 20/100, Loss: 1.1883016526699066\n","Epoch 21/100, Loss: 1.1860088855028152\n","Epoch 22/100, Loss: 1.1963125318288803\n","Epoch 23/100, Loss: 1.1750147938728333\n","Epoch 24/100, Loss: 1.1763979345560074\n","Epoch 25/100, Loss: 1.1738416403532028\n","Epoch 26/100, Loss: 1.1749955117702484\n","Epoch 27/100, Loss: 1.1654334962368011\n","Epoch 28/100, Loss: 1.175134763121605\n","Epoch 29/100, Loss: 1.1620677560567856\n","Epoch 30/100, Loss: 1.1666352152824402\n","Epoch 31/100, Loss: 1.167502224445343\n","Epoch 32/100, Loss: 1.1781082153320312\n","Epoch 33/100, Loss: 1.1653591245412827\n","Epoch 34/100, Loss: 1.1722391992807388\n","Epoch 35/100, Loss: 1.1620082259178162\n","Epoch 36/100, Loss: 1.1655244082212448\n","Epoch 37/100, Loss: 1.1640394479036331\n","Epoch 38/100, Loss: 1.1627682149410248\n","Epoch 39/100, Loss: 1.1701443195343018\n","Epoch 40/100, Loss: 1.157592624425888\n","Epoch 41/100, Loss: 1.1555719077587128\n","Epoch 42/100, Loss: 1.1649809330701828\n","Epoch 43/100, Loss: 1.1667414456605911\n","Epoch 44/100, Loss: 1.1549500524997711\n","Epoch 45/100, Loss: 1.1536292284727097\n","Epoch 46/100, Loss: 1.1539044827222824\n","Epoch 47/100, Loss: 1.1583277136087418\n","Epoch 48/100, Loss: 1.15519118309021\n","Epoch 49/100, Loss: 1.1497652977705002\n","Epoch 50/100, Loss: 1.1508314460515976\n","Epoch 51/100, Loss: 1.147360622882843\n","Epoch 52/100, Loss: 1.138908639550209\n","Epoch 53/100, Loss: 1.1408454775810242\n","Epoch 54/100, Loss: 1.1475221812725067\n","Epoch 55/100, Loss: 1.1526321321725845\n","Epoch 56/100, Loss: 1.1512935012578964\n","Epoch 57/100, Loss: 1.1377816498279572\n","Epoch 58/100, Loss: 1.1438436061143875\n","Epoch 59/100, Loss: 1.146129086613655\n","Epoch 60/100, Loss: 1.1389701515436172\n","Epoch 61/100, Loss: 1.1322462558746338\n","Epoch 62/100, Loss: 1.1388694643974304\n","Epoch 63/100, Loss: 1.1363111734390259\n","Epoch 64/100, Loss: 1.1457684636116028\n","Epoch 65/100, Loss: 1.1349927335977554\n","Epoch 66/100, Loss: 1.1372189670801163\n","Epoch 67/100, Loss: 1.131255865097046\n","Epoch 68/100, Loss: 1.1344666332006454\n","Epoch 69/100, Loss: 1.1370521038770676\n","Epoch 70/100, Loss: 1.1255772709846497\n","Epoch 71/100, Loss: 1.1287743896245956\n","Epoch 72/100, Loss: 1.1222303658723831\n","Epoch 73/100, Loss: 1.1232244819402695\n","Epoch 74/100, Loss: 1.1174743920564651\n","Epoch 75/100, Loss: 1.1285039186477661\n","Epoch 76/100, Loss: 1.1234442740678787\n","Epoch 77/100, Loss: 1.126256912946701\n","Epoch 78/100, Loss: 1.126813367009163\n","Epoch 79/100, Loss: 1.1248472481966019\n","Epoch 80/100, Loss: 1.118955910205841\n","Epoch 81/100, Loss: 1.119245857000351\n","Epoch 82/100, Loss: 1.125279113650322\n","Epoch 83/100, Loss: 1.1233290135860443\n","Epoch 84/100, Loss: 1.1228390783071518\n","Epoch 85/100, Loss: 1.116468921303749\n","Epoch 86/100, Loss: 1.1158337444067001\n","Epoch 87/100, Loss: 1.1171131283044815\n","Epoch 88/100, Loss: 1.1077433377504349\n","Epoch 89/100, Loss: 1.1185767650604248\n","Epoch 90/100, Loss: 1.117820456624031\n","Epoch 91/100, Loss: 1.1083443611860275\n","Epoch 92/100, Loss: 1.1147227138280869\n","Epoch 93/100, Loss: 1.1128325909376144\n","Epoch 94/100, Loss: 1.1082831174135208\n","Epoch 95/100, Loss: 1.111770138144493\n","Epoch 96/100, Loss: 1.103862538933754\n","Epoch 97/100, Loss: 1.104295164346695\n","Epoch 98/100, Loss: 1.10904061794281\n","Epoch 99/100, Loss: 1.106845661997795\n","Epoch 100/100, Loss: 1.104094222187996\n","Epoch 1/100, Loss: 1.3455618917942047\n","Epoch 2/100, Loss: 1.3630315065383911\n","Epoch 3/100, Loss: 1.3630315065383911\n","Epoch 4/100, Loss: 1.3630315065383911\n","Epoch 5/100, Loss: 1.3630315065383911\n","Epoch 6/100, Loss: 1.3630315065383911\n","Epoch 7/100, Loss: 1.3630315065383911\n","Epoch 8/100, Loss: 1.3630315065383911\n","Epoch 9/100, Loss: 1.3630315065383911\n","Epoch 10/100, Loss: 1.3630315065383911\n","Epoch 11/100, Loss: 1.3630315065383911\n","Epoch 12/100, Loss: 1.3630315065383911\n","Epoch 13/100, Loss: 1.3630315065383911\n","Epoch 14/100, Loss: 1.3630315065383911\n","Epoch 15/100, Loss: 1.3630315065383911\n","Epoch 16/100, Loss: 1.3630315065383911\n","Epoch 17/100, Loss: 1.3630315065383911\n","Epoch 18/100, Loss: 1.3630315065383911\n","Epoch 19/100, Loss: 1.3630315065383911\n","Epoch 20/100, Loss: 1.3630315065383911\n","Epoch 21/100, Loss: 1.3630315065383911\n","Epoch 22/100, Loss: 1.3630315065383911\n","Epoch 23/100, Loss: 1.3630315065383911\n","Epoch 24/100, Loss: 1.3630315065383911\n","Epoch 25/100, Loss: 1.3630315065383911\n","Epoch 26/100, Loss: 1.3630315065383911\n","Epoch 27/100, Loss: 1.3630315065383911\n","Epoch 28/100, Loss: 1.3630315065383911\n","Epoch 29/100, Loss: 1.3630315065383911\n","Epoch 30/100, Loss: 1.3630315065383911\n","Epoch 31/100, Loss: 1.3630315065383911\n","Epoch 32/100, Loss: 1.3630315065383911\n","Epoch 33/100, Loss: 1.3630315065383911\n","Epoch 34/100, Loss: 1.3630315065383911\n","Epoch 35/100, Loss: 1.3630315065383911\n","Epoch 36/100, Loss: 1.3630315065383911\n","Epoch 37/100, Loss: 1.3630315065383911\n","Epoch 38/100, Loss: 1.3630315065383911\n","Epoch 39/100, Loss: 1.3630315065383911\n","Epoch 40/100, Loss: 1.3630315065383911\n","Epoch 41/100, Loss: 1.3630315065383911\n","Epoch 42/100, Loss: 1.3630315065383911\n","Epoch 43/100, Loss: 1.3630315065383911\n","Epoch 44/100, Loss: 1.3630315065383911\n","Epoch 45/100, Loss: 1.3630315065383911\n","Epoch 46/100, Loss: 1.3630315065383911\n","Epoch 47/100, Loss: 1.3630315065383911\n","Epoch 48/100, Loss: 1.3630315065383911\n","Epoch 49/100, Loss: 1.3630315065383911\n","Epoch 50/100, Loss: 1.3630315065383911\n","Epoch 51/100, Loss: 1.3630315065383911\n","Epoch 52/100, Loss: 1.3630315065383911\n","Epoch 53/100, Loss: 1.3630315065383911\n","Epoch 54/100, Loss: 1.3630315065383911\n","Epoch 55/100, Loss: 1.3630315065383911\n","Epoch 56/100, Loss: 1.3630315065383911\n","Epoch 57/100, Loss: 1.3630315065383911\n","Epoch 58/100, Loss: 1.3630315065383911\n","Epoch 59/100, Loss: 1.3630315065383911\n","Epoch 60/100, Loss: 1.3630315065383911\n","Epoch 61/100, Loss: 1.3630315065383911\n","Epoch 62/100, Loss: 1.3630315065383911\n","Epoch 63/100, Loss: 1.3630315065383911\n","Epoch 64/100, Loss: 1.3630315065383911\n","Epoch 65/100, Loss: 1.3630315065383911\n","Epoch 66/100, Loss: 1.3630315065383911\n","Epoch 67/100, Loss: 1.3630315065383911\n","Epoch 68/100, Loss: 1.3630315065383911\n","Epoch 69/100, Loss: 1.3630315065383911\n","Epoch 70/100, Loss: 1.3630315065383911\n","Epoch 71/100, Loss: 1.3630315065383911\n","Epoch 72/100, Loss: 1.3630315065383911\n","Epoch 73/100, Loss: 1.3630315065383911\n","Epoch 74/100, Loss: 1.3630315065383911\n","Epoch 75/100, Loss: 1.3630315065383911\n","Epoch 76/100, Loss: 1.3630315065383911\n","Epoch 77/100, Loss: 1.3630315065383911\n","Epoch 78/100, Loss: 1.3630315065383911\n","Epoch 79/100, Loss: 1.3630315065383911\n","Epoch 80/100, Loss: 1.3630315065383911\n","Epoch 81/100, Loss: 1.3630315065383911\n","Epoch 82/100, Loss: 1.3630315065383911\n","Epoch 83/100, Loss: 1.3630315065383911\n","Epoch 84/100, Loss: 1.3630315065383911\n","Epoch 85/100, Loss: 1.3630315065383911\n","Epoch 86/100, Loss: 1.3630315065383911\n","Epoch 87/100, Loss: 1.3630315065383911\n","Epoch 88/100, Loss: 1.3630315065383911\n","Epoch 89/100, Loss: 1.3630315065383911\n","Epoch 90/100, Loss: 1.3630315065383911\n","Epoch 91/100, Loss: 1.3630315065383911\n","Epoch 92/100, Loss: 1.3630315065383911\n","Epoch 93/100, Loss: 1.3630315065383911\n","Epoch 94/100, Loss: 1.3630315065383911\n","Epoch 95/100, Loss: 1.3630315065383911\n","Epoch 96/100, Loss: 1.3630315065383911\n","Epoch 97/100, Loss: 1.3630315065383911\n","Epoch 98/100, Loss: 1.3630315065383911\n","Epoch 99/100, Loss: 1.3630315065383911\n","Epoch 100/100, Loss: 1.3630315065383911\n","tensor(0.8833, device='cuda:0') tensor(0.1313, device='cuda:0')\n","Epoch 1/100, Loss: 1.2298460900783539\n","Epoch 2/100, Loss: 1.2234808057546616\n","Epoch 3/100, Loss: 1.2165504097938538\n","Epoch 4/100, Loss: 1.2187040597200394\n","Epoch 5/100, Loss: 1.2101774364709854\n","Epoch 6/100, Loss: 1.2103014290332794\n","Epoch 7/100, Loss: 1.2101839184761047\n","Epoch 8/100, Loss: 1.2093725204467773\n","Epoch 9/100, Loss: 1.2029758840799332\n","Epoch 10/100, Loss: 1.190080925822258\n","Epoch 11/100, Loss: 1.1953605562448502\n","Epoch 12/100, Loss: 1.1964907050132751\n","Epoch 13/100, Loss: 1.201972872018814\n","Epoch 14/100, Loss: 1.1959494352340698\n","Epoch 15/100, Loss: 1.1958853602409363\n","Epoch 16/100, Loss: 1.1946626752614975\n","Epoch 17/100, Loss: 1.193195402622223\n","Epoch 18/100, Loss: 1.1951009184122086\n","Epoch 19/100, Loss: 1.184190571308136\n","Epoch 20/100, Loss: 1.1792512387037277\n","Epoch 21/100, Loss: 1.17763289809227\n","Epoch 22/100, Loss: 1.188548982143402\n","Epoch 23/100, Loss: 1.1872469633817673\n","Epoch 24/100, Loss: 1.1832275390625\n","Epoch 25/100, Loss: 1.179056465625763\n","Epoch 26/100, Loss: 1.1735671013593674\n","Epoch 27/100, Loss: 1.176101416349411\n","Epoch 28/100, Loss: 1.1792421340942383\n","Epoch 29/100, Loss: 1.1683048605918884\n","Epoch 30/100, Loss: 1.1801786124706268\n","Epoch 31/100, Loss: 1.1691349148750305\n","Epoch 32/100, Loss: 1.1693058162927628\n","Epoch 33/100, Loss: 1.1730739623308182\n","Epoch 34/100, Loss: 1.1678386628627777\n","Epoch 35/100, Loss: 1.1759168058633804\n","Epoch 36/100, Loss: 1.1592666804790497\n","Epoch 37/100, Loss: 1.164625108242035\n","Epoch 38/100, Loss: 1.1585304588079453\n","Epoch 39/100, Loss: 1.1578654199838638\n","Epoch 40/100, Loss: 1.1615125089883804\n","Epoch 41/100, Loss: 1.1587454080581665\n","Epoch 42/100, Loss: 1.1491546034812927\n","Epoch 43/100, Loss: 1.15663842856884\n","Epoch 44/100, Loss: 1.1626258492469788\n","Epoch 45/100, Loss: 1.1674135476350784\n","Epoch 46/100, Loss: 1.1551639437675476\n","Epoch 47/100, Loss: 1.1570444405078888\n","Epoch 48/100, Loss: 1.1540303230285645\n","Epoch 49/100, Loss: 1.1446441113948822\n","Epoch 50/100, Loss: 1.1594365686178207\n","Epoch 51/100, Loss: 1.1499917656183243\n","Epoch 52/100, Loss: 1.1453422009944916\n","Epoch 53/100, Loss: 1.1542232781648636\n","Epoch 54/100, Loss: 1.1472369730472565\n","Epoch 55/100, Loss: 1.1473766714334488\n","Epoch 56/100, Loss: 1.1469474285840988\n","Epoch 57/100, Loss: 1.147245705127716\n","Epoch 58/100, Loss: 1.1393253356218338\n","Epoch 59/100, Loss: 1.1404618471860886\n","Epoch 60/100, Loss: 1.1470756232738495\n","Epoch 61/100, Loss: 1.1436848491430283\n","Epoch 62/100, Loss: 1.1476831883192062\n","Epoch 63/100, Loss: 1.1412989348173141\n","Epoch 64/100, Loss: 1.1378883868455887\n","Epoch 65/100, Loss: 1.1296287029981613\n","Epoch 66/100, Loss: 1.132280558347702\n","Epoch 67/100, Loss: 1.1339321434497833\n","Epoch 68/100, Loss: 1.1375159323215485\n","Epoch 69/100, Loss: 1.1273288428783417\n","Epoch 70/100, Loss: 1.124794602394104\n","Epoch 71/100, Loss: 1.1358190476894379\n","Epoch 72/100, Loss: 1.1393678337335587\n","Epoch 73/100, Loss: 1.1314300447702408\n","Epoch 74/100, Loss: 1.124000370502472\n","Epoch 75/100, Loss: 1.1273591816425323\n","Epoch 76/100, Loss: 1.1262405961751938\n","Epoch 77/100, Loss: 1.1239835023880005\n","Epoch 78/100, Loss: 1.1207693815231323\n","Epoch 79/100, Loss: 1.1170406341552734\n","Epoch 80/100, Loss: 1.119899109005928\n","Epoch 81/100, Loss: 1.1228583753108978\n","Epoch 82/100, Loss: 1.1241683810949326\n","Epoch 83/100, Loss: 1.1288149952888489\n","Epoch 84/100, Loss: 1.120080143213272\n","Epoch 85/100, Loss: 1.1139765828847885\n","Epoch 86/100, Loss: 1.1240530610084534\n","Epoch 87/100, Loss: 1.1182678192853928\n","Epoch 88/100, Loss: 1.119008645415306\n","Epoch 89/100, Loss: 1.1151943504810333\n","Epoch 90/100, Loss: 1.1101949512958527\n","Epoch 91/100, Loss: 1.115922436118126\n","Epoch 92/100, Loss: 1.113749921321869\n","Epoch 93/100, Loss: 1.1084666401147842\n","Epoch 94/100, Loss: 1.1166838556528091\n","Epoch 95/100, Loss: 1.10763218998909\n","Epoch 96/100, Loss: 1.105470448732376\n","Epoch 97/100, Loss: 1.109571561217308\n","Epoch 98/100, Loss: 1.1057717502117157\n","Epoch 99/100, Loss: 1.099591225385666\n","Epoch 100/100, Loss: 1.0988072901964188\n","Epoch 1/100, Loss: 1.3464080840349197\n","Epoch 2/100, Loss: 1.3631981313228607\n","Epoch 3/100, Loss: 1.3631981313228607\n","Epoch 4/100, Loss: 1.3631981313228607\n","Epoch 5/100, Loss: 1.3631981313228607\n","Epoch 6/100, Loss: 1.3631981313228607\n","Epoch 7/100, Loss: 1.3631981313228607\n","Epoch 8/100, Loss: 1.3631981313228607\n","Epoch 9/100, Loss: 1.3631981313228607\n","Epoch 10/100, Loss: 1.3631981313228607\n","Epoch 11/100, Loss: 1.3631981313228607\n","Epoch 12/100, Loss: 1.3631981313228607\n","Epoch 13/100, Loss: 1.3631981313228607\n","Epoch 14/100, Loss: 1.3631981313228607\n","Epoch 15/100, Loss: 1.3631981313228607\n","Epoch 16/100, Loss: 1.3631981313228607\n","Epoch 17/100, Loss: 1.3631981313228607\n","Epoch 18/100, Loss: 1.3631981313228607\n","Epoch 19/100, Loss: 1.3631981313228607\n","Epoch 20/100, Loss: 1.3631981313228607\n","Epoch 21/100, Loss: 1.3631981313228607\n","Epoch 22/100, Loss: 1.3631981313228607\n","Epoch 23/100, Loss: 1.3631981313228607\n","Epoch 24/100, Loss: 1.3631981313228607\n","Epoch 25/100, Loss: 1.3631981313228607\n","Epoch 26/100, Loss: 1.3631981313228607\n","Epoch 27/100, Loss: 1.3631981313228607\n","Epoch 28/100, Loss: 1.3631981313228607\n","Epoch 29/100, Loss: 1.3631981313228607\n","Epoch 30/100, Loss: 1.3631981313228607\n","Epoch 31/100, Loss: 1.3631981313228607\n","Epoch 32/100, Loss: 1.3631981313228607\n","Epoch 33/100, Loss: 1.3631981313228607\n","Epoch 34/100, Loss: 1.3631981313228607\n","Epoch 35/100, Loss: 1.3631981313228607\n","Epoch 36/100, Loss: 1.3631981313228607\n","Epoch 37/100, Loss: 1.3631981313228607\n","Epoch 38/100, Loss: 1.3631981313228607\n","Epoch 39/100, Loss: 1.3631981313228607\n","Epoch 40/100, Loss: 1.3631981313228607\n","Epoch 41/100, Loss: 1.3631981313228607\n","Epoch 42/100, Loss: 1.3631981313228607\n","Epoch 43/100, Loss: 1.3631981313228607\n","Epoch 44/100, Loss: 1.3631981313228607\n","Epoch 45/100, Loss: 1.3631981313228607\n","Epoch 46/100, Loss: 1.3631981313228607\n","Epoch 47/100, Loss: 1.3631981313228607\n","Epoch 48/100, Loss: 1.3631981313228607\n","Epoch 49/100, Loss: 1.3631981313228607\n","Epoch 50/100, Loss: 1.3631981313228607\n","Epoch 51/100, Loss: 1.3631981313228607\n","Epoch 52/100, Loss: 1.3631981313228607\n","Epoch 53/100, Loss: 1.3631981313228607\n","Epoch 54/100, Loss: 1.3631981313228607\n","Epoch 55/100, Loss: 1.3631981313228607\n","Epoch 56/100, Loss: 1.3631981313228607\n","Epoch 57/100, Loss: 1.3631981313228607\n","Epoch 58/100, Loss: 1.3631981313228607\n","Epoch 59/100, Loss: 1.3631981313228607\n","Epoch 60/100, Loss: 1.3631981313228607\n","Epoch 61/100, Loss: 1.3631981313228607\n","Epoch 62/100, Loss: 1.3631981313228607\n","Epoch 63/100, Loss: 1.3631981313228607\n","Epoch 64/100, Loss: 1.3631981313228607\n","Epoch 65/100, Loss: 1.3631981313228607\n","Epoch 66/100, Loss: 1.3631981313228607\n","Epoch 67/100, Loss: 1.3631981313228607\n","Epoch 68/100, Loss: 1.3631981313228607\n","Epoch 69/100, Loss: 1.3631981313228607\n","Epoch 70/100, Loss: 1.3631981313228607\n","Epoch 71/100, Loss: 1.3631981313228607\n","Epoch 72/100, Loss: 1.3631981313228607\n","Epoch 73/100, Loss: 1.3631981313228607\n","Epoch 74/100, Loss: 1.3631981313228607\n","Epoch 75/100, Loss: 1.3631981313228607\n","Epoch 76/100, Loss: 1.3631981313228607\n","Epoch 77/100, Loss: 1.3631981313228607\n","Epoch 78/100, Loss: 1.3631981313228607\n","Epoch 79/100, Loss: 1.3631981313228607\n","Epoch 80/100, Loss: 1.3631981313228607\n","Epoch 81/100, Loss: 1.3631981313228607\n","Epoch 82/100, Loss: 1.3631981313228607\n","Epoch 83/100, Loss: 1.3631981313228607\n","Epoch 84/100, Loss: 1.3631981313228607\n","Epoch 85/100, Loss: 1.3631981313228607\n","Epoch 86/100, Loss: 1.3631981313228607\n","Epoch 87/100, Loss: 1.3631981313228607\n","Epoch 88/100, Loss: 1.3631981313228607\n","Epoch 89/100, Loss: 1.3631981313228607\n","Epoch 90/100, Loss: 1.3631981313228607\n","Epoch 91/100, Loss: 1.3631981313228607\n","Epoch 92/100, Loss: 1.3631981313228607\n","Epoch 93/100, Loss: 1.3631981313228607\n","Epoch 94/100, Loss: 1.3631981313228607\n","Epoch 95/100, Loss: 1.3631981313228607\n","Epoch 96/100, Loss: 1.3631981313228607\n","Epoch 97/100, Loss: 1.3631981313228607\n","Epoch 98/100, Loss: 1.3631981313228607\n","Epoch 99/100, Loss: 1.3631981313228607\n","Epoch 100/100, Loss: 1.3631981313228607\n","tensor(0.8757, device='cuda:0') tensor(0.1313, device='cuda:0')\n","Epoch 1/100, Loss: 1.2337089478969574\n","Epoch 2/100, Loss: 1.2241948395967484\n","Epoch 3/100, Loss: 1.2194481790065765\n","Epoch 4/100, Loss: 1.221699982881546\n","Epoch 5/100, Loss: 1.2032135128974915\n","Epoch 6/100, Loss: 1.2063090950250626\n","Epoch 7/100, Loss: 1.2133093029260635\n","Epoch 8/100, Loss: 1.2012486457824707\n","Epoch 9/100, Loss: 1.1928151845932007\n","Epoch 10/100, Loss: 1.2028356939554214\n","Epoch 11/100, Loss: 1.1986647546291351\n","Epoch 12/100, Loss: 1.197278469800949\n","Epoch 13/100, Loss: 1.195875659584999\n","Epoch 14/100, Loss: 1.1900276243686676\n","Epoch 15/100, Loss: 1.1925281137228012\n","Epoch 16/100, Loss: 1.1916462033987045\n","Epoch 17/100, Loss: 1.1951267272233963\n","Epoch 18/100, Loss: 1.1868061274290085\n","Epoch 19/100, Loss: 1.1859510093927383\n","Epoch 20/100, Loss: 1.1794656217098236\n","Epoch 21/100, Loss: 1.185474917292595\n","Epoch 22/100, Loss: 1.1779135912656784\n","Epoch 23/100, Loss: 1.18386971950531\n","Epoch 24/100, Loss: 1.1816123723983765\n","Epoch 25/100, Loss: 1.176799088716507\n","Epoch 26/100, Loss: 1.1808469891548157\n","Epoch 27/100, Loss: 1.171756774187088\n","Epoch 28/100, Loss: 1.1768076866865158\n","Epoch 29/100, Loss: 1.1791051775217056\n","Epoch 30/100, Loss: 1.1674270033836365\n","Epoch 31/100, Loss: 1.168157011270523\n","Epoch 32/100, Loss: 1.1697142869234085\n","Epoch 33/100, Loss: 1.1713584810495377\n","Epoch 34/100, Loss: 1.169702634215355\n","Epoch 35/100, Loss: 1.1608628630638123\n","Epoch 36/100, Loss: 1.1657519340515137\n","Epoch 37/100, Loss: 1.1632389426231384\n","Epoch 38/100, Loss: 1.1599790155887604\n","Epoch 39/100, Loss: 1.165176972746849\n","Epoch 40/100, Loss: 1.1579322665929794\n","Epoch 41/100, Loss: 1.15338696539402\n","Epoch 42/100, Loss: 1.1536908596754074\n","Epoch 43/100, Loss: 1.157094568014145\n","Epoch 44/100, Loss: 1.1523608714342117\n","Epoch 45/100, Loss: 1.1566198766231537\n","Epoch 46/100, Loss: 1.1559276729822159\n","Epoch 47/100, Loss: 1.1589934974908829\n","Epoch 48/100, Loss: 1.1583758294582367\n","Epoch 49/100, Loss: 1.1498066931962967\n","Epoch 50/100, Loss: 1.1481648832559586\n","Epoch 51/100, Loss: 1.1478731632232666\n","Epoch 52/100, Loss: 1.1480841040611267\n","Epoch 53/100, Loss: 1.1434641927480698\n","Epoch 54/100, Loss: 1.1434328854084015\n","Epoch 55/100, Loss: 1.1373095363378525\n","Epoch 56/100, Loss: 1.1371898800134659\n","Epoch 57/100, Loss: 1.1460960358381271\n","Epoch 58/100, Loss: 1.137634739279747\n","Epoch 59/100, Loss: 1.1338039487600327\n","Epoch 60/100, Loss: 1.1357317566871643\n","Epoch 61/100, Loss: 1.1442609876394272\n","Epoch 62/100, Loss: 1.1359268426895142\n","Epoch 63/100, Loss: 1.1298222541809082\n","Epoch 64/100, Loss: 1.141266107559204\n","Epoch 65/100, Loss: 1.137148305773735\n","Epoch 66/100, Loss: 1.1328278630971909\n","Epoch 67/100, Loss: 1.131342500448227\n","Epoch 68/100, Loss: 1.1287112534046173\n","Epoch 69/100, Loss: 1.125928744673729\n","Epoch 70/100, Loss: 1.1253215074539185\n","Epoch 71/100, Loss: 1.1313188821077347\n","Epoch 72/100, Loss: 1.1249139606952667\n","Epoch 73/100, Loss: 1.1285983324050903\n","Epoch 74/100, Loss: 1.1221037209033966\n","Epoch 75/100, Loss: 1.1266572922468185\n","Epoch 76/100, Loss: 1.1220378428697586\n","Epoch 77/100, Loss: 1.1217754781246185\n","Epoch 78/100, Loss: 1.127012848854065\n","Epoch 79/100, Loss: 1.1209082007408142\n","Epoch 80/100, Loss: 1.1214896142482758\n","Epoch 81/100, Loss: 1.1252598315477371\n","Epoch 82/100, Loss: 1.1160927265882492\n","Epoch 83/100, Loss: 1.1228287667036057\n","Epoch 84/100, Loss: 1.120196208357811\n","Epoch 85/100, Loss: 1.1208423227071762\n","Epoch 86/100, Loss: 1.1204183399677277\n","Epoch 87/100, Loss: 1.1133504062891006\n","Epoch 88/100, Loss: 1.1155763268470764\n","Epoch 89/100, Loss: 1.119265228509903\n","Epoch 90/100, Loss: 1.1150340288877487\n","Epoch 91/100, Loss: 1.1084728091955185\n","Epoch 92/100, Loss: 1.1163426339626312\n","Epoch 93/100, Loss: 1.1093745529651642\n","Epoch 94/100, Loss: 1.109087496995926\n","Epoch 95/100, Loss: 1.1114096343517303\n","Epoch 96/100, Loss: 1.10885551571846\n","Epoch 97/100, Loss: 1.1128338277339935\n","Epoch 98/100, Loss: 1.1060613244771957\n","Epoch 99/100, Loss: 1.1037535220384598\n","Epoch 100/100, Loss: 1.1115695387125015\n","Epoch 1/100, Loss: 1.346370592713356\n","Epoch 2/100, Loss: 1.363364815711975\n","Epoch 3/100, Loss: 1.363364815711975\n","Epoch 4/100, Loss: 1.363364815711975\n","Epoch 5/100, Loss: 1.363364815711975\n","Epoch 6/100, Loss: 1.363364815711975\n","Epoch 7/100, Loss: 1.363364815711975\n","Epoch 8/100, Loss: 1.363364815711975\n","Epoch 9/100, Loss: 1.363364815711975\n","Epoch 10/100, Loss: 1.363364815711975\n","Epoch 11/100, Loss: 1.363364815711975\n","Epoch 12/100, Loss: 1.363364815711975\n","Epoch 13/100, Loss: 1.363364815711975\n","Epoch 14/100, Loss: 1.363364815711975\n","Epoch 15/100, Loss: 1.363364815711975\n","Epoch 16/100, Loss: 1.363364815711975\n","Epoch 17/100, Loss: 1.363364815711975\n","Epoch 18/100, Loss: 1.363364815711975\n","Epoch 19/100, Loss: 1.363364815711975\n","Epoch 20/100, Loss: 1.363364815711975\n","Epoch 21/100, Loss: 1.363364815711975\n","Epoch 22/100, Loss: 1.363364815711975\n","Epoch 23/100, Loss: 1.363364815711975\n","Epoch 24/100, Loss: 1.363364815711975\n","Epoch 25/100, Loss: 1.363364815711975\n","Epoch 26/100, Loss: 1.363364815711975\n","Epoch 27/100, Loss: 1.363364815711975\n","Epoch 28/100, Loss: 1.363364815711975\n","Epoch 29/100, Loss: 1.363364815711975\n","Epoch 30/100, Loss: 1.363364815711975\n","Epoch 31/100, Loss: 1.363364815711975\n","Epoch 32/100, Loss: 1.363364815711975\n","Epoch 33/100, Loss: 1.363364815711975\n","Epoch 34/100, Loss: 1.363364815711975\n","Epoch 35/100, Loss: 1.363364815711975\n","Epoch 36/100, Loss: 1.363364815711975\n","Epoch 37/100, Loss: 1.363364815711975\n","Epoch 38/100, Loss: 1.363364815711975\n","Epoch 39/100, Loss: 1.363364815711975\n","Epoch 40/100, Loss: 1.363364815711975\n","Epoch 41/100, Loss: 1.363364815711975\n","Epoch 42/100, Loss: 1.363364815711975\n","Epoch 43/100, Loss: 1.363364815711975\n","Epoch 44/100, Loss: 1.363364815711975\n","Epoch 45/100, Loss: 1.363364815711975\n","Epoch 46/100, Loss: 1.363364815711975\n","Epoch 47/100, Loss: 1.363364815711975\n","Epoch 48/100, Loss: 1.363364815711975\n","Epoch 49/100, Loss: 1.363364815711975\n","Epoch 50/100, Loss: 1.363364815711975\n","Epoch 51/100, Loss: 1.363364815711975\n","Epoch 52/100, Loss: 1.363364815711975\n","Epoch 53/100, Loss: 1.363364815711975\n","Epoch 54/100, Loss: 1.363364815711975\n","Epoch 55/100, Loss: 1.363364815711975\n","Epoch 56/100, Loss: 1.363364815711975\n","Epoch 57/100, Loss: 1.363364815711975\n","Epoch 58/100, Loss: 1.363364815711975\n","Epoch 59/100, Loss: 1.363364815711975\n","Epoch 60/100, Loss: 1.363364815711975\n","Epoch 61/100, Loss: 1.363364815711975\n","Epoch 62/100, Loss: 1.363364815711975\n","Epoch 63/100, Loss: 1.363364815711975\n","Epoch 64/100, Loss: 1.363364815711975\n","Epoch 65/100, Loss: 1.363364815711975\n","Epoch 66/100, Loss: 1.363364815711975\n","Epoch 67/100, Loss: 1.363364815711975\n","Epoch 68/100, Loss: 1.363364815711975\n","Epoch 69/100, Loss: 1.363364815711975\n","Epoch 70/100, Loss: 1.363364815711975\n","Epoch 71/100, Loss: 1.363364815711975\n","Epoch 72/100, Loss: 1.363364815711975\n","Epoch 73/100, Loss: 1.363364815711975\n","Epoch 74/100, Loss: 1.363364815711975\n","Epoch 75/100, Loss: 1.363364815711975\n","Epoch 76/100, Loss: 1.363364815711975\n","Epoch 77/100, Loss: 1.363364815711975\n","Epoch 78/100, Loss: 1.363364815711975\n","Epoch 79/100, Loss: 1.363364815711975\n","Epoch 80/100, Loss: 1.363364815711975\n","Epoch 81/100, Loss: 1.363364815711975\n","Epoch 82/100, Loss: 1.363364815711975\n","Epoch 83/100, Loss: 1.363364815711975\n","Epoch 84/100, Loss: 1.363364815711975\n","Epoch 85/100, Loss: 1.363364815711975\n","Epoch 86/100, Loss: 1.363364815711975\n","Epoch 87/100, Loss: 1.363364815711975\n","Epoch 88/100, Loss: 1.363364815711975\n","Epoch 89/100, Loss: 1.363364815711975\n","Epoch 90/100, Loss: 1.363364815711975\n","Epoch 91/100, Loss: 1.363364815711975\n","Epoch 92/100, Loss: 1.363364815711975\n","Epoch 93/100, Loss: 1.363364815711975\n","Epoch 94/100, Loss: 1.363364815711975\n","Epoch 95/100, Loss: 1.363364815711975\n","Epoch 96/100, Loss: 1.363364815711975\n","Epoch 97/100, Loss: 1.363364815711975\n","Epoch 98/100, Loss: 1.363364815711975\n","Epoch 99/100, Loss: 1.363364815711975\n","Epoch 100/100, Loss: 1.363364815711975\n","tensor(0.8650, device='cuda:0') tensor(0.1313, device='cuda:0')\n"]}]},{"cell_type":"code","source":["def extract_single(records, metric = \"recall\"):\n","  result = []\n","  for record in records:\n","      result.append(record[metric].cpu())\n","  return np.array(result)"],"metadata":{"id":"fzH4D_x-kqRy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","x = np.linspace(0,1,11)\n","fig, axs = plt.subplots(2,1, figsize=(5, 10))\n","\n","axs[0].set_xlabel(\"x = weight of teacher-student loss in total loss\")\n","axs[0].title.set_text('Student Model - CNN')\n","axs[1].set_xlabel(\"x = pruning rate\")\n","axs[1].title.set_text('Student Model - CNNLSTM')\n","lines_labels = [ax.get_legend_handles_labels() for ax in fig.axes]\n","lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n","fig.legend(lines[:1], labels[:1], loc='upper center', bbox_to_anchor=(0.5, 0.01), fancybox=True, shadow=False, ncol=4)\n","fig.tight_layout(pad=1.0)\n","plt.savefig(\"Pruning_Local_Global.png\", bbox_inches='tight')"],"metadata":{"id":"v073ywukU8ip","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1705999632834,"user_tz":0,"elapsed":1705,"user":{"displayName":"Xiaochen Zoey Tan","userId":"02061312605743465288"}},"outputId":"55ffb20c-2526-4a52-c406-b67140798446"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 500x1000 with 2 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAewAAAP2CAYAAAAl+K60AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABvcUlEQVR4nO3deVhV5f7//9cGZRAFMRXQEHLAWSkHQjMbME55KC3LKafSBs1SPg2aJs56GjyaaR5t0NPJtFKrX45JUpmWpuLRo+I8VIJDCYoFCvfvD7/u3ALKRlBvfT6ua19X+973vdZ73ex87bX2Wns5jDFGAADgquZxpQsAAAAXR2ADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgA25yOBwaPnz4lS7jqjB8+HA5HI4ije3Zs6fCw8OLtyDgGkZgwyqbNm1Shw4dFBYWJh8fH1WtWlVt2rTR5MmTXfqNHTtWn3322ZUpshjMnj1bEydOLHT/8PBwORwOxcTE5Pv6jBkz5HA45HA49NNPPxVTlVevpKQkPfjggwoODpaXl5cqV66suLg4zZ8/39ln7969zjmZN29enmWc/TBy5MgRZ1vPnj3lcDjUqFEj5ferzg6HQ88880zJbBSuewQ2rLFq1So1bdpUGzduVJ8+ffTWW2+pd+/e8vDw0KRJk1z6Xm+BLUk+Pj5asWKFUlNT87z24YcfysfHp5iqu7olJCTozjvv1ObNm/Xkk09q2rRpeuGFF3TixAk99NBDmj17dp4xI0eOzDeAC7Jp0yaX8Acuh1JXugCgsMaMGaOAgACtXbtW5cuXd3nt0KFDV6aoq0jLli21du1azZ07V88995yz/eeff9Z3332n9u3b57sneS359NNPNXLkSHXo0EGzZ89W6dKlna+98MILWrp0qU6dOuUyJjIyUsnJyVqwYIEefPDBi67D19dXoaGhGjlypB588MEifyUAuIs9bFhj165dql+/fp6wlqTKlSs7/9vhcCgzM1OzZs1yHvLs2bOnpIK/N83vu9isrCwNHDhQlSpVUrly5XT//ffr559/zre2X375RY899piCgoLk7e2t+vXr67333nPpk5SUJIfDoY8//lhjxozRjTfeKB8fH919993auXOns98dd9yhhQsXat++fc76C/Ndr4+Pjx588ME8e5AfffSRAgMDFRsbm++4r7/+Wq1atZKfn5/Kly+vBx54QFu3bs3Tb+XKlWrWrJl8fHxUo0YN/etf/yqwlv/85z9q0qSJfH19VaFCBXXq1EkHDhy46DZcqldeeUUVKlTQe++95xLWZ8XGxurvf/+7S1unTp0UERFR6L1sDw8PDR06VP/973+1YMGCYqsduBj2sGGNsLAwrV69Wps3b1aDBg0K7PfBBx+od+/eat68uZ544glJUo0aNdxeX+/evfWf//xHXbp0UYsWLfT111+rbdu2efqlpaXp1ltvdX5/WalSJS1evFiPP/64MjIyNGDAAJf+48ePl4eHh55//nmlp6fr1VdfVdeuXfXjjz9KkoYMGaL09HT9/PPP+uc//ylJKlu2bKFq7tKli+655x7t2rXLuc2zZ89Whw4d8g2w5cuX695771X16tU1fPhw/fHHH5o8ebJatmyp9evXOz8obNq0Sffcc48qVaqk4cOH6/Tp00pISFBQUFCeZY4ZM0avvPKKHnnkEfXu3VuHDx/W5MmTdfvtt2vDhg35fuAqDjt27NC2bdv02GOPqVy5coUe5+npqaFDh6p79+6F3svu0qWLRo0apZEjR6p9+/bsZePyMIAlli1bZjw9PY2np6eJjo42L774olm6dKnJzs7O09fPz8/06NEjT3uPHj1MWFhYnvaEhARz7v8OycnJRpLp27evS78uXboYSSYhIcHZ9vjjj5uQkBBz5MgRl76dOnUyAQEB5uTJk8YYY1asWGEkmbp165qsrCxnv0mTJhlJZtOmTc62tm3b5ltnQcLCwkzbtm3N6dOnTXBwsBk1apQxxpgtW7YYSeabb74x77//vpFk1q5d6xwXGRlpKleubI4ePeps27hxo/Hw8DDdu3d3trVr1874+PiYffv2Odu2bNliPD09XeZt7969xtPT04wZM8alvk2bNplSpUq5tBf0tyiqzz//3Egy//znPwvVf8+ePUaSee2118zp06dNrVq1TOPGjU1ubq4x5q/3xOHDh11q9vPzM8YYM2vWLCPJzJ8/3/m6JNOvX79i2ybgXBwShzXatGmj1atX6/7779fGjRv16quvKjY2VlWrVtUXX3xRrOtatGiRJOnZZ591aT9/b9kYo3nz5ikuLk7GGB05csT5iI2NVXp6utavX+8yplevXvLy8nI+b9WqlSRp9+7dl1y3p6enHnnkEX300UeSzpxsFhoa6lzHuQ4ePKjk5GT17NlTFSpUcLY3atRIbdq0cc5BTk6Oli5dqnbt2qlatWrOfnXr1s1zmH3+/PnKzc3VI4884jIXwcHBqlWrllasWHHJ21iQjIwMSXJr7/qss3vZGzduLPTJil27dlWtWrXcPmENKCoCG1Zp1qyZ5s+fr99//11r1qzR4MGDdfz4cXXo0EFbtmwptvXs27dPHh4eeQ6l165d2+X54cOHdezYMU2fPl2VKlVyefTq1UtS3hPizg09SQoMDJQk/f7778VSe5cuXbRlyxZt3LhRs2fPVqdOnfI9ZLtv3758t0k6E8ZHjhxRZmamDh8+rD/++EO1atXK0+/8sTt27JAxRrVq1cozH1u3bnX75MD09HSlpqY6H7/99luBff39/SVJx48fd2sdZ3Xt2lU1a9YsdACfDfnk5GSrr0iAPfgOG1by8vJSs2bN1KxZM0VERKhXr1765JNPlJCQcMFxBX3XmJOTU6Q6cnNzJUmPPvqoevTokW+fRo0auTz39PTMt19x7aVFRUWpRo0aGjBggPbs2aMuXboUy3ILIzc3Vw6HQ4sXL853Owv7XfxZzz33nGbNmuV83rp1ayUlJeXbt06dOpLOfN9eFGcDuGfPnvr8888LNaZr167O77LbtWtXpPUChUVgw3pNmzaVdOYQ71kFBXNgYKCOHTuWp/3s3uZZYWFhys3N1a5du1z2IlNSUlz6nT2DPCcnp8AfLSmKSz2JqXPnzho9erTq1q2ryMjIfPuEhYVJyrtNkrRt2zZVrFhRfn5+8vHxka+vr3bs2JGn3/lja9SoIWOMbrrpJkVERFzSNkjSiy++qEcffdT5/OzRiPxERESodu3a+vzzzzVp0iS3PxxIZz54jR49WiNGjND9999/0f5FCXmgqDgkDmusWLEi373Qs9+1nhusfn5++QZzjRo1lJ6erv/+97/OtoMHD+a5POfee++VJL355psu7ef/mImnp6ceeughzZs3T5s3b86zvsOHD194owrg5+en9PT0Io2VzpzhnpCQoDfeeKPAPiEhIYqMjNSsWbNc5mrz5s1atmyZ7rvvPklntjE2NlafffaZ9u/f7+y3detWLV261GWZDz74oDw9PTVixIg8fytjjI4ePerWdtSrV08xMTHOR5MmTS7Yf8SIETp69Kh69+6t06dP53l92bJl+vLLLwscf+5h7sKeF/Hoo4+qZs2aGjFiRKH6A0XFHjas0b9/f508eVLt27dXnTp1lJ2drVWrVmnu3LkKDw93fmcsSU2aNNHy5cs1YcIEValSRTfddJOioqLUqVMnvfTSS2rfvr2effZZnTx5Um+//bYiIiJcTg6LjIxU586dNXXqVKWnp6tFixZKTEx0uV76rPHjx2vFihWKiopSnz59VK9ePf32229av369li9ffsHvXQvSpEkTzZ07V/Hx8WrWrJnKli2ruLi4Qo8PCwsr1O+dv/baa7r33nsVHR2txx9/3HlZV0BAgMv4ESNGaMmSJWrVqpX69u2r06dPa/Lkyapfv77Lh58aNWpo9OjRGjx4sPbu3at27dqpXLly2rNnjxYsWKAnnnhCzz//vDtT4ZaOHTtq06ZNGjNmjDZs2KDOnTsrLCxMR48e1ZIlS5SYmJjvL52d6+xh7uTk5EKt09PTU0OGDHF5/wEl4kqdng64a/Hixeaxxx4zderUMWXLljVeXl6mZs2apn///iYtLc2l77Zt28ztt99ufH19jSSXS7yWLVtmGjRoYLy8vEzt2rXNf/7znzyXdRljzB9//GGeffZZc8MNNxg/Pz8TFxdnDhw4kOeyLmOMSUtLM/369TOhoaGmdOnSJjg42Nx9991m+vTpzj5nL+v65JNPXMaevbzo/fffd7adOHHCdOnSxZQvX95IuujlT2cv67qQ/C7rMsaY5cuXm5YtWxpfX1/j7+9v4uLizJYtW/KM/+abb0yTJk2Ml5eXqV69upk2bVq+82aMMfPmzTO33Xab8fPzM35+fqZOnTqmX79+JiUlxdmnuC/rOldiYqJ54IEHTOXKlU2pUqVMpUqVTFxcnPn888+dfc69rOt8Z+dKF7is61ynTp0yNWrU4LIulCiHMVyPAADA1Y7vsAEAsACBDQCABQhsAAAs4HZgf/vtt4qLi1OVKlXkcDgK9Qs/SUlJuuWWW+Tt7a2aNWtq5syZRSgVAIDrl9uBnZmZqcaNG2vKlCmF6r9nzx61bdtWd955p5KTkzVgwAD17t07z/WbAACgYJd0lrjD4dCCBQsu+JN8L730khYuXOjyoxKdOnXSsWPHtGTJkqKuGgCA60qJ/3DK6tWr8/xkY2xsbJ67Hp0rKytLWVlZzue5ubn67bffdMMNN3DfWQDAVc0Yo+PHj6tKlSry8Ci+U8VKPLBTU1Pz3OQ+KChIGRkZ+uOPP+Tr65tnzLhx4/iZPwCA1Q4cOKAbb7yx2JZ3Vf406eDBgxUfH+98np6ermrVqunAgQPOW+gBAHA1ysjIUGhoaJHuzX4hJR7YwcHBSktLc2lLS0uTv79/vnvXkuTt7S1vb+887f7+/gQ2AMAKxf0Vbolfhx0dHa3ExESXtq+++krR0dElvWoAAK4Zbgf2iRMnlJyc7LyTzZ49e5ScnOy87d7gwYPVvXt3Z/+nnnpKu3fv1osvvqht27Zp6tSp+vjjjzVw4MDi2QIAAK4Dbgf2Tz/9pJtvvlk333yzJCk+Pl4333yzhg0bJunMvYXPvWfuTTfdpIULF+qrr75S48aN9cYbb+idd95RbGxsMW0CAADXPivu1pWRkaGAgAClp6fzHTYA4KpWUpnFb4kDAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsUKTAnjJlisLDw+Xj46OoqCitWbPmgv0nTpyo2rVry9fXV6GhoRo4cKD+/PPPIhUMAMD1yO3Anjt3ruLj45WQkKD169ercePGio2N1aFDh/LtP3v2bA0aNEgJCQnaunWr3n33Xc2dO1cvv/zyJRcPAMD1wu3AnjBhgvr06aNevXqpXr16mjZtmsqUKaP33nsv3/6rVq1Sy5Yt1aVLF4WHh+uee+5R586dL7pXDgAA/uJWYGdnZ2vdunWKiYn5awEeHoqJidHq1avzHdOiRQutW7fOGdC7d+/WokWLdN999xW4nqysLGVkZLg8AAC4npVyp/ORI0eUk5OjoKAgl/agoCBt27Yt3zFdunTRkSNHdNttt8kYo9OnT+upp5664CHxcePGacSIEe6UBgDANa3EzxJPSkrS2LFjNXXqVK1fv17z58/XwoULNWrUqALHDB48WOnp6c7HgQMHSrpMAACuam7tYVesWFGenp5KS0tzaU9LS1NwcHC+Y1555RV169ZNvXv3liQ1bNhQmZmZeuKJJzRkyBB5eOT9zODt7S1vb293SgMA4Jrm1h62l5eXmjRposTERGdbbm6uEhMTFR0dne+YkydP5gllT09PSZIxxt16AQC4Lrm1hy1J8fHx6tGjh5o2barmzZtr4sSJyszMVK9evSRJ3bt3V9WqVTVu3DhJUlxcnCZMmKCbb75ZUVFR2rlzp1555RXFxcU5gxsAAFyY24HdsWNHHT58WMOGDVNqaqoiIyO1ZMkS54lo+/fvd9mjHjp0qBwOh4YOHapffvlFlSpVUlxcnMaMGVN8WwEAwDXOYSw4Lp2RkaGAgAClp6fL39//SpcDAECBSiqz+C1xAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsECRAnvKlCkKDw+Xj4+PoqKitGbNmgv2P3bsmPr166eQkBB5e3srIiJCixYtKlLBAABcj0q5O2Du3LmKj4/XtGnTFBUVpYkTJyo2NlYpKSmqXLlynv7Z2dlq06aNKleurE8//VRVq1bVvn37VL58+eKoHwCA64LDGGPcGRAVFaVmzZrprbfekiTl5uYqNDRU/fv316BBg/L0nzZtml577TVt27ZNpUuXLlKRGRkZCggIUHp6uvz9/Yu0DAAALoeSyiy3DolnZ2dr3bp1iomJ+WsBHh6KiYnR6tWr8x3zxRdfKDo6Wv369VNQUJAaNGigsWPHKicn59IqBwDgOuLWIfEjR44oJydHQUFBLu1BQUHatm1bvmN2796tr7/+Wl27dtWiRYu0c+dO9e3bV6dOnVJCQkK+Y7KyspSVleV8npGR4U6ZAABcc0r8LPHc3FxVrlxZ06dPV5MmTdSxY0cNGTJE06ZNK3DMuHHjFBAQ4HyEhoaWdJkAAFzV3ArsihUrytPTU2lpaS7taWlpCg4OzndMSEiIIiIi5Onp6WyrW7euUlNTlZ2dne+YwYMHKz093fk4cOCAO2UCAHDNcSuwvby81KRJEyUmJjrbcnNzlZiYqOjo6HzHtGzZUjt37lRubq6zbfv27QoJCZGXl1e+Y7y9veXv7+/yAADgeub2IfH4+HjNmDFDs2bN0tatW/X0008rMzNTvXr1kiR1795dgwcPdvZ/+umn9dtvv+m5557T9u3btXDhQo0dO1b9+vUrvq0AAOAa5/Z12B07dtThw4c1bNgwpaamKjIyUkuWLHGeiLZ//355ePz1OSA0NFRLly7VwIED1ahRI1WtWlXPPfecXnrppeLbCgAArnFuX4d9JXAdNgDAFlfFddgAAODKILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAkUK7ClTpig8PFw+Pj6KiorSmjVrCjVuzpw5cjgcateuXVFWCwDAdcvtwJ47d67i4+OVkJCg9evXq3HjxoqNjdWhQ4cuOG7v3r16/vnn1apVqyIXCwDA9crtwJ4wYYL69OmjXr16qV69epo2bZrKlCmj9957r8AxOTk56tq1q0aMGKHq1atfUsEAAFyP3Ars7OxsrVu3TjExMX8twMNDMTExWr16dYHjRo4cqcqVK+vxxx8v1HqysrKUkZHh8gAA4HrmVmAfOXJEOTk5CgoKcmkPCgpSampqvmNWrlypd999VzNmzCj0esaNG6eAgADnIzQ01J0yAQC45pToWeLHjx9Xt27dNGPGDFWsWLHQ4wYPHqz09HTn48CBAyVYJQAAV79S7nSuWLGiPD09lZaW5tKelpam4ODgPP137dqlvXv3Ki4uztmWm5t7ZsWlSiklJUU1atTIM87b21ve3t7ulAYAwDXNrT1sLy8vNWnSRImJic623NxcJSYmKjo6Ok//OnXqaNOmTUpOTnY+7r//ft15551KTk7mUDcAAIXk1h62JMXHx6tHjx5q2rSpmjdvrokTJyozM1O9evWSJHXv3l1Vq1bVuHHj5OPjowYNGriML1++vCTlaQcAAAVzO7A7duyow4cPa9iwYUpNTVVkZKSWLFniPBFt//798vDgB9QAAChODmOMudJFXExGRoYCAgKUnp4uf3//K10OAAAFKqnMYlcYAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBggSIF9pQpUxQeHi4fHx9FRUVpzZo1BfadMWOGWrVqpcDAQAUGBiomJuaC/QEAQF5uB/bcuXMVHx+vhIQErV+/Xo0bN1ZsbKwOHTqUb/+kpCR17txZK1as0OrVqxUaGqp77rlHv/zyyyUXDwDA9cJhjDHuDIiKilKzZs301ltvSZJyc3MVGhqq/v37a9CgQRcdn5OTo8DAQL311lvq3r17odaZkZGhgIAApaeny9/f351yAQC4rEoqs9zaw87Ozta6desUExPz1wI8PBQTE6PVq1cXahknT57UqVOnVKFChQL7ZGVlKSMjw+UBAMD1zK3APnLkiHJychQUFOTSHhQUpNTU1EIt46WXXlKVKlVcQv9848aNU0BAgPMRGhrqTpkAAFxzLutZ4uPHj9ecOXO0YMEC+fj4FNhv8ODBSk9Pdz4OHDhwGasEAODqU8qdzhUrVpSnp6fS0tJc2tPS0hQcHHzBsa+//rrGjx+v5cuXq1GjRhfs6+3tLW9vb3dKAwDgmubWHraXl5eaNGmixMREZ1tubq4SExMVHR1d4LhXX31Vo0aN0pIlS9S0adOiVwsAwHXKrT1sSYqPj1ePHj3UtGlTNW/eXBMnTlRmZqZ69eolSerevbuqVq2qcePGSZL+8Y9/aNiwYZo9e7bCw8Od33WXLVtWZcuWLcZNAQDg2uV2YHfs2FGHDx/WsGHDlJqaqsjISC1ZssR5Itr+/fvl4fHXjvvbb7+t7OxsdejQwWU5CQkJGj58+KVVDwDAdcLt67CvBK7DBgDY4qq4DhsAAFwZBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxQpsKdMmaLw8HD5+PgoKipKa9asuWD/Tz75RHXq1JGPj48aNmyoRYsWFalYAACuV24H9ty5cxUfH6+EhAStX79ejRs3VmxsrA4dOpRv/1WrVqlz5856/PHHtWHDBrVr107t2rXT5s2bL7l4AACuFw5jjHFnQFRUlJo1a6a33npLkpSbm6vQ0FD1799fgwYNytO/Y8eOyszM1Jdffulsu/XWWxUZGalp06YVap0ZGRkKCAhQenq6/P393SkXAIDLqqQyy6097OzsbK1bt04xMTF/LcDDQzExMVq9enW+Y1avXu3SX5JiY2ML7A8AAPIq5U7nI0eOKCcnR0FBQS7tQUFB2rZtW75jUlNT8+2fmppa4HqysrKUlZXlfJ6eni7pzKcWAACuZmezys0D2BflVmBfLuPGjdOIESPytIeGhl6BagAAcN/Ro0cVEBBQbMtzK7ArVqwoT09PpaWlubSnpaUpODg43zHBwcFu9ZekwYMHKz4+3vn82LFjCgsL0/79+4t14693GRkZCg0N1YEDBzg3oBgxr8WPOS0ZzGvJSE9PV7Vq1VShQoViXa5bge3l5aUmTZooMTFR7dq1k3TmpLPExEQ988wz+Y6Jjo5WYmKiBgwY4Gz76quvFB0dXeB6vL295e3tnac9ICCAN1UJ8Pf3Z15LAPNa/JjTksG8lgwPj+L9qRO3D4nHx8erR48eatq0qZo3b66JEycqMzNTvXr1kiR1795dVatW1bhx4yRJzz33nFq3bq033nhDbdu21Zw5c/TTTz9p+vTpxbohAABcy9wO7I4dO+rw4cMaNmyYUlNTFRkZqSVLljhPLNu/f7/Lp4oWLVpo9uzZGjp0qF5++WXVqlVLn332mRo0aFB8WwEAwDWuSCedPfPMMwUeAk9KSsrT9vDDD+vhhx8uyqoknTlEnpCQkO9hchQd81oymNfix5yWDOa1ZJTUvLr9wykAAODy4+YfAABYgMAGAMACBDYAABYgsAEAsMBVE9jcY7tkuDOvM2bMUKtWrRQYGKjAwEDFxMRc9O9wPXL3vXrWnDlz5HA4nD86BFfuzuuxY8fUr18/hYSEyNvbWxEREfw7kA9353XixImqXbu2fH19FRoaqoEDB+rPP/+8TNXa4dtvv1VcXJyqVKkih8Ohzz777KJjkpKSdMstt8jb21s1a9bUzJkz3V+xuQrMmTPHeHl5mffee8/873//M3369DHly5c3aWlp+fb//vvvjaenp3n11VfNli1bzNChQ03p0qXNpk2bLnPlVzd357VLly5mypQpZsOGDWbr1q2mZ8+eJiAgwPz888+XufKrl7tzetaePXtM1apVTatWrcwDDzxweYq1iLvzmpWVZZo2bWruu+8+s3LlSrNnzx6TlJRkkpOTL3PlVzd35/XDDz803t7e5sMPPzR79uwxS5cuNSEhIWbgwIGXufKr26JFi8yQIUPM/PnzjSSzYMGCC/bfvXu3KVOmjImPjzdbtmwxkydPNp6enmbJkiVurfeqCOzmzZubfv36OZ/n5OSYKlWqmHHjxuXb/5FHHjFt27Z1aYuKijJPPvlkidZpG3fn9XynT5825cqVM7NmzSqpEq1TlDk9ffq0adGihXnnnXdMjx49COx8uDuvb7/9tqlevbrJzs6+XCVayd157devn7nrrrtc2uLj403Lli1LtE6bFSawX3zxRVO/fn2Xto4dO5rY2Fi31nXFD4lzj+2SUZR5Pd/Jkyd16tSpYv8Be1sVdU5HjhypypUr6/HHH78cZVqnKPP6xRdfKDo6Wv369VNQUJAaNGigsWPHKicn53KVfdUryry2aNFC69atcx423717txYtWqT77rvvstR8rSquzLrit9e8XPfYvt4UZV7P99JLL6lKlSp53mjXq6LM6cqVK/Xuu+8qOTn5MlRop6LM6+7du/X111+ra9euWrRokXbu3Km+ffvq1KlTSkhIuBxlX/WKMq9dunTRkSNHdNttt8kYo9OnT+upp57Syy+/fDlKvmYVlFkZGRn6448/5OvrW6jlXPE9bFydxo8frzlz5mjBggXy8fG50uVY6fjx4+rWrZtmzJihihUrXulyrim5ubmqXLmypk+friZNmqhjx44aMmSIpk2bdqVLs1pSUpLGjh2rqVOnav369Zo/f74WLlyoUaNGXenSoKtgD/ty3WP7elOUeT3r9ddf1/jx47V8+XI1atSoJMu0irtzumvXLu3du1dxcXHOttzcXElSqVKllJKSoho1apRs0RYoyns1JCREpUuXlqenp7Otbt26Sk1NVXZ2try8vEq0ZhsUZV5feeUVdevWTb1795YkNWzYUJmZmXriiSc0ZMiQYr9d5PWioMzy9/cv9N61dBXsYZ97j+2zzt5ju6B7Zp+9x/a5LnaP7etNUeZVkl599VWNGjVKS5YsUdOmTS9HqdZwd07r1KmjTZs2KTk52fm4//77deeddyo5OVmhoaGXs/yrVlHeqy1bttTOnTudH4Akafv27QoJCSGs/5+izOvJkyfzhPLZD0WG204UWbFllnvnw5WMOXPmGG9vbzNz5kyzZcsW88QTT5jy5cub1NRUY4wx3bp1M4MGDXL2//77702pUqXM66+/brZu3WoSEhK4rCsf7s7r+PHjjZeXl/n000/NwYMHnY/jx49fqU246rg7p+fjLPH8uTuv+/fvN+XKlTPPPPOMSUlJMV9++aWpXLmyGT169JXahKuSu/OakJBgypUrZz766COze/dus2zZMlOjRg3zyCOPXKlNuCodP37cbNiwwWzYsMFIMhMmTDAbNmww+/btM8YYM2jQINOtWzdn/7OXdb3wwgtm69atZsqUKfZe1mWMMZMnTzbVqlUzXl5epnnz5uaHH35wvta6dWvTo0cPl/4ff/yxiYiIMF5eXqZ+/fpm4cKFl7liO7gzr2FhYUZSnkdCQsLlL/wq5u579VwEdsHcnddVq1aZqKgo4+3tbapXr27GjBljTp8+fZmrvvq5M6+nTp0yw4cPNzVq1DA+Pj4mNDTU9O3b1/z++++Xv/Cr2IoVK/L9t/LsXPbo0cO0bt06z5jIyEjj5eVlqlevbt5//32318vtNQEAsMAV/w4bAABcHIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYuSVJSkhwOh44dO1boMcOHD1dkZGSJ1XS+zz77TDVr1pSnp6cGDBhw2dZbGHfcccdVV9OlmjlzpsqXL8+6LVCY/38dDoc+++yzy1YTCkZg45K0aNFCBw8eVEBAQLEutziD7Mknn1SHDh104MCBAu86dL3/o9SzZ0+1a9fuSpdRoKJ8MLzcOnbsqO3bt1/SMooa+jZ+WID7rvjdumA3Ly+vq/ouaSdOnNChQ4cUGxurKlWqXOlyLgvuVnVl+Pr6unXnJcBd7GFfQYcPH1ZwcLDGjh3rbFu1apW8vLzy3NmluHTo0EHPPPOM8/mAAQPkcDicN7TPzs6Wn5+fli9fLunM3X3GjRunm266Sb6+vmrcuLE+/fRT5/j89nxmzJih0NBQlSlTRu3bt9eECRPy/fT/wQcfKDw8XAEBAerUqZOOHz8u6cze3jfffKNJkybJ4XDI4XBo7969+W7P77//ru7duyswMFBlypTRvffeqx07djhrK1eunCTprrvuksPhUFJSUp5lhIeHS5Lat28vh8PhfC5Jn3/+uW655Rb5+PioevXqGjFihE6fPu18fcKECWrYsKH8/PwUGhqqvn376sSJEy7L//7773XHHXeoTJkyCgwMVGxsrH7//Xfn67m5uXrxxRdVoUIFBQcHa/jw4S7jjx07pt69e6tSpUry9/fXXXfdpY0bNzpfP/sVwzvvvKObbrqpwPuXf/rpp2rYsKF8fX11ww03KCYmRpmZmRo+fLhmzZqlzz//3DnfSUlJ+f5tk5OT8/w9Zs6cqWrVqjn/3kePHs2z7ovNo8Ph0DvvvKP27durTJkyqlWrlr744gtJ0t69e3XnnXdKkgIDA+VwONSzZ898tzE/b7/9tmrUqCEvLy/Vrl1bH3zwgfM1Y4yGDx+uatWqydvbW1WqVNGzzz7rfH3q1KmqVauWfHx8FBQUpA4dOhS4nvP3cs/+XQp6n58vKSlJvXr1Unp6uvPvcPa9cLH3eUHjPvjgAzVt2lTlypVTcHCwunTpokOHDhV67vKzadMm3XXXXc730RNPPOHynk9KSlLz5s3l5+en8uXLq2XLltq3b58kaePGjbrzzjtVrlw5+fv7q0mTJvrpp58uqZ7ryiX+Bjou0cKFC03p0qXN2rVrTUZGhqlevboZOHDgBcfUq1fP+Pn5Ffj429/+VuDYN99809SvX9/5PDIy0lSsWNG8/fbbxhhjVq5caUqXLm0yMzONMcaMHj3a1KlTxyxZssTs2rXLvP/++8bb29skJSUZY/76EfyzNwdYuXKl8fDwMK+99ppJSUkxU6ZMMRUqVDABAQHOdSYkJJiyZcuaBx980GzatMl8++23Jjg42Lz88svGGGOOHTtmoqOjTZ8+fZx3DCvopg7333+/qVu3rvn2229NcnKyiY2NNTVr1jTZ2dkmKyvLpKSkGElm3rx55uDBgyYrKyvPMg4dOmQkmffff98cPHjQHDp0yBhjzLfffmv8/f3NzJkzza5du8yyZctMeHi4GT58uHPsP//5T/P111+bPXv2mMTERFO7dm3z9NNPO1/fsGGD8fb2Nk8//bRJTk42mzdvNpMnTzaHDx82xpy5+YK/v78ZPny42b59u5k1a5ZxOBxm2bJlzmXExMSYuLg4s3btWrN9+3bzf//3f+aGG24wR48edc7n2b/7+vXrzcaNG/Ns46+//mpKlSplJkyYYPbs2WP++9//milTppjjx4+b48ePm0ceecT87W9/c853VlZWnr/t2e2RZPbs2WOMMeaHH34wHh4e5h//+IdJSUkxkyZNMuXLl3f5exdmHiWZG2+80cyePdvs2LHDPPvss6Zs2bLm6NGj5vTp02bevHlGkklJSTEHDx40x44dy/f98P7777use/78+aZ06dJmypQpJiUlxbzxxhvG09PTfP3118YYYz755BPj7+9vFi1aZPbt22d+/PFHM336dGOMMWvXrjWenp5m9uzZZu/evWb9+vVm0qRJ+a43v3Vf7H1+vqysLDNx4kTj7++f5055F3ufFzTu3XffNYsWLTK7du0yq1evNtHR0ebee+91rjO/v/H5JJkFCxYYY4w5ceKECQkJcW5TYmKiuemmm5w3vTh16pQJCAgwzz//vNm5c6fZsmWLmTlzpvMuVvXr1zePPvqo2bp1q9m+fbv5+OOPTXJycoHrhisC+yrQt29fExERYbp06WIaNmxo/vzzzwv237t3r9mxY0eBj59//rnAsf/973+Nw+Ewhw4dMr/99pvx8vIyo0aNMh07djTGnAnoFi1aGGOM+fPPP02ZMmXMqlWrXJbx+OOPm86dOxtj8v4P37FjR9O2bVuX/l27ds3zD1mZMmVMRkaGs+2FF14wUVFRzuetW7c2zz333AXnYfv27UaS+f77751tR44cMb6+vubjjz82xhjz+++/G0lmxYoVF1zWuf8onXX33XebsWPHurR98MEHJiQkpMDlfPLJJ+aGG25wPu/cubNp2bJlgf1bt25tbrvtNpe2Zs2amZdeeskYY8x3331n/P3987wnatSoYf71r38ZY4zz9rJnP2jkZ926dUaS2bt3b76v53cXscIEdufOnc19993nMq5jx44uf+/CzKMkM3ToUOfzEydOGElm8eLFBdaSn/NDs0WLFqZPnz4ufR5++GFnzW+88YaJiIgw2dnZeZY1b9484+/v7/I+dWfdhXmfX2wZxhTufZ7fuPysXbvWSHIGuruBPX36dBMYGGhOnDjhfH3hwoXGw8PDpKammqNHjxpJzg/05ytXrpyZOXPmRetE/jgkfhV4/fXXdfr0aX3yySf68MMP5e3tfcH+YWFhqlmzZoGPqlWrFji2QYMGqlChgr755ht99913uvnmm/X3v/9d33zzjSTpm2++0R133CFJ2rlzp06ePKk2bdqobNmyzse///1v7dq1K9/lp6SkqHnz5i5t5z+XzhyGPnu4WpJCQkLcPlS3detWlSpVSlFRUc62G264QbVr19bWrVvdWlZ+Nm7cqJEjR7pse58+fXTw4EGdPHlSkrR8+XLdfffdqlq1qsqVK6du3brp6NGjzteTk5N19913X3A9jRo1cnl+7lxs3LhRJ06c0A033OBSx549e1z+BmFhYapUqZIk6bvvvnPp++GHH6px48a6++671bBhQz388MOaMWOGy2H5otq6davL/EtSdHS0y/PCzOP58+Dn5yd/f/9LPny7detWtWzZ0qWtZcuWzvfHww8/rD/++EPVq1dXnz59tGDBAueh+jZt2igsLEzVq1dXt27d9OGHH7rUWxhX+n2+bt06xcXFqVq1aipXrpxat24tSdq/f79bNZxbS+PGjeXn5+dsa9mypXJzc5WSkqIKFSqoZ8+eio2NVVxcnCZNmqSDBw86+8bHx6t3796KiYnR+PHjC/x3BPkjsK8Cu3bt0q+//qrc3NwCv6s9V/369V3+8Tv/ce+99xY41uFw6Pbbb1dSUpIznBs1aqSsrCxt3rxZq1atcv5PffZ7qYULFyo5Odn52LJli8v32EVRunTpPHXl5uZe0jKL24kTJzRixAiXbd+0aZN27NghHx8f7d27V3//+9/VqFEjzZs3T+vWrdOUKVMknTkXQFKhTkK60FycOHFCISEhLjUkJycrJSVFL7zwgnPMuf+ANm3a1KXv/fffL09PT3311VdavHix6tWrp8mTJ6t27dras2dPgXV5eJz558GccwfeU6dOXXR7znexeSzMPJSU0NBQpaSkaOrUqfL19VXfvn11++2369SpUypXrpzWr1+vjz76SCEhIRo2bJgaN27s1pnqV/J9npmZqdjYWPn7++vDDz/U2rVrtWDBAkl/vT9Lwvvvv6/Vq1erRYsWmjt3riIiIvTDDz9IOvO9/v/+9z+1bdtWX3/9terVq+esCRfHWeJXWHZ2th599FF17NhRtWvXVu/evbVp0yZVrly5wDGLFi264D+cFwuJ1q1ba8aMGfL29taYMWPk4eGh22+/Xa+99pqysrKceyT16tWTt7e39u/f7wzxi6ldu7bWrl3r0nb+88Lw8vJSTk7OBfvUrVtXp0+f1o8//qgWLVpIko4ePaqUlBTVq1fPrfWVLl06z/puueUWpaSkqGbNmvmOWbdunXJzc/XGG284w+3jjz926dOoUSMlJiZqxIgRbtVzbg2pqakqVaqUy8lwF+Lr65tvzQ6HQy1btlTLli01bNgwhYWFacGCBYqPj893vs/usR88eFCBgYGSzhwxOFfdunX1448/urSd/cf53G240DwWxtmz3i/2njhf3bp19f3336tHjx7Otu+//97l/eHr66u4uDjFxcWpX79+qlOnjjZt2qRbbrlFpUqVUkxMjGJiYpSQkKDy5cvr66+/1oMPPljkbbmQ/P4OhXmf5zdu27ZtOnr0qMaPH6/Q0FBJuuQTvOrWrauZM2cqMzPT+SHx+++/l4eHh2rXru3sd/PNN+vmm2/W4MGDFR0drdmzZ+vWW2+VJEVERCgiIkIDBw5U586d9f7776t9+/aXVNf1gsC+woYMGaL09HS9+eabKlu2rBYtWqTHHntMX375ZYFjwsLCLmmdd9xxhwYOHCgvLy/ddtttzrbnn39ezZo1c/6PWK5cOT3//PMaOHCgcnNzddtttyk9PV3ff/+9/P39Xf4RPKt///66/fbbNWHCBMXFxenrr7/W4sWL5XA43KoxPDxcP/74o/bu3auyZcuqQoUKzlA8q1atWnrggQfUp08f/etf/1K5cuU0aNAgVa1aVQ888IDb60tMTFTLli3l7e2twMBADRs2TH//+99VrVo1dejQQR4eHtq4caM2b96s0aNHq2bNmjp16pQmT56suLg4ff/995o2bZrLcgcPHqyGDRuqb9++euqpp+Tl5aUVK1bo4YcfVsWKFS9aV0xMjKKjo9WuXTu9+uqrioiI0K+//qqFCxeqffv2atq0aaG278cff1RiYqLuueceVa5cWT/++KMOHz6sunXrOrd/6dKlSklJ0Q033KCAgADVrFlToaGhGj58uMaMGaPt27frjTfecFnus88+q5YtW+r111/XAw88oKVLl2rJkiUufS42j4URFhYmh8OhL7/8Uvfdd598fX1VtmzZi4574YUX9Mgjj+jmm29WTEyM/r//7//T/PnznVdBzJw5Uzk5OYqKilKZMmX0n//8R76+vgoLC9OXX36p3bt36/bbb1dgYKAWLVqk3Nxcl2AqbuHh4Tpx4oQSExPVuHFj5xnzF3uf5zeuWrVq8vLy0uTJk/XUU09p8+bNBf4OQWF17dpVCQkJ6tGjh4YPH67Dhw+rf//+6tatm4KCgrRnzx5Nnz5d999/v6pUqaKUlBTt2LFD3bt31x9//KEXXnhBHTp00E033aSff/5Za9eu1UMPPVQcU3d9uNJfol/PVqxYYUqVKmW+++47Z9uePXuMv7+/mTp1aomtNycnxwQGBrqc/HL2ZKJBgwa59M3NzTUTJ040tWvXNqVLlzaVKlUysbGx5ptvvnFug847aWX69OmmatWqxtfX17Rr186MHj3aBAcHO19PSEgwjRs3dlnPP//5TxMWFuZ8npKSYm699Vbj6+vrcpLT+X777TfTrVs3ExAQYHx9fU1sbKzZvn278/XCnnT2xRdfmJo1a5pSpUq51LFkyRLTokUL4+vra/z9/U3z5s2dZxEbY8yECRNMSEiIc93//ve/88xHUlKSadGihfH29jbly5c3sbGxztfzO7nugQcecJ51a4wxGRkZpn///qZKlSqmdOnSJjQ01HTt2tXs37+/wPk835YtW0xsbKypVKmS8fb2NhEREWby5MnO1w8dOmTatGljypYt6zJfK1euNA0bNjQ+Pj6mVatW5pNPPsnz93j33XfNjTfeaHx9fU1cXJx5/fXX85wAdbF5VD4n/QUEBJj333/f+XzkyJEmODjYOBwOl/k5V34nX02dOtVUr17dlC5d2kRERJh///vfztcWLFhgoqKijL+/v/Hz8zO33nqrWb58uTHmzAl/rVu3NoGBgcbX19c0atTIzJ07t8A5zu+ks4u9z/Pz1FNPmRtuuMFIMgkJCcaYi7/PCxo3e/ZsEx4ebry9vU10dLT54osvjCSzYcMGY4z7J50Zc+bE1TvvvNP4+PiYChUqmD59+jhPYktNTTXt2rUzISEhxsvLy4SFhZlhw4aZnJwck5WVZTp16mRCQ0ONl5eXqVKlinnmmWfMH3/8ccH5wF8cxpzzBRVQAvr06aNt27bpu+++u9KlAIC1OCSOYvf666+rTZs28vPz0+LFizVr1ixNnTr1SpcFAFZjDxvF7pFHHlFSUpKOHz+u6tWrq3///nrqqaeudFkAYDUCGwAAC3AdNgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhvXLIfDoeHDh1/pMq4Kw4cPl8PhKNLYnj17Kjw8vHgLAuA2AhslYtOmTerQoYPCwsLk4+OjqlWrqk2bNpo8ebJLv7Fjx+qzzz67MkUWg9mzZ2vixImF7h8eHi6Hw6GYmJh8X58xY4YcDoccDod++umnYqry6pWUlKQHH3xQwcHB8vLyUuXKlRUXF6f58+c7++zdu9c5J/PmzcuzjLMfRo4cOeJs69mzpxwOhxo1aqT87iDscDj0zDPP5FnH66+/fsF6s7OzNWnSJN18883y9/dX+fLlVb9+fT3xxBPatm2bc9mFeSQlJbls2+jRo/NdZ9euXeVwOFS2bNkLTyaueaWudAG49qxatUp33nmnqlWrpj59+ig4OFgHDhzQDz/8oEmTJql///7OvmPHjlWHDh3Url27K1fwJZg9e7Y2b96sAQMGFHqMj4+PVqxYodTUVAUHB7u89uGHH8rHx0d//vlnMVd69UlISNDIkSNVq1YtPfnkkwoLC9PRo0e1aNEiPfTQQ/rwww/VpUsXlzEjR47Ugw8+WOijBZs2bdL8+fP10EMPFUvNDz30kBYvXqzOnTurT58+OnXqlLZt26Yvv/xSLVq0UJ06dfTBBx+4jPn3v/+tr776Kk973bp19ccff0g685746KOPNHToUJc+mZmZ+vzzz+Xj41Ms9cNuBDaK3ZgxYxQQEKC1a9eqfPnyLq8dOnToyhR1FWnZsqXWrl2ruXPn6rnnnnO2//zzz/ruu+/Uvn37fPckryWffvqpRo4cqQ4dOmj27NkqXbq087UXXnhBS5cu1alTp1zGREZGKjk5WQsWLNCDDz540XX4+voqNDTU7ZAvyNq1a/Xll19qzJgxevnll11ee+utt3Ts2DFJ0qOPPury2g8//KCvvvoqT7t0Zs9eku677z7Nnz9fGzduVOPGjZ2vf/7558rOztbf/vY3ff3115dUP+zHIXEUu127dql+/fp5wlqSKleu7Pxvh8OhzMxMzZo1y3lYsGfPnpIK/t40v+9is7KyNHDgQFWqVEnlypXT/fffr59//jnf2n755Rc99thjCgoKkre3t+rXr6/33nvPpU9SUpIcDoc+/vhjjRkzRjfeeKN8fHx09913a+fOnc5+d9xxhxYuXKh9+/Y56y/Md70+Pj568MEHNXv2bJf2jz76SIGBgYqNjc133Ndff61WrVrJz89P5cuX1wMPPKCtW7fm6bdy5Uo1a9ZMPj4+qlGjhv71r38VWMt//vMfNWnSRL6+vqpQoYI6deqkAwcOXHQbLtUrr7yiChUq6L333nMJ67NiY2P197//3aWtU6dOioiI0MiRI/M9zH0+Dw8PDR06VP/973+1YMGCS655165dks584Dqfp6enbrjhhiIvOzo6WjfddFOe98SHH36ov/3tb6pQoUKRl41rB4GNYhcWFqZ169Zp8+bNF+z3wQcfyNvbW61atdIHH3ygDz74QE8++aTb6+vdu7cmTpyoe+65R+PHj1fp0qXVtm3bPP3S0tJ06623avny5XrmmWc0adIk1axZU48//ni+30OPHz9eCxYs0PPPP6/Bgwfrhx9+UNeuXZ2vDxkyRJGRkapYsaKz/sJ+n92lSxetWbPGGQLSmcPrHTp0yDfAli9frtjYWB06dEjDhw9XfHy8Vq1apZYtWzr30qQzh4DvueceZ79evXopISEh38AaM2aMunfvrlq1amnChAkaMGCAEhMTdfvttzv3FkvCjh07tG3bNrVr107lypUr9DhPT08NHTpUGzduLHQAd+nSRbVq1Sp0yF9IWFiYpDMhevr06UtaVn46d+6sOXPmOOs8cuSIli1bludrAVzHDFDMli1bZjw9PY2np6eJjo42L774olm6dKnJzs7O09fPz8/06NEjT3uPHj1MWFhYnvaEhARz7ts2OTnZSDJ9+/Z16delSxcjySQkJDjbHn/8cRMSEmKOHDni0rdTp04mICDAnDx50hhjzIoVK4wkU7duXZOVleXsN2nSJCPJbNq0ydnWtm3bfOssSFhYmGnbtq05ffq0CQ4ONqNGjTLGGLNlyxYjyXzzzTfm/fffN5LM2rVrneMiIyNN5cqVzdGjR51tGzduNB4eHqZ79+7Otnbt2hkfHx+zb98+Z9uWLVuMp6eny7zt3bvXeHp6mjFjxrjUt2nTJlOqVCmX9oL+FkX1+eefG0nmn//8Z6H679mzx0gyr732mjl9+rSpVauWady4scnNzTXG/PWeOHz4sEvNfn5+xhhjZs2aZSSZ+fPnO1+XZPr165fvOgqSm5trWrdubSSZoKAg07lzZzNlyhSXuc5Pv379TEH/1J673s2bNxtJ5rvvvjPGGDNlyhRTtmxZk5mZ6bI9uH6xh41i16ZNG61evVr333+/Nm7cqFdffVWxsbGqWrWqvvjii2Jd16JFiyRJzz77rEv7+SeBGWM0b948xcXFyRijI0eOOB+xsbFKT0/X+vXrXcb06tVLXl5ezuetWrWSJO3evfuS6/b09NQjjzyijz76SNKZvbbQ0FDnOs518OBBJScnq2fPni6HRhs1aqQ2bdo45yAnJ0dLly5Vu3btVK1aNWe/unXr5jnMPn/+fOXm5uqRRx5xmYvg4GDVqlVLK1asuORtLEhGRoYkubV3fda5e9mFvbqga9euxbKX7XA4tHTpUo0ePVqBgYH66KOP1K9fP4WFhaljx46XfFSifv36atSokfM9MXv2bD3wwAMqU6bMJS0X1w4CGyWiWbNmmj9/vn7//XetWbNGgwcP1vHjx9WhQwdt2bKl2Nazb98+eXh4qEaNGi7ttWvXdnl++PBhHTt2TNOnT1elSpVcHr169ZKU94S4c0NPkgIDAyVJv//+e7HU3qVLF23ZskUbN27U7Nmz1alTp3xPjNq3b1++2ySdCeMjR44oMzNThw8f1h9//KFatWrl6Xf+2B07dsgYo1q1auWZj61bt7p9cmB6erpSU1Odj99++63Avv7+/pKk48ePu7WOs7p27aqaNWsWOoDPhnxycvIlX0Lo7e2tIUOGaOvWrfr111/10Ucf6dZbb9XHH3/scplYUXXp0kWffPKJdu7cqVWrVnE4HC44SxwlysvLS82aNVOzZs0UERGhXr166ZNPPlFCQsIFxxV0Rm9OTk6R6sjNzZV05gzeHj165NunUaNGLs89PT3z7Xcpe2nnioqKUo0aNTRgwADt2bPnsv7jnJubK4fDocWLF+e7ne5e8/vcc89p1qxZzuetW7dWUlJSvn3r1Kkj6cz37UVxNoB79uypzz//vFBjunbtqlGjRmnkyJHFdglhSEiIOnXqpIceekj169fXxx9/rJkzZ6pUqaL/s9q5c2cNHjxYffr00Q033KB77rmnWGrFtYHAxmXTtGlTSWcO8Z5VUDAHBgbme4jx7N7mWWFhYcrNzdWuXbtc9iJTUlJc+p09gzwnJ6fAHy0piku9VKhz584aPXq06tatq8jIyHz7nD3Z6fxtkqRt27apYsWK8vPzk4+Pj3x9fbVjx448/c4fW6NGDRljdNNNNykiIuKStkGSXnzxRZfLls4ejchPRESEateurc8//1yTJk0q0g+CPProoxo9erRGjBih+++//6L9ixLyhVW6dGk1atRIO3bscH6tUFTVqlVTy5YtlZSUpKeffvqSwh/XHg6Jo9itWLEi373Qs9+1nhusfn5++QZzjRo1lJ6erv/+97/OtoMHD+Y5O/jee++VJL355psu7eefre3p6amHHnpI8+bNy/fs9cOHD194owrg5+en9PT0Io2VzpzhnpCQoDfeeKPAPiEhIYqMjNSsWbNc5mrz5s1atmyZ7rvvPklntjE2NlafffaZ9u/f7+y3detWLV261GWZDz74oDw9PTVixIg8fytjjI4ePerWdtSrV08xMTHOR5MmTS7Yf8SIETp69Kh69+6d7xnXy5Yt05dfflng+HMPcxf2vIhHH31UNWvW1IgRIwrV/3w7duxwmdezjh07ptWrVyswMFCVKlUq0rLPNXr0aCUkJLj8wBAgsYeNEtC/f3+dPHlS7du3V506dZSdna1Vq1Zp7ty5Cg8Pd35nLElNmjTR8uXLNWHCBFWpUkU33XSToqKi1KlTJ7300ktq3769nn32WZ08eVJvv/22IiIiXE4Oi4yMVOfOnTV16lSlp6erRYsWSkxMdLle+qzx48drxYoVioqKUp8+fVSvXj399ttvWr9+vZYvX37B710L0qRJE82dO1fx8fFq1qyZypYtq7i4uEKPDwsLK9Tvnb/22mu69957FR0drccff1x//PGHJk+erICAAJfxI0aM0JIlS9SqVSv17dtXp0+f1uTJk1W/fn2XDz81atTQ6NGjNXjwYO3du9d5idWePXu0YMECPfHEE3r++efdmQq3dOzYUZs2bdKYMWO0YcMGde7c2flLZ0uWLFFiYmKea5LPd/Ywd3JycqHW6enpqSFDhri8/86XmJiY76/MtWvXTtu2bVOXLl107733qlWrVqpQoYJ++eUXzZo1S7/++qsmTpxY4Nco7mjdurVat259ycvBNehKnZ6Oa9fixYvNY489ZurUqWPKli1rvLy8TM2aNU3//v1NWlqaS99t27aZ22+/3fj6+hpJLpd4LVu2zDRo0MB4eXmZ2rVrm//85z95Lusyxpg//vjDPPvss+aGG24wfn5+Ji4uzhw4cCDPZV3GGJOWlmb69etnQkNDTenSpU1wcLC5++67zfTp0519zl7W9cknn7iMPXsJzvvvv+9sO3HihOnSpYspX768kXTRy5/OXtZ1Ifld1mWMMcuXLzctW7Y0vr6+xt/f38TFxZktW7bkGf/NN9+YJk2aGC8vL1O9enUzbdq0fOfNGGPmzZtnbrvtNuPn52f8/PxMnTp1TL9+/UxKSoqzT3Ff1nWuxMRE88ADD5jKlSubUqVKmUqVKpm4uDjz+eefO/tc6JKrs3OlC1zWda5Tp06ZGjVqFHhZV0GPDz74wKSlpZnx48eb1q1bm5CQEFOqVCkTGBho7rrrLvPpp58WuI2FvazrQrisC8YY4zCmmM6gAQAAJYbvsAEAsACBDQCABQhsAAAs4HZgf/vtt4qLi1OVKlXkcDgK9ctBSUlJuuWWW+Tt7a2aNWtq5syZRSgVAIDrl9uBnZmZqcaNG2vKlCmF6r9nzx61bdtWd955p5KTkzVgwAD17t07z3WhAACgYJd0lrjD4dCCBQsu+FN/L730khYuXOjyYxWdOnXSsWPHtGTJkqKuGgCA60qJ/3DK6tWr8/wUZGxsbJ67KZ0rKytLWVlZzue5ubn67bffdMMNN1zyT0ECAFCSjDE6fvy4qlSpIg+P4jtVrMQDOzU1VUFBQS5tQUFBysjI0B9//CFfX988Y8aNG1fknw8EAOBqcODAAd14443Ftryr8qdJBw8erPj4eOfz9PR0VatWTQcOHHDemg8AgKtRRkaGQkNDi3TP9wsp8cAODg5WWlqaS1taWpr8/f3z3buWztxz1tvbO0+7v78/gQ0AsEJxf4Vb4tdhR0dHKzEx0aXtq6++UnR0dEmvGgCAa4bbgX3ixAklJyc775CzZ88eJScnO287N3jwYHXv3t3Z/6mnntLu3bv14osvatu2bZo6dao+/vhjDRw4sHi2AACA64Dbgf3TTz/p5ptv1s033yxJio+P180336xhw4ZJOnPP4nPvGXvTTTdp4cKF+uqrr9S4cWO98cYbeueddxQbG1tMmwAAwLXPirt1ZWRkKCAgQOnp6XyHDQC4qpVUZvFb4gAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsUKbCnTJmi8PBw+fj4KCoqSmvWrLlg/4kTJ6p27dry9fVVaGioBg4cqD///LNIBQMAcD1yO7Dnzp2r+Ph4JSQkaP369WrcuLFiY2N16NChfPvPnj1bgwYNUkJCgrZu3ap3331Xc+fO1csvv3zJxQMAcL1wO7AnTJigPn36qFevXqpXr56mTZumMmXK6L333su3/6pVq9SyZUt16dJF4eHhuueee9S5c+eL7pUDAIC/uBXY2dnZWrdunWJiYv5agIeHYmJitHr16nzHtGjRQuvWrXMG9O7du7Vo0SLdd999Ba4nKytLGRkZLg8AAK5npdzpfOTIEeXk5CgoKMilPSgoSNu2bct3TJcuXXTkyBHddtttMsbo9OnTeuqppy54SHzcuHEaMWKEO6UBAHBNK/GzxJOSkjR27FhNnTpV69ev1/z587Vw4UKNGjWqwDGDBw9Wenq683HgwIGSLhMAgKuaW3vYFStWlKenp9LS0lza09LSFBwcnO+YV155Rd26dVPv3r0lSQ0bNlRmZqaeeOIJDRkyRB4eeT8zeHt7y9vb253SAAC4prm1h+3l5aUmTZooMTHR2Zabm6vExERFR0fnO+bkyZN5QtnT01OSZIxxt14AAK5Lbu1hS1J8fLx69Oihpk2bqnnz5po4caIyMzPVq1cvSVL37t1VtWpVjRs3TpIUFxenCRMm6Oabb1ZUVJR27typV155RXFxcc7gBgAAF+Z2YHfs2FGHDx/WsGHDlJqaqsjISC1ZssR5Itr+/ftd9qiHDh0qh8OhoUOH6pdfflGlSpUUFxenMWPGFN9WAABwjXMYC45LZ2RkKCAgQOnp6fL397/S5QAAUKCSyix+SxwAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsUKTAnjJlisLDw+Xj46OoqCitWbPmgv2PHTumfv36KSQkRN7e3oqIiNCiRYuKVDAAANejUu4OmDt3ruLj4zVt2jRFRUVp4sSJio2NVUpKiipXrpynf3Z2ttq0aaPKlSvr008/VdWqVbVv3z6VL1++OOoHAOC64DDGGHcGREVFqVmzZnrrrbckSbm5uQoNDVX//v01aNCgPP2nTZum1157Tdu2bVPp0qWLVGRGRoYCAgKUnp4uf3//Ii0DAIDLoaQyy61D4tnZ2Vq3bp1iYmL+WoCHh2JiYrR69ep8x3zxxReKjo5Wv379FBQUpAYNGmjs2LHKycm5tMoBALiOuHVI/MiRI8rJyVFQUJBLe1BQkLZt25bvmN27d+vrr79W165dtWjRIu3cuVN9+/bVqVOnlJCQkO+YrKwsZWVlOZ9nZGS4UyYAANecEj9LPDc3V5UrV9b06dPVpEkTdezYUUOGDNG0adMKHDNu3DgFBAQ4H6GhoSVdJgAAVzW3ArtixYry9PRUWlqaS3taWpqCg4PzHRMSEqKIiAh5eno62+rWravU1FRlZ2fnO2bw4MFKT093Pg4cOOBOmQAAXHPcCmwvLy81adJEiYmJzrbc3FwlJiYqOjo63zEtW7bUzp07lZub62zbvn27QkJC5OXlle8Yb29v+fv7uzwAALieuX1IPD4+XjNmzNCsWbO0detWPf3008rMzFSvXr0kSd27d9fgwYOd/Z9++mn99ttveu6557R9+3YtXLhQY8eOVb9+/YpvKwAAuMa5fR12x44ddfjwYQ0bNkypqamKjIzUkiVLnCei7d+/Xx4ef30OCA0N1dKlSzVw4EA1atRIVatW1XPPPaeXXnqp+LYCAIBrnNvXYV8JXIcNALDFVXEdNgAAuDIIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALBAkQJ7ypQpCg8Pl4+Pj6KiorRmzZpCjZszZ44cDofatWtXlNUCAHDdcjuw586dq/j4eCUkJGj9+vVq3LixYmNjdejQoQuO27t3r55//nm1atWqyMUCAHC9cjuwJ0yYoD59+qhXr16qV6+epk2bpjJlyui9994rcExOTo66du2qESNGqHr16pdUMAAA1yO3Ajs7O1vr1q1TTEzMXwvw8FBMTIxWr15d4LiRI0eqcuXKevzxxwu1nqysLGVkZLg8AAC4nrkV2EeOHFFOTo6CgoJc2oOCgpSamprvmJUrV+rdd9/VjBkzCr2ecePGKSAgwPkIDQ11p0wAAK45JXqW+PHjx9WtWzfNmDFDFStWLPS4wYMHKz093fk4cOBACVYJAMDVr5Q7nStWrChPT0+lpaW5tKelpSk4ODhP/127dmnv3r2Ki4tztuXm5p5ZcalSSklJUY0aNfKM8/b2lre3tzulAQBwTXNrD9vLy0tNmjRRYmKisy03N1eJiYmKjo7O079OnTratGmTkpOTnY/7779fd955p5KTkznUDQBAIbm1hy1J8fHx6tGjh5o2barmzZtr4sSJyszMVK9evSRJ3bt3V9WqVTVu3Dj5+PioQYMGLuPLly8vSXnaAQBAwdwO7I4dO+rw4cMaNmyYUlNTFRkZqSVLljhPRNu/f788PPgBNQAAipPDGGOudBEXk5GRoYCAAKWnp8vf3/9KlwMAQIFKKrPYFQYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFigSIE9ZcoUhYeHy8fHR1FRUVqzZk2BfWfMmKFWrVopMDBQgYGBiomJuWB/AACQl9uBPXfuXMXHxyshIUHr169X48aNFRsbq0OHDuXbPykpSZ07d9aKFSu0evVqhYaG6p577tEvv/xyycUDAHC9cBhjjDsDoqKi1KxZM7311luSpNzcXIWGhqp///4aNGjQRcfn5OQoMDBQb731lrp3716odWZkZCggIEDp6eny9/d3p1wAAC6rksost/aws7OztW7dOsXExPy1AA8PxcTEaPXq1YVaxsmTJ3Xq1ClVqFChwD5ZWVnKyMhweQAAcD1zK7CPHDminJwcBQUFubQHBQUpNTW1UMt46aWXVKVKFZfQP9+4ceMUEBDgfISGhrpTJgAA15zLepb4+PHjNWfOHC1YsEA+Pj4F9hs8eLDS09OdjwMHDlzGKgEAuPqUcqdzxYoV5enpqbS0NJf2tLQ0BQcHX3Ds66+/rvHjx2v58uVq1KjRBft6e3vL29vbndIAALimubWH7eXlpSZNmigxMdHZlpubq8TEREVHRxc47tVXX9WoUaO0ZMkSNW3atOjVAgBwnXJrD1uS4uPj1aNHDzVt2lTNmzfXxIkTlZmZqV69ekmSunfvrqpVq2rcuHGSpH/84x8aNmyYZs+erfDwcOd33WXLllXZsmWLcVMAALh2uR3YHTt21OHDhzVs2DClpqYqMjJSS5YscZ6Itn//fnl4/LXj/vbbbys7O1sdOnRwWU5CQoKGDx9+adUDAHCdcPs67CuB67ABALa4Kq7DBgAAVwaBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABQhsAAAsQGADAGABAhsAAAsQ2AAAWIDABgDAAgQ2AAAWILABALAAgQ0AgAUIbAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACxAYAMAYAECGwAACxDYAABYgMAGAMACRQrsKVOmKDw8XD4+PoqKitKaNWsu2P+TTz5RnTp15OPjo4YNG2rRokVFKhYAgOuV24E9d+5cxcfHKyEhQevXr1fjxo0VGxurQ4cO5dt/1apV6ty5sx5//HFt2LBB7dq1U7t27bR58+ZLLh4AgOuFwxhj3BkQFRWlZs2a6a233pIk5ebmKjQ0VP3799egQYPy9O/YsaMyMzP15ZdfOttuvfVWRUZGatq0aYVaZ0ZGhgICApSeni5/f393ygUA4LIqqcxyaw87Oztb69atU0xMzF8L8PBQTEyMVq9ene+Y1atXu/SXpNjY2AL7AwCAvEq50/nIkSPKyclRUFCQS3tQUJC2bduW75jU1NR8+6empha4nqysLGVlZTmfp6enSzrzqQUAgKvZ2axy8wD2RbkV2JfLuHHjNGLEiDztoaGhV6AaAADcd/ToUQUEBBTb8twK7IoVK8rT01NpaWku7WlpaQoODs53THBwsFv9JWnw4MGKj493Pj927JjCwsK0f//+Yt34611GRoZCQ0N14MABzg0oRsxr8WNOSwbzWjLS09NVrVo1VahQoViX61Zge3l5qUmTJkpMTFS7du0knTnpLDExUc8880y+Y6Kjo5WYmKgBAwY427766itFR0cXuB5vb295e3vnaQ8ICOBNVQL8/f2Z1xLAvBY/5rRkMK8lw8OjeH/qxO1D4vHx8erRo4eaNm2q5s2ba+LEicrMzFSvXr0kSd27d1fVqlU1btw4SdJzzz2n1q1b64033lDbtm01Z84c/fTTT5o+fXqxbggAANcytwO7Y8eOOnz4sIYNG6bU1FRFRkZqyZIlzhPL9u/f7/KpokWLFpo9e7aGDh2ql19+WbVq1dJnn32mBg0aFN9WAABwjSvSSWfPPPNMgYfAk5KS8rQ9/PDDevjhh4uyKklnDpEnJCTke5gcRce8lgzmtfgxpyWDeS0ZJTWvbv9wCgAAuPy4+QcAABYgsAEAsACBDQCABQhsAAAscNUENvfYLhnuzOuMGTPUqlUrBQYGKjAwUDExMRf9O1yP3H2vnjVnzhw5HA7njw7BlbvzeuzYMfXr108hISHy9vZWREQE/w7kw915nThxomrXri1fX1+FhoZq4MCB+vPPPy9TtXb49ttvFRcXpypVqsjhcOizzz676JikpCTdcsst8vb2Vs2aNTVz5kz3V2yuAnPmzDFeXl7mvffeM//73/9Mnz59TPny5U1aWlq+/b///nvj6elpXn31VbNlyxYzdOhQU7p0abNp06bLXPnVzd157dKli5kyZYrZsGGD2bp1q+nZs6cJCAgwP//882Wu/Orl7pyetWfPHlO1alXTqlUr88ADD1yeYi3i7rxmZWWZpk2bmvvuu8+sXLnS7NmzxyQlJZnk5OTLXPnVzd15/fDDD423t7f58MMPzZ49e8zSpUtNSEiIGThw4GWu/Oq2aNEiM2TIEDN//nwjySxYsOCC/Xfv3m3KlClj4uPjzZYtW8zkyZONp6enWbJkiVvrvSoCu3nz5qZfv37O5zk5OaZKlSpm3Lhx+fZ/5JFHTNu2bV3aoqKizJNPPlmiddrG3Xk93+nTp025cuXMrFmzSqpE6xRlTk+fPm1atGhh3nnnHdOjRw8COx/uzuvbb79tqlevbrKzsy9XiVZyd1779etn7rrrLpe2+Ph407JlyxKt02aFCewXX3zR1K9f36WtY8eOJjY21q11XfFD4txju2QUZV7Pd/LkSZ06darYf8DeVkWd05EjR6py5cp6/PHHL0eZ1inKvH7xxReKjo5Wv379FBQUpAYNGmjs2LHKycm5XGVf9Yoyry1atNC6deuch813796tRYsW6b777rssNV+riiuzrvjtNS/XPbavN0WZ1/O99NJLqlKlSp432vWqKHO6cuVKvfvuu0pOTr4MFdqpKPO6e/duff311+ratasWLVqknTt3qm/fvjp16pQSEhIuR9lXvaLMa5cuXXTkyBHddtttMsbo9OnTeuqpp/Tyyy9fjpKvWQVlVkZGhv744w/5+voWajlXfA8bV6fx48drzpw5WrBggXx8fK50OVY6fvy4unXrphkzZqhixYpXupxrSm5uripXrqzp06erSZMm6tixo4YMGaJp06Zd6dKslpSUpLFjx2rq1Klav3695s+fr4ULF2rUqFFXujToKtjDvlz32L7eFGVez3r99dc1fvx4LV++XI0aNSrJMq3i7pzu2rVLe/fuVVxcnLMtNzdXklSqVCmlpKSoRo0aJVu0BYryXg0JCVHp0qXl6enpbKtbt65SU1OVnZ0tLy+vEq3ZBkWZ11deeUXdunVT7969JUkNGzZUZmamnnjiCQ0ZMqTYbxd5vSgos/z9/Qu9dy1dBXvY595j+6yz99gu6J7ZZ++xfa6L3WP7elOUeZWkV199VaNGjdKSJUvUtGnTy1GqNdyd0zp16mjTpk1KTk52Pu6//37deeedSk5OVmho6OUs/6pVlPdqy5YttXPnTucHIEnavn27QkJCCOv/pyjzevLkyTyhfPZDkeG2E0VWbJnl3vlwJWPOnDnG29vbzJw502zZssU88cQTpnz58iY1NdUYY0y3bt3MoEGDnP2///57U6pUKfP666+brVu3moSEBC7ryoe78zp+/Hjj5eVlPv30U3Pw4EHn4/jx41dqE6467s7p+ThLPH/uzuv+/ftNuXLlzDPPPGNSUlLMl19+aSpXrmxGjx59pTbhquTuvCYkJJhy5cqZjz76yOzevdssW7bM1KhRwzzyyCNXahOuSsePHzcbNmwwGzZsMJLMhAkTzIYNG8y+ffuMMcYMGjTIdOvWzdn/7GVdL7zwgtm6dauZMmWKvZd1GWPM5MmTTbVq1YyXl5dp3ry5+eGHH5yvtW7d2vTo0cOl/8cff2wiIiKMl5eXqV+/vlm4cOFlrtgO7sxrWFiYkZTnkZCQcPkLv4q5+149F4FdMHfnddWqVSYqKsp4e3ub6tWrmzFjxpjTp09f5qqvfu7M66lTp8zw4cNNjRo1jI+PjwkNDTV9+/Y1v//+++Uv/Cq2YsWKfP+tPDuXPXr0MK1bt84zJjIy0nh5eZnq1aub999/3+31cntNAAAscMW/wwYAABdHYAMAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDcCFw+HQZ599dqXLAHCeK37zDwBXl4MHDyowMPBKl1GgO+64Q5GRkZo4ceKVLgW4rNjDBixn/t99i4tLcHCwvL29i215hXXq1KnLvk7AJgQ2UIDDhw8rODhYY8eOdbatWrVKXl5eee68U1z27t0rh8OhOXPmqEWLFvLx8VGDBg30zTffOPskJSXJ4XBo8eLFatKkiby9vbVy5Ur17NlT7dq1c1negAEDdMcddzif33HHHXr22Wf14osvqkKFCgoODtbw4cNdxpx7SPxsPfPnz9edd96pMmXKqHHjxlq9erXLmBkzZig0NFRlypRR+/btNWHCBJUvX/6i2zl37ly1bt1aPj4++vDDD3X06FF17txZVatWVZkyZdSwYUN99NFHznE9e/bUN998o0mTJsnhcMjhcGjv3r2SpM2bN+vee+9V2bJlFRQUpG7duunIkSOFnnvgqneJv4EOXNMWLlxoSpcubdauXWsyMjJM9erVzcCBAy84pl69esbPz6/Ax9/+9rcCx+7Zs8dIMjfeeKP59NNPzZYtW0zv3r1NuXLlzJEjR4wxf914oFGjRmbZsmVm586d5ujRo/neWOS5555zuQlB69atjb+/vxk+fLjZvn27mTVrlnE4HGbZsmXOPpLMggULXOqpU6eO+fLLL01KSorp0KGDCQsLM6dOnTLGGLNy5Urj4eFhXnvtNZOSkmKmTJliKlSoYAICAi66neHh4WbevHlm9+7d5tdffzU///yzee2118yGDRvMrl27zJtvvmk8PT3Njz/+aIwx5tixYyY6Otr06dPHeTe506dPm99//91UqlTJDB482GzdutWsX7/etGnTxtx5550X/FsBNiGwgYvo27eviYiIMF26dDENGzY0f/755wX779271+zYsaPAx88//1zg2LNBNn78eGfbqVOnzI033mj+8Y9/GGP+CuzPPvvMZWxhA/u2225z6dOsWTPz0ksvOZ/nF9jvvPOO8/X//e9/RpLZunWrMcaYjh07mrZt27oss2vXroUK7IkTJxbY56y2bdua//u//3PZhueee86lz6hRo8w999zj0nbgwAEjyaSkpFx0HYANOOkMuIjXX39dDRo00CeffKJ169Zd9PvdsLCwS17nuTe2L1WqlJo2baqtW7e69GnatGmRlt2oUSOX5yEhITp06FChx4SEhEiSDh06pDp16iglJUXt27d36d+8eXN9+eWXF63l/G3IycnR2LFj9fHHH+uXX35Rdna2srKyVKZMmQsuZ+PGjVqxYoXKli2b57Vdu3YpIiLiorUAVzsCG7iIXbt26ddff1Vubq727t2rhg0bXrB//fr1tW/fvgJfb9WqlRYvXnzJdfn5+bk89/DwkDnvbrn5nchVunRpl+cOh0O5ubkXXNe5YxwOhyRddExhnL8Nr732miZNmqSJEyeqYcOG8vPz04ABA5SdnX3B5Zw4cUJxcXH6xz/+kee1sx8wANsR2MAFZGdn69FHH1XHjh1Vu3Zt9e7dW5s2bVLlypULHLNo0aILnvHs6+t70fX+8MMPuv322yVJp0+f1rp16/TMM89ccEylSpW0efNml7bk5OQ8AV3cateurbVr17q0nf+8sL7//ns98MADevTRRyWd+VCwfft21atXz9nHy8tLOTk5LuNuueUWzZs3T+Hh4SpVin/WcG3iLHHgAoYMGaL09HS9+eabeumllxQREaHHHnvsgmPCwsJUs2bNAh9Vq1a96HqnTJmiBQsWaNu2berXr59+//33i673rrvu0k8//aR///vf2rFjhxISEvIEeEno37+/Fi1apAkTJmjHjh3617/+pcWLFzv3xN1Rq1YtffXVV1q1apW2bt2qJ598UmlpaS59wsPD9eOPP2rv3r06cuSIcnNz1a9fP/3222/q3Lmz1q5dq127dmnp0qXq1atXnnAHbEVgAwVISkrSxIkT9cEHH8jf318eHh764IMP9N133+ntt98u0XWPHz9e48ePV+PGjbVy5Up98cUXqlix4gXHxMbG6pVXXtGLL76oZs2a6fjx4+revXuJ1ilJLVu21LRp0zRhwgQ1btxYS5Ys0cCBA+Xj4+P2soYOHapbbrlFsbGxuuOOOxQcHJznUrXnn39enp6eqlevnipVqqT9+/erSpUq+v7775WTk6N77rlHDRs21IABA1S+fHl5ePDPHK4NDnP+l14Arpi9e/fqpptu0oYNGxQZGXmlyymyPn36aNu2bfruu++udCnANYMvewBcstdff11t2rSRn5+fFi9erFmzZmnq1KlXuizgmkJgA7hka9as0auvvqrjx4+revXqevPNN9W7d+8rXRZwTeGQOAAAFuBsDAAALEBgAwBgAQIbAAALENgAAFiAwAYAwAIENgAAFiCwAQCwAIENAIAFCGwAACzAT5MC14Dc3FxlZ2cX+Hrp0qXl6el5GSsCUNwIbMBy2dnZ2rNnj3Jzcy/Yr3z58goODi7SfaoBXHkENmAxY4wOHjwoT09PhYaG5nvvZ2OMTp48qUOHDkmSQkJCLneZAIoBgQ1Y7PTp0zp58qSqVKmiMmXKFNjP19dXknTo0CFVrlyZw+OAhTjpDLBYTk6OJMnLy+uifc8G+qlTp0q0JgAlg8AGrgGF+V6a764BuxHYAABYgMAGAMACBDYAABYgsIFrgDGmWPoAuHoR2IDFzl6edaFfOTvr5MmTks786hkA+3AdNmCxUqVKqUyZMjp8+LBKly590R9OKV++PNdgA5ZyGI6TAVbjp0mB6wOBDVwDuPkHcO0jsAEAsAAnnQEAYAECGwAACxDYAABYgMAGAMACBDYAABYgsAEAsACBDQCABf5/lDLjWneAV4wAAAAASUVORK5CYII=\n"},"metadata":{}}]},{"cell_type":"code","source":["extract_single(results_conv,\"precision\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CVt7yT4JXLjx","executionInfo":{"status":"ok","timestamp":1705951546933,"user_tz":0,"elapsed":560,"user":{"displayName":"Xiaochen Zoey Tan","userId":"02061312605743465288"}},"outputId":"7acd4b2c-e244-4359-92af-5143a83020d8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.8541707 , 0.00729927, 0.00729927, 0.8489272 , 0.00729927,\n","       0.8489272 , 0.8489272 , 0.00729927, 0.8489272 , 0.00729927,\n","       0.8489272 ], dtype=float32)"]},"metadata":{},"execution_count":122}]},{"cell_type":"code","source":[],"metadata":{"id":"AT9aQ79HeImO"},"execution_count":null,"outputs":[]}]}